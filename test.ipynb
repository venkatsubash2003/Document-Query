{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\venka\\anaconda3\\Lib\\site-packages\\pinecone\\index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import langchain\n",
    "import pinecone\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_doc(directory):\n",
    "    file_loader = PyPDFDirectoryLoader(directory)\n",
    "    documents = file_loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Deep Computer Vision Using  Convolutional  \\nNeural  Networks  \\nAlthough  IBM’s Deep Blue supercomputer beat the chess world champion Garry Kasparov back \\nin 1996, it wasn’t until fairly recently that computers were able to reliably perform seemingly \\ntrivial tasks such as detecting a puppy in a picture or recognizing spoken words. Why are these \\ntasks so effortless to us humans? The answer lies in the fact that perception largely takes place \\noutside the realm of our consciousness, within specialized visual, auditory,  and other sensory \\nmodules in our brains. By the time sensory information reaches our consciousness, it is already \\nadorned with high -level features; for example, when you look at a picture of a cute puppy, you \\ncannot choose  not to see the puppy,  not to not ice its cuteness. Nor can you explain  how you \\nrecognize a cute puppy; it’s just obvious to you. Thus, we cannot trust our subjective experience: \\nperception is not trivial at all, and to understand it we must look at how the sensory modules \\nwork.  \\nConvolutio nal neural networks (CNNs) emerged from the study of the brain’s visual cortex, and \\nthey have been used in image recognition since the 1980s. In the last few years, thanks to the \\nincrease in computational power, the amount of available training da ta, and t he tricks presented \\nbefore  for training deep nets, CNNs have managed to achieve superhuman performance on some \\ncomplex visual tasks. They power image search services, self -driving cars, automatic video \\nclassification systems, and more. Moreover, CNNs  are n ot restricted to visual perception: they \\nare also successful at many other tasks, such as voice recognition and natural language \\nprocessing. However, we will focus on visual applications for now.  \\nIn this section  we will explore where CNNs came from, what t heir building blocks look like, and \\nhow to implement them using TensorFlow and Keras. Then we will discuss some of the best \\nCNN architectures, as well as other visual tasks, including object detection (classifying multiple \\nobjects in an image and placing b ounding boxes around them) and semantic segmentation \\n(classifying each pixel according to the class of the object it belongs to).  \\nThe Architecture of the Visual Cortex  \\nDavid H. Hubel  and Torsten Wiesel performed a series of experiments on cats in  1958  and 1959  \\n(and a  few years  later on monkeys ), giving crucial insights into the structure of the visual cortex \\n(the authors received the Nobel Prize in  Physiology or Medicine in 1981 for their work). In \\nparticular, they showed that many neurons in the visual cortex have a small  local  receptive  field, \\nmeaning they react only to visual stimuli located in a limited region of the visual field \\n(see Figure  1, in which the local receptive fields of five neurons are represented by dashed \\ncircles). The receptive fields of diffe rent neurons may overlap, and together they tile the whole \\nvisual field.  \\nMoreover, the authors showed that some neurons react only to images of horizontal lines, while \\nothers react only to lines with different orientations (two neurons may have the same re ceptive ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 0}),\n",
       " Document(page_content='field but react to different line orientations). They also noticed that some neurons have larger \\nreceptive fields, and they react to more complex patterns that are combinations of the lower -level \\npatterns. These observations led to the idea that th e higher -level neurons are based on the outputs \\nof neighboring lower -level neurons (in  Figure  1, notice that each neu ron is connected only to a \\nfew neurons from the previous layer). This powerful architecture is able to detect all sorts of \\ncomplex patterns in any area of the visual field.  \\n \\nFigure  1. Biological neurons in the visual cortex respond to specific patterns in  small regions of the visual field called receptive fields; as the visual \\nsignal makes its way through consecutive brain modules, neurons respond to more complex patterns in larger receptive fields.  \\nThese studies of the visual cortex inspired the  neocognitron , introduced in 1980, which gradually \\nevolved into what we now call  convolutional  neural  networks . An important milestone was \\na 1998  paper  by Yann LeCun et al. that introduced the  famous  LeNet -5 architecture, widely used \\nby banks to recognize handwritten check numbers. This architecture has some building blocks \\nthat you already know, such as fully connected layers and sigmoid activation functions, but it \\nalso introduces two new bui lding blocks:  convolutional  layers  and pooling  layers . Let’s look at \\nthem now.  \\nNOTE  \\nWhy not simply use a deep neural network with fully connected layers for image recognition tasks? \\nUnfortunately, although this works fine for small images (e.g., MNIST), it  breaks down for larger images \\nbecause of the huge number of parameters it requires. For example, a 100 × 100 –pixel image has 10,000 \\npixels, and if the first layer has just 1,000 neurons (which already severely restricts the amount of \\ninformation transmitt ed to the next layer), this means a total of 10 million connections. And that’s just the \\nfirst layer. CNNs solve this problem using partially connected layers and weight sharing.  \\nConvolutional Layers  \\nThe most important building block of a CNN is the  convol utional  layer : neurons in the first \\nconvolutional layer are not connected to every single pixel in the input image (like they were in \\nthe layers discussed in previous section s), but only to pixels in their receptive fields \\n(see Figure  2). In turn, each neuron in the second convolutional layer is connected only to \\nneurons located within a small rectangle in the first layer. This  architecture allows the network to \\nconcentrate on small low -level features in the first hidden layer, then assemble them into larger \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 1}),\n",
       " Document(page_content='higher -level features in the next hidden layer, and so on. This hierarchical structure is common in \\nreal-world images, whi ch is one of the reasons why CNNs work so well for image recognition.  \\n \\nFigure  2. CNN layers with rectangular local receptive fields  \\nNOTE  \\nAll the multilayer neural networks we’ve looked at so far had layers composed of a long line of neurons, \\nand we had to  flatten input images to 1D before feeding them to the neural network. In a CNN each layer \\nis represented in 2D, which makes it easier to match neurons with their corresponding inputs.  \\nA neuron located in row  i, column  j of a given layer is connected to th e outputs of the neurons in \\nthe previous layer located in rows  i to i + fh – 1, columns  j to j + fw – 1, where  fh and fw are the \\nheight and width of the receptive field (see  Figure  3). In order for a layer to have the same height \\nand width as the previous layer, it is common to add zeros around the inputs, as shown in the \\ndiagram. This  is called  zero padding . \\n \\nFigure  3. Connections between layers and zero padding  \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 2}),\n",
       " Document(page_content='It is also possible to connect a large input layer to a much smaller layer by spacing out the \\nreceptive fields, as shown in  Figure  4. This dramatically reduces the model’s computational \\ncomplexity. The shift from one receptive field to  the next is called the  stride . In the diagram, a 5 \\n× 7 input layer (plus zero padding) is connected to a 3 × 4 layer, using 3 × 3 receptive fields and \\na stride of 2 (in this example the stride is the same in both directions, but it does not have to be \\nso). A neuron located in row  i, column  j in the upper layer is connected to the outputs of the \\nneurons in t he previous layer located in rows  i × sh to i × sh + fh – 1, \\ncolumns  j × sw to j × sw + fw – 1, where  sh and sw are the vertical and horizontal strides.  \\n \\nFigure  4. Reducing dimensionality using a stride of 2  \\nFilters  \\nA neuron’s weights can be represented a s a small image the size of the receptive field. For \\nexample,  Figure  5 shows two possible sets of weights, called  filters  (or convolution  kernels ). The \\nfirst one is represented as a black square with a vertical white line in the middle (it is a 7 × 7 \\nmatrix full of 0s except for the central column, which is full of 1s); neurons using these weights \\nwill ignore everything in their r eceptive field except for the central vertical line (since all inputs \\nwill get multiplied by 0, except for the ones located in the central vertical line). The second filter \\nis a black square with a horizontal white line in the middle. Once again, neurons u sing these \\nweights will ignore everything in their receptive field except for the central horizontal line.  \\nNow if all neurons in a layer use the same vertical line filter (and the same bias term), and you \\nfeed the network the input image shown in  Figure  5 (the bottom image), the layer will output the \\ntop-left image. Notice that the vertical white lines get enhanced while the rest gets blurred. \\nSimilarly, the upper -right image is what you get if all neurons use the same horizontal line filter; \\nnotice that the horizontal white lines get enhanced while the rest is blurred out. Thus, a layer  full \\nof neurons using the same filter output s a feature  map, which highlights the areas in an image \\nthat activate the filter the most. Of course, you do not have to define the filters manually: instead, \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 3}),\n",
       " Document(page_content='during training the convolutional layer will automatically learn the most useful filters for its task, \\nand the layers above will learn to combine them into more complex  patterns . \\n \\nFigure  5. Applying two different filters to get two feature maps  \\nStacking Multiple Feature Maps  \\nUp to now, for simplicity, I have represented the output of each convolution al layer as a 2D \\nlayer, but in reality a convolutional layer has multiple filters (you decide how many) and outputs \\none feature map per filter, so it is more accurately represented in 3D (see  Figure  6). It has one \\nneuron per pixel in each feature map, and all neurons within a given feature map share the same \\nparameters (i.e., the same weights and bias term). Neurons in d ifferent feature maps use different \\nparameters. A neuron’s receptive field is the same as described earlier, but it extends across all \\nthe previous layers’ feature maps. In short, a convolutional layer simultaneously applies multiple \\ntrainable filters to i ts inputs, making it capable of detecting multiple features anywhere in its \\ninputs.  \\nNOTE  \\nThe fact that all neurons in a feature map share the same parameters dramatically reduces the number of \\nparameters in the model. Once the CNN has learned to recognize a pattern in one location, it can \\nrecognize it in any other location. In contrast, once a regular DNN has learned to recognize a pattern in \\none location, it can recognize it only in that particular location.  \\nInput images are also composed of multiple subla yers: one  per color  channel . There are typically \\nthree: red, green, and blue (RGB). Grayscale images have just one  channel , but some images \\nmay have much more —for example, satellite images that capture extra light frequencies (such as \\ninfrared).  \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 4}),\n",
       " Document(page_content=' \\nFigure  6. Convolutional layers with multiple feature maps, and images with three color channels  \\nSpecifically, a neuron located in row  i, column  j of the feature map  k in a given convolutional \\nlayer  l is connected to the outputs of the neurons in the previous layer  l – 1, located in \\nrows  i × sh to i × sh + fh – 1 and columns  j × sw to j × sw + fw – 1, across all feature maps (in \\nlayer  l – 1). Note that all neurons located in the same row  i and column  j but in different feature \\nmaps are connected to the outputs of th e exact same neurons in the previous layer.  \\nEquation  1 summarizes the preceding explanations in one big mathematic al equation: it shows \\nhow to compute the output of a given neuron in a convolutional layer. It is a bit ugly due to all \\nthe different indices, but all it does is calculate the weighted sum of all the inputs, plus the bias \\nterm.  \\nEquation  1. Computing the ou tput of a neuron in a convolutional layer  \\n \\nIn this equation:  \\n\\uf0b7 zi, j, k is the output of the neuron located in row  i, column  j in feature map  k of the \\nconvolutional layer (layer  l). \\n\\uf0b7 As explained earlier,  sh and sw are the v ertical and horizontal strides,  fh and fw are the \\nheight and width of the receptive field, and  fn′ is the number of feature maps in the \\nprevious layer (layer  l – 1). \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 5}),\n",
       " Document(page_content='\\uf0b7 xi′, j′, k′ is the output of the neuron located in layer  l – 1, row  i′, column  j′, feature  map k′ (or \\nchannel  k′ if the previous layer is the input layer).  \\n\\uf0b7 bk is the bias term for feature map  k (in layer  l). You can think of it as a knob that tweaks \\nthe overall brightness of the feature map  k. \\n\\uf0b7 wu, v, k′ ,k is the connection weight between any n euron in feature map  k of the layer  l and \\nits input located at row  u, column  v (relative to the neuron’s receptive field), and feature \\nmap k′. \\nTensorFlow Implementation  \\nIn TensorFlow, each input image is typically represented as a 3D tensor of shape [ heigh t, width,  \\nchannels ]. A mini -batch is represented as a 4D tensor of shape [ mini-batch  size, height,  width,  \\nchannels ]. The weights of a convolutional layer are represented as a 4D tensor of shape \\n[fh, fw, fn′, fn]. The bias terms of a convolutional layer are  simply represented as a 1D tensor of \\nshape [ fn]. \\nLet’s look at a simple example. The following code loads two sample images, using Scikit -\\nLearn’s  load_sample_image()  (which loads two color images, one of a Chinese temple, and \\nthe other of a flower), then it creates two filters and applies them to both images, and finally it \\ndisplays one of the resulting feature maps. Note that you must pip install the  Pillow  package to \\nuse load_sample_image() . \\nfrom sklearn.datasets  import load_sample_image  \\n \\n# Load sample images \\nchina = load_sample_image (\"china.jpg\" ) / 255 \\nflower = load_sample_image (\"flower.jpg\" ) / 255 \\nimages = np.array([china, flower]) \\nbatch_size , height, width, channels = images.shape \\n \\n# Create 2 filters \\nfilters = np.zeros(shape=(7, 7, channels , 2), dtype=np.float32) \\nfilters[:, 3, :, 0] = 1  # vertical  line \\nfilters[3, :, :, 1] = 1  # horizontal  line \\n \\noutputs = tf.nn.conv2d(images, filters, strides=1, padding=\"SAME\") \\n \\nplt.imshow(outputs[0, :, :, 1], cmap=\"gray\") # plot 1st image\\'s 2nd feature map \\nplt.show() \\nLet’s go through this code:  \\n\\uf0b7 The pixel intensity for each color channel is represented as a byte from 0 to 255, so we \\nscale these features simply by dividing by 255, to get floats ranging from 0 to 1.  \\n\\uf0b7 Then we create two 7 × 7 filters (one with a vertical wh ite line in the middle, and the \\nother with a horizontal white line in the middle).  ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 6}),\n",
       " Document(page_content='\\uf0b7 We apply them to both images using the  tf.nn.conv2d()  function, which is part of \\nTensorFlow’s low -level Deep Learning API. In this example, we use zero padding \\n(padding=\"SAM E\") and a stride of 1.  \\n\\uf0b7 Finally, we plot one of the resulting feature maps (similar to the top -right image \\nin Figure  5). \\nThe tf.nn.conv2d()  line deserves a bit more explanation:  \\n\\uf0b7 images  is the input mini -batch (a 4D tensor, as explained earlier).  \\n\\uf0b7 filters  is the set of filters to apply (also a 4D tensor, as explained earlier).  \\n\\uf0b7 strides  is equal to  1, but it could also be a 1D array  with four elements, where the two \\ncentral elements are the vertical and horizontal strides ( sh and sw). The first and last \\nelements must currently be equal to  1. They may one day be used to specify a batch stride \\n(to skip some instances) and a channel str ide (to skip some of the previous layer’s feature \\nmaps or channels).  \\n\\uf0b7 padding  must be either  \"SAME\"  or \"VALID\" : \\n\\uf0b7 If set to  \"SAME\" , the convolutional layer uses zero padding if necessary. The \\noutput size is set to the number of input neurons divided by the st ride, rounded up. \\nFor example, if the input size is 13 and the stri de is 5 (see  Figure  7), then the \\noutput size is 3  (i.e., 13 / 5 = 2.6, rounded up to 3). Then zeros are added as \\nevenly as possible around the inputs, as needed. When  strides=1 , the layer’s \\noutputs will have the same spatial dimensions (width and height) as its inputs, \\nhence the name  same . \\n\\uf0b7 If set to  \"VALID\", the convolutional layer does  not use zero padding and may \\nignore some rows and columns at the bottom and right of the input image, \\ndepending on the stride, as shown in  Figure  7 (for simplicity , only the horizontal \\ndimension is shown here, but of course the same logic applies to the vertical \\ndimension). This means that every neuron’s receptive field lies strictly wi thin \\nvalid positions inside the input (it does not go out of bounds), hence the \\nname  valid . \\n \\nFigure  7. “SAME” or “VALID” padding (with input width 13, filter width 6, stride 5)  \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 7}),\n",
       " Document(page_content='In this example we manually defined the filters, but in a real CNN you woul d normally define \\nfilters as trainable variables so the neural net can learn which filters work best, as explained \\nearlier. Instead of manually creating the variables, use the  keras.layers.Conv2D  layer:  \\nconv = keras.layers.Conv2D(filters=32, kernel_size =3, strides=1, \\n                           padding=\"same\", activation =\"relu\") \\nThis code creates a  Conv2D  layer with 32 filters, each 3 × 3, using a stride of 1 (both \\nhorizontally and vertically) and  \"same\"  padding, and applying the ReLU activation function to \\nits outputs. As you can see, convolutional layers have quite a few hyperparameters: you must \\nchoose the number of filters, their height and width, the strides, and the padding type. As always, \\nyou can use cross -validation to find the right hyperparameter v alues, but this is very time -\\nconsuming. We will discuss common CNN architectures later, to give you some idea of which \\nhyperparameter values work best in practice.  \\nMemory Requirements  \\nAnother  problem with CNNs is that the convolutional layers require a hug e amount of RAM. \\nThis is especially true during training, because the reverse pass of backpropagation requires all \\nthe intermediate values computed during the forward pass.  \\nFor example, consider a convolutional layer with 5 × 5 filters, outputting 200 feat ure maps of \\nsize 150 × 100, with stride 1 and  \"same\"  padding. If the input is a 150 × 100 RGB image (three \\nchannels), then the number of parameters is (5 × 5 × 3 + 1) × 200 = 15,200 (the + 1 corresponds \\nto the bias terms), which is fairly small compared to  a fully connected layer. 7 However, each of \\nthe 200 feature maps contains 150 × 100 neurons, and each of these neurons needs to compute a \\nweighted sum of its 5 × 5 × 3 = 75 inputs: that’s a total of 225 million float multiplications. Not \\nas bad as a fully connected layer, but still quite computationally intensive. Moreover, if the \\nfeature maps are represented using 32 -bit floats , then the convolutional layer’s output will \\noccupy 200 × 150 × 100 × 32 = 96 million bits (12 MB) of RAM. 8 And that’s just f or one \\ninstance —if a training batch contains 100 instances, then this layer will use up 1.2 GB of RAM!  \\nDuring inference (i.e., when making a prediction for a new instance) the RAM occupied by one \\nlayer can be released as soon as the next layer has been com puted, so you only need as much \\nRAM as required by two consecutive layers. But during training everything computed during the \\nforward pass needs to be preserved for the reverse pass, so the amount of RAM needed is (at \\nleast) the total amount of RAM require d by all layers.  \\nTIP  \\nIf training crashes because of an out -of-memory error, you can try reducing the mini -batch size. \\nAlternatively, you can try reducing dimensionality using a stride, or removing a few layers. Or you can \\ntry using 16 -bit floats instead of  32-bit floats. Or you could distribute the CNN across multiple devices.  \\nNow let’s look at the second common building block of CNNs: the  pooling  layer . ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 8}),\n",
       " Document(page_content='Pooling Layers  \\nOnce  you understand how convolutional layers work, the pooling layers are quite easy to g rasp. \\nTheir  goal is to  subsample  (i.e., shrink) the input image in order to reduce the computational \\nload, the memory usage, and the number of parameters  (thereby limiting the risk of overfitting).  \\nJust like in convolutional layers, each neuron in a poolin g layer is connected to the outputs of a \\nlimited number of neurons in the previous layer, located within a small rectangular receptive \\nfield. You must define its size, the stride, and the padding type, just like before. However, a \\npooling neuron has no weights; all it does is aggregate the inputs using an aggregation function \\nsuch as the max or mean.  Figure  8 shows  a max pooling  layer , which is the most common type \\nof pooling layer. In this example, we use a 2 × 2  pooling  kernel , with a stride o f 2 and no \\npadding. Only the max input value in each receptive field makes it to the next layer, while the \\nother inputs are dropped. For example, in the lower -left receptive field in  Figure  8, the input \\nvalues are 1, 5, 3, 2, so only the max value, 5, is propagated to the next layer. Because of the \\nstride of 2, the output image has half the height and half the width of the input image (rounded \\ndown since we use no padding).  \\n \\nFigure  8. Max pooling layer (2 × 2 pooling kernel, stride 2, no padding)  \\nNOTE  \\nA pooling layer typically works on every input channel independently, so the output depth is the same as \\nthe input depth.  \\nOther  than reducing computations, memory usage, and the number of parameters, a max pooling \\nlayer also introduces some level of  invariance  to small translations, as shown in  Figure  9. Here \\nwe assume that the bright pixels have a lower value than dark pixels, and we consider three \\nimages (A, B, C) going through a max pooling layer with a 2 × 2 kernel and stride 2. Im ages B \\nand C are the same as image A, but shifted by one and two pixels to the right. As you can see, \\nthe outputs of the max pooling layer for images A and B are identical. This is what translation \\ninvariance means. For image C, the output is different: it  is shifted one pixel to the right (but \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 9}),\n",
       " Document(page_content='there is still 50% invariance). By inserting a max pooling layer every few layers in a CNN, it is \\npossible to get some level of translation invariance at a larger scale. Moreover, max pooling \\noffers a small amount of  rotational invariance and a slight scale invariance. Such invariance \\n(even if it is limited) can be useful in cases where the prediction should not depend on these \\ndetails, such as in classification tasks.  \\n \\nFigure  9. Invariance to small translations  \\nHowever, max pooling has some downsides too. Firstly, it is obviously very destructive: even \\nwith a tiny 2 × 2 kernel and a stride of 2, the output will be two times smaller in both directions \\n(so its area will be four times smaller), simply dropping 75% of  the input values. And in some \\napplications, invariance is not desirable. Take  semantic segmentation (the task of classifying \\neach pixel in an image according to the object that pixel belongs to, which we’ll explore later in \\nthis section ): obviously, if th e input image is translated by one pixel to the right, the output \\nshould also be translated by one pixel to the right. The goal in this case is  equivariance , not \\ninvariance: a small change to the inputs should lead to a corresponding small change in the \\noutput. \\nTensorFlow Implementation  \\nImplementing  a max pooling layer in TensorFlow is quite easy. The following code creates a \\nmax pooling layer using a 2 × 2 kernel. The strides default to the kernel size, so this layer will \\nuse a stride of 2 (both horizontal ly and vertically). By default, it uses  \"valid\"  padding (i.e., no \\npadding at all):  \\nmax_pool = keras.layers.MaxPool2D (pool_size =2) \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 10}),\n",
       " Document(page_content='To create an  average  pooling  layer , just use  AvgPool2D  instead of  MaxPool2D . As you might \\nexpect, it works exactly like a max pooling layer, except it computes the mean rather than the \\nmax. Average pooling layers used to be very popular, but people mostly use max pooling layers \\nnow, as they generally perform better. This may seem surprising, since computing the mean \\ngenerally los es less information than computing the max. But on the other hand, max pooling \\npreserves only the strongest features, getting rid of all the meaningless ones, so the next layers \\nget a cleaner signal to work with. Moreover, max pooling offers stronger trans lation invariance \\nthan average pooling, and it requires slightly less compute.  \\nNote that max pooling and average pooling can be performed along the depth dimension rather \\nthan the spatial dimensions, although this is not as common. This can allow the CNN t o learn to \\nbe invariant to various features. For example, it could learn multiple filters, each detecting a \\ndifferent rotation of the same pattern (such as hand -written digits; see  Figure  10), and the \\ndepthwise max pooling layer would ensure that the output is the same regardless of the rotation. \\nThe CNN could similarly learn to be invariant to anything else: thickne ss, brightness, skew, \\ncolor, and so on.  \\n \\nFigure  10. Depthwise max pooling can help the CNN learn any invariance  \\nKeras does not include a depthwise max pooling layer, but TensorFlow’s low -level Deep \\nLearning API does: just use the  tf.nn.max_pool()  funct ion, and specify the kernel size and \\nstrides as 4 -tuples (i.e., tuples of size 4). The first three values of each should be 1: this indicates \\nthat the kernel size and stride along the batch, height, and width dimensions should be 1. The last \\nvalue should b e whatever kernel size and stride you want along the depth dimension —for \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 11}),\n",
       " Document(page_content='example, 3 (this must be a divisor of the input depth; it will not work if the previous layer \\noutputs 20 feature maps, since 20 is not a multiple of 3):  \\noutput = tf.nn.max_pool (images, \\n                        ksize=(1, 1, 1, 3), \\n                        strides=(1, 1, 1, 3), \\n                        padding=\"VALID\") \\nIf you want to include this as a layer in your Keras models, wrap it in a  Lambda  layer (or create a \\ncustom Keras layer):  \\ndepth_pool = keras.layers.Lambda( \\n    lambda X: tf.nn.max_pool (X, ksize=(1, 1, 1, 3), strides=(1, 1, 1, 3), \\n                             padding=\"VALID\")) \\nOne last type of pooling layer that you will often see in modern architectures is the  global  \\naverage  pooling  layer . It works very differently: all it does is compute the mean of each entire \\nfeature map (it’s like an average pooling layer using a pooling kernel with the same spatial \\ndimensions as the inputs). This means that it just outputs a single number p er feature map and \\nper instance. Although this is of course extremely destructive (most of the information in the \\nfeature map is lost), it can be useful as the output layer, as we will see later in this section . To \\ncreate such a layer, simply use the  keras.layers.GlobalAvgPool2D  class:  \\nglobal_avg_pool  = keras.layers.GlobalAvgPool2D () \\nIt’s equivalent to this simple  Lambda  layer, which computes the mean over the spatial \\ndimensions (height and width):  \\nglobal_avg_pool  = keras.layers.Lambda(lambda X: tf.reduce_m ean(X, axis=[1, 2])) \\nNow you know all the building blocks to create convolutional neural networks. Let’s see how to \\nassemble them.  \\nCNN Architectures  \\nTypical CNN  architectures stack a few convolutional layers (each one generally followed by a \\nReLU layer), t hen a pooling layer, then another few convolutional layers (+ReLU), then another \\npooling layer, and so on. The image gets smaller and smaller as it progresses through the \\nnetwork, but it also typically gets deeper and deeper (i.e., with more feature maps),  thanks to the \\nconvolutional layers ( see Figure  11). At the top of the stack, a regular feedforward neural \\nnetwork is added, composed of a few fully connected layers (+ReLUs), and the final layer \\noutputs the prediction (e.g., a softmax layer that outputs estimated class probabilities).  ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 12}),\n",
       " Document(page_content=' \\nFigure  11. Typical CNN architecture  \\nTIP  \\nA common mistake is to use convolution kernels that are too large. For example, instead of using a \\nconvolutional layer with a 5 × 5 kernel, stack two layers with 3 × 3 kernels: it will use fewer parameters \\nand require fewer computations, and it will usually perform better. One exception is for the first \\nconvolutional layer: it can typically have a large kernel (e.g., 5 × 5), usually with a stride of 2 or more: \\nthis will reduce the spatial dimension of the image without losing too much information, and since the \\ninput image only has three channel s in general, it will not be too costly.  \\nHere is how you can implement a simple CNN to tackle the Fashion MNIST dataset : \\nmodel = keras.models.Sequential ([ \\n    keras.layers.Conv2D(64, 7, activation =\"relu\", padding=\"same\", \\n                        input_shape =[28, 28, 1]), \\n    keras.layers.MaxPooling2D (2), \\n    keras.layers.Conv2D(128, 3, activation =\"relu\", padding=\"same\"), \\n    keras.layers.Conv2D(128, 3, activation =\"relu\", padding=\"same\"), \\n    keras.layers.MaxPooling2D (2), \\n    keras.layers.Conv2D(256, 3, activation =\"relu\", padding=\"same\"), \\n    keras.layers.Conv2D(256, 3, activation =\"relu\", padding=\"same\"), \\n    keras.layers.MaxPooling2D (2), \\n    keras.layers.Flatten(), \\n    keras.layers.Dense(128, activation =\"relu\"), \\n    keras.layers.Dropout(0.5), \\n    keras.layers.Dense(64, activation =\"relu\"), \\n    keras.layers.Dropout(0.5), \\n    keras.layers.Dense(10, activation =\"softmax\") \\n]) \\nLet’s go through this model:  \\n\\uf0b7 The first layer uses 64 fairly large filters (7 × 7) but only stride 1 because the input \\nimages are not very large. It also sets  input_shape=[28,  28, 1], because the images are \\n28 × 28 pixels, with a single color chan nel (i.e., grayscale).  \\n\\uf0b7 Next we have a max pooling layer which uses a pool size of 2, so it divides each spatial \\ndimension by a factor of 2.  \\n\\uf0b7 Then we repeat the same structure twice: two convolutional layers followed by a max \\npooling layer. For larger images , we could repeat this structure several more times (the \\nnumber of repetitions is a hyperparameter you can tune).  \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 13}),\n",
       " Document(page_content='\\uf0b7 Note that the number of filters grows as we climb up the CNN toward the output layer (it \\nis initially 64, then 128, then 256): it makes sense for it to grow, since the number of low -\\nlevel features is often fairly low (e.g., small circles, horizontal lines), but there are many \\ndifferent ways to combine them into higher -level features. It is a common practice to \\ndouble the number of filters after each pooling layer: since a pooling layer divides each \\nspatial dimension by a factor of 2, we can afford to double the number of feature maps in \\nthe next layer without fear of exploding the number of parameters, memory usage, or \\ncomputational load.  \\n\\uf0b7 Next is  the fully connected network, composed of two hidden dense layers and a dense \\noutput layer. Note that we must flatten its inputs, since a dense network expects a 1D \\narray of features for each instance. We also add two dropout layers, with a dropout rate of  \\n50% each, to reduce overfitting.  \\nThis CNN reaches over 92% accuracy on the test set. It’s not state of the art, but it is pretty good, \\nand clearly much better than what we achieved with dense networks in previous section . \\nOver the years, variants of this fundamental architecture have been developed, leading to \\namazing advances in the field. A good measure of this progress is the error rate  in competitions \\nsuch as the ILSVRC  ImageNet  challenge . In this competition the top -five error rate for image \\nclassification fell from over 26% to less than 2.3% in just six years. The top -five error rate is the \\nnumber  of test images for which the system’s top five predictions did not include the correct \\nanswer. The images are large (256 pixels high) and there are 1,000 classes, some of which are \\nreally subtle (try distinguishing 120 dog breeds). Looking at the evolutio n of the winning entries \\nis a good way to understand how CNNs work.  \\nWe will first look at the classical LeNet -5 architecture (1998), then three of the winners of the \\nILSVRC challenge: AlexNet (2012), GoogLeNet (2014), and ResNet (2015).  \\nLeNet -5 \\nThe LeNet -5 architecture  is perhaps the most widely known CNN architecture. As mentioned \\nearlier, it was  created by Yann LeCun in 1998 and has been widely used for handwritten digit \\nrecognition (MNIST). It is composed of the layers shown in  Table  1. \\nLayer  Type  Maps  Size Kernel size  Stride  Activation  \\nOut Fully connected  – 10 – – RBF  \\nF6 Fully connected  – 84 – – tanh \\nC5 Convolution  120 1 × 1  5 × 5  1 tanh ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 14}),\n",
       " Document(page_content='Layer  Type  Maps  Size Kernel size  Stride  Activation  \\nS4 Avg pooling  16 5 × 5  2 × 2  2 tanh \\nC3 Convolution  16 10 × 10  5 × 5  1 tanh \\nS2 Avg pooling  6 14 × 14  2 × 2  2 tanh \\nC1 Convolution  6 28 × 28  5 × 5  1 tanh \\nIn Input  1 32 × 32  – – – \\nTable  1. LeNet -5 architecture  \\nThere are a few extra details to be noted:  \\n\\uf0b7 MNIST images are 28 × 28 pixels, but they are zero -padded to 32 × 3 2 pixels and \\nnormalized before being fed to the network. The rest of the network does not use any \\npadding, which is why the size keeps shrinking as the image progresses through the \\nnetwork.  \\n\\uf0b7 The average pooling layers are slightly more complex than usual: e ach neuron computes \\nthe mean of its inputs, then multiplies the result by a learnable coefficient (one per map) \\nand adds a learnable bias term (again, one per map), then finally applies the activation \\nfunction.  \\n\\uf0b7 Most neurons in C3 maps are connected to neur ons in only three or four S2 maps (instead \\nof all six S2 maps). See table  1 (page 8) in the original paper  for details.  \\n\\uf0b7 The output lay er is a bit special: instead of computing the matrix multiplication of the \\ninputs and the weight vector, each neuron outputs the square of the Euclidian distance \\nbetween its input vector and its weight vector. Each output measures how much the \\nimage belong s to a particular digit class. The cross -entropy cost function is now \\npreferred, as it penalizes bad predictions much more, producing larger gradients and \\nconverging faster.  \\nYann LeCun’s  website  features great demos of LeNet -5 classifying digits.  \\nAlexNet  \\nThe AlexNet  CNN  architecture  won the 2012 ImageNet ILSVRC challenge by a large margin: it \\nachieved a top -five error rate of 17%, while the second best achieved only 26%! It was \\ndeveloped by Alex Krizhevsky (hence the name), Ilya Sutskever, and Geoffrey Hinton. It is \\nsimilar to LeNet -5, only much larger and deeper, and it was the first to stack convolutional layers ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 15}),\n",
       " Document(page_content='directly on top of one another, instead of stacking a pooling layer on top of each convolutional \\nlayer.  Table  2 presents this architecture.  \\nLaye\\nr Type  Maps  Size Kerne\\nl size  Strid\\ne Paddin\\ng Activatio\\nn \\nOut Fully \\nconnected  – 1,00\\n0 – – – Softmax  \\nF10 Fully \\nconnected  – 4,09\\n6 – – – ReLU  \\nF9 Fully  \\nconnected  – 4,09\\n6 – – – ReLU  \\nS8 Max \\npooling  256 6 × 6  3 × 3  2 valid  – \\nC7 Convolutio\\nn 256 13 × \\n13 3 × 3  1 same  ReLU  \\nC6 Convolutio\\nn 384 13 × \\n13 3 × 3  1 same  ReLU  \\nC5 Convolutio\\nn 384 13 × \\n13 3 × 3  1 same  ReLU  \\nS4 Max \\npooling  256 13 × \\n13 3 × 3  2 valid  – \\nC3 Convolutio\\nn 256 27 × \\n27 5 × 5  1 same  ReLU  \\nS2 Max \\npooling  96 27 × \\n27 3 × 3  2 valid  – \\nC1 Convolutio\\nn 96 55 × \\n55 11 × \\n11 4 valid  ReLU  ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 16}),\n",
       " Document(page_content='Laye\\nr Type  Maps  Size Kerne\\nl size  Strid\\ne Paddin\\ng Activatio\\nn \\nIn Input  3 \\n(RGB\\n) 227 \\n× \\n227 – – – – \\nTable  2. AlexNet architecture  \\nTo reduce overfitting, the authors used two regula rization techniques . First, they applied dropout  \\nwith a 50% dropout rate during training to the outputs  of layers F9 and F10. Second, they \\nperformed  data augmentation  by randomly shifting the training images by various offsets, \\nflipping them horizontally, and changing the lighting conditions.  \\nDATA AUGMENTATION  \\nData augmentation artificially increases the si ze of the training set by generating many realistic \\nvariants of each training instance. This reduces overfitting, making this a regularization \\ntechnique. The generated instances should be as realistic as possible: ideally, given an image \\nfrom the augmented  training set, a human should not be able to tell whether it was augmented or \\nnot. Simply adding white noise will not help; the modifications should be learnable (white noise \\nis not).  \\nFor example, you can slightly shift, rotate, and resize every picture in  the training set by various \\namounts and add the resulting pictures to the training set (see  Figure  12). This forc es the model \\nto be more tolerant to variations in the position, orientation, and size of the objects in the \\npictures. For a model that’s more tolerant of different lighting conditions, you can similarly \\ngenerate many images with various contrasts. In gener al, you can also flip the pictures \\nhorizontally (except for text, and other asymmetrical objects). By combining these \\ntransformations, you can greatly increase the size of your training set.  \\n \\nFIGURE  12. GENERATING NEW TRAIN ING INSTANCES FROM E XISTING O NES  \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 17}),\n",
       " Document(page_content='AlexNet  also uses a competitive normalization step immediately after the ReLU step of layers \\nC1 and C3, called  local  response  normalization  (LRN): the most strongly activated neurons \\ninhibit other neurons located at the same position in neighboring fea ture maps (such competitive \\nactivation has been observed in biological neurons). This encourages different feature maps to \\nspecialize, pushing them apart and forcing them to explore a wider range of features, ultimately \\nimproving generalization.  Equation  2 shows  how to apply LRN.  \\nEquation  2. Local response normalization (LRN)  \\n \\nIn this equation:  \\n\\uf0b7 bi is the normalized output of the neuron located in feature map  i, at some row  u and \\ncolumn  v (note that in this equation we consider only neurons located at this row and \\ncolumn, so  u and v are not shown).  \\n\\uf0b7 ai is the activation of that neuron after the ReLU step, but before normalization.  \\n\\uf0b7 k, α, β, and  r are hyperparameters.  k is called the  bias, and  r is called the  depth  radius . \\n\\uf0b7 fn is the number of feature maps.  \\nFor example, if  r = 2 and a neuron has a strong activation, it will inhibit the activation of the \\nneurons located in the feature maps immediately above and below its own.  \\nIn AlexNet, the hyperparameters are set as follows:  r = 5, α = 0.0001,  β = 0.75, and  k = 2. This \\nstep can be implemented using the  tf.nn.local_response_normalization()  function \\n(which you can wrap in a  Lambda  layer if you want to use it in a Keras model).  \\nA variant of AlexNet called  ZF Net was developed by Matthew Zeiler and Rob Fergus and won \\nthe 2013 ILSVRC challenge. It is essentially AlexNet with a few tweaked hyperparameters \\n(numbe r of feature maps, kernel size, stride, etc.).  \\nGoogLeNet  \\nThe GoogLeNet  architecture  was developed by Christian Szegedy et al. from Google \\nResearch,  and it won the ILSVRC 2014 challenge by pushing the top -five error rate below 7%. \\nThis great performance came in large part from the fact that the network was much deeper than \\nprevious CNNs (as you’ll see in  Figure  14). This was made possible by subnetworks \\ncalled  inception  modules , which allow GoogLeNet to use parameters much more efficiently than \\nprevious architectures: GoogLeNet actually has 10 times fewer parameters than AlexNet \\n(roughly 6 million instead of 60 million).  \\nFigure  13 shows the architecture of an inception module. The notation “3 ×  3 + 1(S)” means that \\nthe layer uses a 3 × 3 kernel, stride 1, and  \"same\"  padding. The input signal is first copied and \\nfed to four different layers. All convolutional layers use the ReLU activation function. Note that \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 18}),\n",
       " Document(page_content='the second set of convolutional layer s uses different kernel sizes (1 × 1, 3 × 3, and 5 × 5), \\nallowing them to capture patterns at different scales. Also note that every single layer uses a \\nstride of 1 and  \"same\"  padding (even the max pooling layer), so their outputs all have the same \\nheight and width as their inputs. This makes it possible to concatenate all the outputs along the \\ndepth dimension  in the final  depth  concatenation  layer  (i.e., stack the feature maps from all four \\ntop convolutional layers). This concatenation layer can be impleme nted in TensorFlow using \\nthe tf.concat()  operation, with  axis=3  (the axis is the depth).  \\n \\nFigure  13. Inception module  \\nYou may wonder why inception modules have convolutional layers with 1 × 1 kernels. Surely \\nthese layers cannot capture any features bec ause they look at only one pixel at a time? In fact, the \\nlayers serve three purposes:  \\n\\uf0b7 Although they cannot capture spatial patterns, they can capture patterns along the depth \\ndimension.  \\n\\uf0b7 They  are configured to output fewer feature maps than their inputs, so  they serve \\nas bottleneck  layers , meaning they reduce dimensionality. This cuts the computational \\ncost and the number of parameters, speeding up training and improving generalization.  \\n\\uf0b7 Each pair of convolutional layers ([1 × 1, 3 × 3] and [1 × 1, 5 × 5]) ac ts like a single \\npowerful convolutional layer, capable of capturing more complex patterns. Indeed, \\ninstead of sweeping a simple linear classifier across the image (as a single convolutional \\nlayer does), this pair of convolutional layers sweeps a two -layer neural network across \\nthe image.  \\nIn short, you can think of the whole inception module as a convolutional layer on steroids, able \\nto output feature maps that capture complex patterns at various scales.  \\nWARNING  \\nThe number of convolutional kernels for each c onvolutional layer is a hyperparameter. Unfortunately, this \\nmeans that you have six more hyperparameters to tweak for every inception layer you add.  \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 19}),\n",
       " Document(page_content='Now let’s look at the architecture of the GoogLeNet CNN ( see Figure  14). The number of \\nfeature maps output by each convolutional layer and each pooling layer is shown before the \\nkernel size. The architecture is so deep that it ha s to be represented in three columns, but \\nGoogLeNet is actually one tall stack, including nine inception modules (the boxes with the \\nspinning tops). The six numbers in the inception modules represent the number of feature maps \\noutput by each convolutional layer in the module (in the same order as in  Figure  13). Note that \\nall the convolutional layers use the ReLU activa tion function.  \\n \\nFigure  14. GoogLeNet architecture  \\nLet’s go through this network:  \\n\\uf0b7 The first two layers divide the image’s height and width by 4 (so its area is divided by \\n16), to reduce the computational load. The first layer uses a large kernel size so  that much \\nof the information is preserved.  \\n\\uf0b7 Then the local response normalization layer ensures that the previous layers learn a wide \\nvariety of features (as discussed earlier).  \\n\\uf0b7 Two convolutional layers follow, where the first acts like a bottleneck layer.  As \\nexplained earlier, you can think of this pair as a single smarter convolutional layer.  \\n\\uf0b7 Again, a local response normalization layer ensures that the previous layers capture a \\nwide variety of patterns.  \\n\\uf0b7 Next, a max pooling layer reduces the image height a nd width by 2, again to speed up \\ncomputations.  \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 20}),\n",
       " Document(page_content='\\uf0b7 Then comes the tall stack of nine inception modules, interleaved with a couple max \\npooling layers to reduce dimensionality and speed up the net.  \\n\\uf0b7 Next, the global average pooling layer outputs the mean of each feature map: this drops \\nany remaining spatial information, which is fine because there was not much spatial \\ninformation left at that point. Indeed, GoogLeNet input images are typically expected to \\nbe 224 × 224 pixels, so after 5 max pooling layers, each di viding the height and width by \\n2, the feature maps are down to 7 × 7. Moreover, it is a classification task, not \\nlocalization, so it does not matter where the object is. Thanks to the dimensionality \\nreduction brought by this layer, there is no need to have  several fully connected layers at \\nthe top of the CNN (like in AlexNet), and this considerably reduces the number of \\nparameters in the network and limits the risk of overfitting.  \\n\\uf0b7 The last layers are self -explanatory: dropout for regularization, then a full y connected \\nlayer with 1,000 units (since there are 1,000 classes) and a softmax activation function  to \\noutput estimated class probabilities.  \\nThis diagram is slightly simplified: the original GoogLeNet architecture also included two \\nauxiliary classifiers p lugged on top of the third and sixth inception modules. They were both \\ncomposed of one average pooling layer, one convolutional layer, two fully connected layers, and \\na softmax activation layer. During training, their loss (scaled down by 70%) was added to  the \\noverall loss. The goal was to fight the vanishing gradients problem and regularize the network. \\nHowever, it was later shown that their effect was relatively minor.  \\nSeveral variants of the GoogLeNet architecture were later proposed by Google researcher s, \\nincluding Inception -v3 and Inception -v4, using slightly different inception modules and reaching \\neven better performance.  \\nVGGNet  \\nThe runner -up in the ILSVRC 2014 challenge was  VGGNet , developed by Karen Simonyan and \\nAndrew Zisserman from the Visual Geometry Group (VGG) research lab at Oxford University. \\nIt had a very simple and classical  architecture, with 2 or 3 convolutional layers and a pooling \\nlayer, then again 2 or 3 convolutional layers and a pooling layer, and so on (reaching a total of \\njust 16 or 19 convolutional layers, depending on the VGG variant), plus a final dense network \\nwith 2 hidden layers and the output layer. It used only 3 × 3 filters, but many filters.  \\nResNet  \\nKaiming He et al. won  the ILSVRC 2015 challenge using a  Residual  Network  (or ResNet ), that \\ndelivered an astounding top -five error rate under 3.6%. The winning variant used an extremely \\ndeep CNN composed of 152 layers (other variants had 34, 50, and  101 layers). It confirmed the \\ngeneral trend: models are getting deeper and deeper, with fewer and fewer parameters. The key \\nto being able to train such a deep network is to  use skip connections  (also called  shortcut  \\nconnections ): the signal feeding into a  layer is also added to the output of a layer located a bit \\nhigher up the stack. Let’s see why this is useful.  ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 21}),\n",
       " Document(page_content='When training a neural network, the goal is to make it model a target function  h(x). If you add \\nthe input  x to the output of the network (i.e., y ou add a skip connection), then the network will be \\nforced to model  f(x) = h(x) – x rather than  h(x). This  is called  residual  learning  (see Figure  15). \\n \\nFigure  15. Residual learning  \\nWhen you initialize a regular neural network, its weights are close to zero, so the network just \\noutputs values close to zero. If you add a skip connection, the resulting network just outputs a \\ncopy of its inputs; in other words, it initially models the identity function. If the target function is \\nfairly close to the identity function (which is often the case), this will speed up training \\nconsiderably.  \\nMoreover, if you add many skip con nections, the network can start making progress even if \\nseveral layers have not started learning yet (see  Figure  16). Thanks to skip connections, the \\nsignal can easily make its way across the whole network. The deep residual network can be seen \\nas a stack of  residual  units  (RUs), where each residual unit is a small neural network with a skip \\nconnection.  \\n \\nFigure  16. Regular deep neural network (left) and deep residual network (right)  \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 22}),\n",
       " Document(page_content='Now let’s look at ResNet’s architecture ( see Figure  17). It is surprisingly simple. It starts and \\nends exactly like GoogLeNet (except without a dropout layer), and in between is just a very deep \\nstack of simple residual units. Each residual unit is composed of two convolutional layers (and \\nno pooling laye r!), with Batch Normalization (BN) and ReLU activation, using 3 × 3 kernels and \\npreserving spatial dimensions (stride 1,  \"same\"  padding).  \\n \\nFigure  17. ResNet architecture  \\nNote that the number of feature maps is doubled every few residual units, at the s ame time as \\ntheir height and width are halved (using a convolutional layer with stride 2). When this happens, \\nthe inputs cannot be added directly to the outputs of the residual unit because they don’t have the \\nsame shape (for example, this problem affects the skip connection represented by the dashed \\narrow in  Figure  17). To solve this problem, the inputs are passed through a 1 ×  1 convolutional \\nlayer with stride 2 and the right number of output feature maps (see  Figure  18). \\n \\nFigure  18. Skip connection when changing feature map size and depth  \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 23}),\n",
       " Document(page_content='ResNet -34 is the ResNet with 34 layers (only counting the convolutional layers and the fully \\nconnected layer)  containing 3 residual units that output 64 feature maps, 4 RUs with 128 maps, 6 \\nRUs with 256 maps, and 3 RUs with 512 maps. We will implement this architecture later in this \\nsection . \\nResNets deeper than that , such as ResNet -152, use slightly different residual units. Instead of \\ntwo 3 × 3 convolutional layers with, say, 256 feature maps, they use three convolutional layers: \\nfirst a 1 × 1 convolutional layer with just 64 feature maps (4 times less), which acts as a \\nbottleneck layer (as discussed already), then a 3 × 3 layer with 64 feature maps, and finally \\nanother 1 × 1 convolutional layer with 256 feature maps (4 times 64) that restores the original \\ndepth. ResNet -152 contains 3 such RUs that output 256 maps, t hen 8 RUs with 512 maps, a \\nwhopping 36 RUs with 1,024 maps, and finally 3 RUs with 2,048 maps.  \\nNOTE  \\nGoogle’s  Inception -v4 architecture merged the ideas of GoogLeNet and ResNet and achieved a top -five \\nerror rate of close to 3% on ImageNet classification.  \\nXception  \\nAnother  variant of the GoogLeNet architecture is worth noting:  Xception  (which stands \\nfor Extreme  Inception ) was proposed in 2016 by François Chollet (the auth or of Keras), and it \\nsignificantly outperformed Inception -v3 on a huge vision task (350 million images and 17,000 \\nclasses). Just like Inception -v4, it merges the ideas of GoogLeNet and ResNet, but it replaces \\nthe inception modules with a special type of la yer called a  depthwise  separable  convolution  \\nlayer  (or separable  convolution  layer  for short ). These layers had been used b efore in some CNN \\narchitectures, but they were not as central as in the Xception architecture. While a regular \\nconvolutional layer uses filters that try to simultaneously capture spatial patterns (e.g., an oval) \\nand cross -channel patterns (e.g., mouth + no se + eyes = face), a separable convolutional layer \\nmakes the strong assumption that spatial patterns and cross -channel patterns can be modeled \\nseparately (see  Figure  19). Thus, it is composed of two parts: the first part applies a single spatial \\nfilter for each input feature map, then the second part looks exclusively for cross -channel \\npatterns —it is just a regula r convolutional layer with 1 × 1 filters.  \\n \\nFigure  19. Depthwise separable convolutional layer  \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 24}),\n",
       " Document(page_content='Since separable convolutional layers only have one spatial filter per input channel, you should \\navoid using them after layers that have too few channels, such  as the input layer (granted, that’s \\nwhat  Figure  19 represents, but it is just for illustration purposes). For  this reason, the Xception \\narchitecture starts with 2 regular convolutional layers, but then the rest of the architecture uses \\nonly separable convolutions (34 in all), plus a few max pooling layers and the usual final layers \\n(a global average pooling layer  and a dense output layer).  \\nYou might wonder why Xception is considered a variant of GoogLeNet, since it contains no \\ninception module at all. Well, as we discussed earlier, an inception module contains \\nconvolutional layers with 1 × 1 filters: these look ex clusively for cross -channel patterns. \\nHowever, the convolutional layers that sit on top of them are regular convolutional layers that \\nlook both for spatial and cross -channel patterns. So you can think of an inception module as an \\nintermediate between a reg ular convolutional layer (which considers spatial patterns and cross -\\nchannel patterns jointly) and a separable convolutional layer (which considers them separately). \\nIn practice, it seems that separable convolutional layers generally perform better.  \\nTIP  \\nSeparable convolutional layers use fewer parameters, less memory, and fewer computations than regular \\nconvolutional layers, and in general they even perform better, so you should consider using them by \\ndefault (except after layers with few channels).  \\nThe ILS VRC 2016 challenge was won by the CUImage team from the Chinese University of \\nHong Kong. They used an ensemble of many different techniques, including a sophisticated \\nobject -detection system called  GBD -Net, to achieve a top -five error rate below 3%. Although \\nthis result is unquestionably impressive, the complexity of the solution contra sted with the \\nsimplicity of ResNets. Moreover, one year later another fairly simple architecture performed \\neven better, as we will see now.  \\nSENet  \\nThe winning architecture in the ILSVRC 2017 challenge was the  Squeeze -and-Excitation  \\nNetwork  (SENet) . This architecture extends existing architectures such as inception networks \\nand ResNets, an d boosts their performance. This allowed SENet to win the competition with an \\nastonishing 2.25% top -five error rate! The extended versions of inception networks  and ResNets \\nare called  SE-Inception  and SE-ResNet , respectively. The boost comes from the fact that a SENet \\nadds a small neural network, called an  SE block , to every unit in the original architecture (i.e., \\nevery inception module or every residual unit), as shown in  Figure  20. ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 25}),\n",
       " Document(page_content=' \\nFigure  20. SE-Inception module (left) and SE -ResNet unit (right)  \\nAn SE block analyzes the output of the unit it is attached to, focusing exclusively on the depth \\ndimension (it does not look for any spatial pattern), and it learns which features are usually most \\nactive together. It then uses this information to recalibrate the feature maps, as shown \\nin Figure  21. For example, an SE block may learn that mouths, noses, and eyes usually appear \\ntogether in pictures: if you see a mouth and a nose, you should expect to see eyes as well. So if \\nthe block sees a strong ac tivation in the mouth and nose feature maps, but only mild activation in \\nthe eye feature map, it will boost the eye feature map (more accurately, it will reduce irrelevant \\nfeature maps). If the eyes were somewhat confused with something else, this feature map \\nrecalibration will help resolve the ambiguity.  \\n \\nFigure  21. An SE block performs feature map recalibration  \\nAn SE block is composed of just three layers: a global average pooling layer, a hidden dense \\nlayer using the ReLU activation function, and a d ense output layer using the sigmoid activation \\nfunction (see  Figure  22). \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 26}),\n",
       " Document(page_content=' \\nFigure  22. SE block architecture  \\nAs earlier, t he global average pooling layer computes the mean activation for each feature map: \\nfor example, if its input contains 256 feature maps, it will output 256  numbers  representing the \\noverall level of response for each filter. The next layer is where the “sque eze” happens: this layer \\nhas significantly fewer than 256 neurons —typically 16 times fewer than the number of feature \\nmaps (e.g., 16 neurons) —so the 256 numbers get compressed into a small vector (e.g., 16 \\ndimensions). This is a low -dimensional vector repr esentation (i.e., an embedding) of the \\ndistribution of feature responses. This bottleneck step forces the SE block to learn a general \\nrepresentation of the feature combinations (we will see this principle in action again when we \\ndiscuss autoencoders in later  section ). Finally, the output layer takes the embedding and outputs \\na recalibration vector containing one number per featu re map (e.g., 256), each between 0 and 1. \\nThe feature maps are then multiplied by this recalibration vector, so irrelevant features (with a \\nlow recalibration score) get scaled down while relevant features (with a recalibration score close \\nto 1) are left al one. \\nImplementing a ResNet -34 CNN Using Keras  \\nMost  CNN architectures described so far are fairly straightforward to implement (although \\ngenerally you would load a pretrained network instead, as we will see). To illustrate the process, \\nlet’s implement a Res Net-34 from scratch using Keras. First, let’s create \\na ResidualUnit  layer:  \\nclass ResidualUnit (keras.layers.Layer): \\n    def __init__ (self, filters, strides=1, activation =\"relu\", **kwargs): \\n        super().__init__ (**kwargs) \\n        self.activation  = keras.activations .get(activation ) \\n        self.main_layers  = [ \\n            keras.layers.Conv2D(filters, 3, strides=strides, \\n                                padding=\"same\", use_bias =False), \\n            keras.layers.BatchNormalization (), \\n            self.activation , \\n            keras.layers.Conv2D(filters, 3, strides=1, \\n                                padding=\"same\", use_bias =False), \\n            keras.layers.BatchNormalization ()] \\n        self.skip_layers  = [] \\n        if strides > 1: \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 27}),\n",
       " Document(page_content='            self.skip_layers  = [ \\n                keras.layers.Conv2D(filters, 1, strides=strides, \\n                                    padding=\"same\", use_bias =False), \\n                keras.layers.BatchNormalization ()] \\n \\n    def call(self, inputs): \\n        Z = inputs \\n        for layer in self.main_layers : \\n            Z = layer(Z) \\n        skip_Z = inputs \\n        for layer in self.skip_layers : \\n            skip_Z = layer(skip_Z) \\n        return self.activation (Z + skip_Z) \\nAs you can see, this code matches  Figure  18 pretty  closely. In the constructor, we create all the \\nlayers we will need: the main layers are the ones on the right side of the diagram, a nd the skip \\nlayers are the ones on the left (only needed if the stride is greater than 1). Then in \\nthe call()  method, we make the inputs go through the main layers and the skip layers (if any), \\nthen we add both outputs and apply the activation function.  \\nNext, we can build the ResNet -34 using a  Sequential  model, since it’s really just a long \\nsequence of layers (we can treat each residual unit as a single layer now that we have \\nthe ResidualUnit  class):  \\nmodel = keras.models.Sequential () \\nmodel.add(keras.layers.Conv2D(64, 7, strides=2, input_shape =[224, 224, 3], \\n                              padding=\"same\", use_bias =False)) \\nmodel.add(keras.layers.BatchNormalization ()) \\nmodel.add(keras.layers.Activation (\"relu\")) \\nmodel.add(keras.layers.MaxPool2D (pool_size =3, strides=2, padding=\"same\")) \\nprev_filters  = 64 \\nfor filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3: \\n    strides = 1 if filters == prev_filters  else 2 \\n    model.add(ResidualUnit (filters, strides=strides)) \\n    prev_filters  = filters \\nmodel.add(keras.layers.GlobalAvgPool2D ()) \\nmodel.add(keras.layers.Flatten()) \\nmodel.add(keras.layers.Dense(10, activation =\"softmax\" )) \\nThe only slightly tricky part in this code is the loop that adds the  ResidualUnit  layers to the \\nmodel: as explained earlier, the first 3 RUs have 64 filters, then the next 4 RUs have 128 filters, \\nand so on. We then set the stride to 1 when the number of filters is the same as in the previous \\nRU, or else we set it to 2. Then we add the  ResidualUnit , and finally we \\nupdate  prev_filters . \\nIt is amazing that  in fewer than 40 lines of code, we can build the model that won the ILSVRC \\n2015 challenge! This demonstrates both the elegance of the ResNet model and the \\nexpressiveness of the Keras API. Implementing the other CNN architectures is not much harder. \\nHoweve r, Keras comes with several of these architectures built in, so why not use them instead?  ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 28}),\n",
       " Document(page_content='Using Pretrained Models from Keras  \\nIn general, you won’t have to implement standard models like GoogLeNet or ResNet manually, \\nsince pretrained networks are readily av ailable with a single line of code in \\nthe keras.applications  package. For example, you can load the ResNet -50 model, \\npretrained on ImageNet, with the following line of code:  \\nmodel = keras.applications .resnet50 .ResNet50 (weights=\"imagenet\" ) \\nThat’s all! This will create a ResNet -50 model and download weights pretrained on the \\nImageNet dataset. To use it, you first need to ensure that the images have the right size. A \\nResNet -50 model expects 224 × 224 -pixel images (other models may expect other sizes, such as \\n299 × 299), so let’s use TensorFlow’s  tf.image.resize()  function to resize the images we \\nloaded earlier:  \\nimages_resized  = tf.image.resize(images, [224, 224]) \\nTIP  \\nThe tf.image.resize()  will not preserve the aspect ratio. If this is a problem, try cropping th e images \\nto the appropriate aspect ratio before resizing. Both operations can be done in one shot \\nwith tf.image.crop_and_resize() . \\nThe pretrained models assume that the images are preprocessed in a specific way. In some cases \\nthey may expect the inputs to be scaled from 0 to 1, or –1 to 1, and so on. Each model provides \\na preprocess_input()  function that you can use to preprocess your images. These functions \\nassume that the pixel values range from 0 to 255, so we must multiply them by 255 (since earlier \\nwe scaled them to the 0 –1 range):  \\ninputs = keras.applications .resnet50 .preprocess_input (images_resized  * 255) \\nNow we can use the pretrained model to make predictions:  \\nY_proba = model.predict(inputs) \\nAs usual, the output  Y_proba  is a matrix with one row per im age and one column per class (in \\nthis case, there are 1,000 classes). If you want to display the top  K predictions, including the \\nclass name and the estimated probability of each predicted class, use \\nthe decode_predictions()  function. For each image, it re turns an array containing the \\ntop K predictions, where each prediction is represented as an array containing the class \\nidentifier,  its name, and the corresponding confidence score:  \\ntop_K = keras.applications .resnet50 .decode_predictions (Y_proba, top=3) \\nfor image_index  in range(len(images)): \\n    print(\"Image #{}\".format(image_index )) \\n    for class_id , name, y_proba in top_K[image_index]: \\n        print(\"  {} - {:12s} {:.2f}%\" .format(class_id , name, y_proba * 100)) ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 29}),\n",
       " Document(page_content='    print() \\nThe output looks like this:  \\nImage #0  \\n  n03877845 - palace       42.87%  \\n  n02825657 - bell_cote    40.57%  \\n  n03781244 - monastery    14.56%  \\n \\nImage #1  \\n  n04522168 - vase         46.83%  \\n  n07930864 - cup          7.78%  \\n  n11939491 - daisy        4.87%  \\nThe correct classes (monastery and daisy) appear in the top three results for both images. That’s \\npretty good, considering that the model had to choose from among 1 ,000 classes.  \\nAs you can see, it is very easy to create a pretty good image classifier using a pretrained model. \\nOther vision models are available in  keras.applications , including several ResNet variants, \\nGoogLeNet variants like Inception -v3 and Xception, VGGNet variants, and MobileNet and \\nMobileNetV2 (lightweight models for use in mobile applications).  \\nBut what if you want to use an image classifier for classes of images that are not part of \\nImageNet? In that case, you may still benefit from the pretrained  models to perform transfer \\nlearning.  \\nPretrained Models for Transfer Learning  \\nIf you want to build an image classifier but you do not have enough training data, then it is often \\na good idea to reuse the low er layers of a pretrained model . For example, let’s train a model to \\nclassify pictures of flowers, reusing a pretrained Xception model. First, let’s load the dat aset \\nusing TensorFlow Datasets : \\nimport tensorflow_datasets  as tfds \\n \\ndataset, info = tfds.load(\"tf_flowers\", as_supervised =True, with_info =True) \\ndataset_size  = info.splits[\"train\"].num_examples  # 3670 \\nclass_names  = info.features [\"label\"].names # [\"dandelion\",  \"daisy\",  ...] \\nn_classes  = info.features [\"label\"].num_classes  # 5 \\nNote that you can get information  about the dataset by setting  with_info=True . Here, we get \\nthe dataset size and the names of the classes. Unfortunately, there is only a  \"train\"  dataset, no \\ntest set or validation set, so we need to split the training set. The TF Datasets project provides an \\nAPI for this. For example, let’s take the first 10% of the dataset for testing, the next 15% for \\nvalidation, and the remaining 75% for  training : \\ntest_set_raw , valid_set_raw , train_set_raw  = tfds.load( \\n    \"tf_flowers\" , ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 30}),\n",
       " Document(page_content='    split=[\"train[:10%]\" , \"train[1 0%:25%]\" , \"train[25%:]\" ], \\n    as_supervised =True) \\nNext we must preprocess the images. The CNN expects 224 × 224 images, so we need to resize \\nthem. We also need to run the images through Xception’s  preprocess_input()  function:  \\ndef preprocess (image, label): \\n    resized_image  = tf.image.resize(image, [224, 224]) \\n    final_image  = keras.applications .xception .preprocess_input (resized_image ) \\n    return final_image , label \\nLet’s apply this preprocessing function to all three datasets, shuffle the training set, and add \\nbatching and prefetching to all the datasets:  \\nbatch_size  = 32 \\ntrain_set  = train_set .shuffle(1000) \\ntrain_set  = train_set .map(preprocess ).batch(batch_size ).prefetch (1) \\nvalid_set  = valid_set .map(preprocess ).batch(batch_size ).prefetch (1) \\ntest_set = test_set.map(preprocess ).batch(batch_size ).prefetch (1) \\nIf you want to perform some data augmentation, change the preprocessing function for the \\ntraining set, adding some random transformations to the training images. For example, \\nuse tf.image.random_crop()  to ran domly crop the images, \\nuse tf.image.random_flip_left_right()  to randomly flip the images horizontally, and \\nso on (see the “Pretrained Models for Transfer Learning” section of the notebook for an \\nexample).  \\nTIP  \\nThe keras.preprocessing.image.ImageDataGenerato r class makes it easy to load images from \\ndisk and augment them in various ways: you can shift each image, rotate it, rescale it, flip it horizontally \\nor vertically, shear it, or apply any transformation function you want to it. This is very convenient for  \\nsimple projects. However, building a tf.data pipeline has many advantages: it can read the images \\nefficiently (e.g., in parallel) from any source, not just the local disk; you can manipulate the  Dataset  as \\nyou wish; and if you write a preprocessing functi on based on  tf.image  operations, this function can be \\nused both in the tf.data pipeline and in the model you will deplo y to production . \\nNext let’s load an Xception model, pretrained on ImageNet. We exclude the top of the network \\nby setting  include_top=False : this excludes the global average pooling layer and the dense \\noutput layer. We then add our own global av erage pooling layer, based on the output of the base \\nmodel, followed by a dense output layer with one unit per class, using the  softmax activation \\nfunction. Finally, we create the Keras  Model : \\nbase_model  = keras.applications .xception .Xception (weights=\"imagenet\", \\n                                                  include_top =False) \\navg = keras.layers.GlobalAveragePooling2D ()(base_model .output) \\noutput = keras.layers.Dense(n_classes , activation =\"softmax\" )(avg) \\nmodel = keras.Model(inputs=base_model .input, outputs=output) ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 31}),\n",
       " Document(page_content='As explained before , it’s usually a good idea to freeze the weights of the pretrained layers, at \\nleast at the beg inning of training:  \\nfor layer in base_model .layers: \\n    layer.trainable  = False \\nNOTE  \\nSince our model uses the base model’s layers directly, rather than the  base_model  object itself, \\nsetting  base_model.trainable=False  would have no effect.  \\nFinally, we can c ompile the model and start training:  \\noptimizer  = keras.optimizers .SGD(lr=0.2, momentum =0.9, decay=0.01) \\nmodel.compile(loss=\"sparse_categorical_crossentropy\" , optimizer =optimizer , \\n              metrics=[\"accuracy\" ]) \\nhistory = model.fit(train_set , epochs=5, validation_data =valid_set ) \\nWARNING  \\nThis will be very slow, unless you have a GPU. If you do not, then you should run this code in Colab, \\nusing a GPU runtime (it’s free!).  \\nAfter training the model for a few epochs, its validation accuracy should reach about 75 –80% \\nand stop making much progress. This means that the top layers are now pretty well trained, so \\nwe are ready to unfreez e all the layers (or you could try unfreezing just the top ones) and \\ncontinue training (don’t forget to compile the model when you freeze or unfreeze layers). This \\ntime we use a much lower learning rate to avoid damaging the pretrained weights:  \\nfor layer in base_model .layers: \\n    layer.trainable  = True \\n \\noptimizer  = keras.optimizers .SGD(lr=0.01, momentum =0.9, decay=0.001) \\nmodel.compile(...) \\nhistory = model.fit(...) \\nIt will take a while, but this model should reach around 95% accuracy on the test set. With th at, \\nyou can start training amazing image classifiers! But there’s more to computer vision than just \\nclassification. For example, what if you also want to know  where  the flower is in the picture? \\nLet’s look at this now.  \\nClassification and Localization  \\nLocal izing  an object in a picture can be expressed as a regression task, as discussed  before : to \\npredict a bounding box around th e object, a common approach is to predict the horizontal and \\nvertical coordinates of the object’s center, as well as its height and width. This means we have \\nfour numbers to predict. It does not require much change to the model; we just need to add a ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 32}),\n",
       " Document(page_content='secon d dense output layer with four units (typically on top of the global average pooling layer), \\nand it can be trained using the MSE loss:  \\nbase_model  = keras.applications .xception .Xception (weights=\"imagenet\" , \\n                                                  include_top =False) \\navg = keras.layers.GlobalAveragePooling2D ()(base_model .output) \\nclass_output  = keras.layers.Dense(n_classes , activation =\"softmax\" )(avg) \\nloc_output  = keras.layers.Dense(4)(avg) \\nmodel = keras.Model(inputs=base_model .input, \\n                    outputs=[class_output , loc_output ]) \\nmodel.compile(loss=[\"sparse_categorical_crossentropy\" , \"mse\"], \\n              loss_weights =[0.8, 0.2], # depends on what you care most about \\n              optimizer =optimizer , metrics=[\"accuracy\" ]) \\nBut now we have a pro blem: the flowers dataset does not have bounding boxes around the \\nflowers. So, we need to add them ourselves. This is often one of the hardest and most costly parts \\nof a Machine Learning project: getting the labels. It’s a good idea to spend time looking f or the \\nright tools. To annotate images with bounding boxes, you may want to use an open source image \\nlabeling tool like VGG Image Annotator, LabelImg, OpenLabeler, or ImgLab, or perhaps a \\ncommercial tool like LabelBox or Supervisely. You may also want to c onsider crowdsourcing \\nplatforms such as Amazon Mechanical Turk if you have a very large number of images to \\nannotate. However, it is quite a lot of work to set up a crowdsourcing platform, prepare the form \\nto be sent to the workers, supervise them, and ens ure that the quality of the bounding boxes they \\nproduce is good, so make sure it is worth the effort. If there are just a few thousand images to \\nlabel, and you don’t plan to do this frequently, it may be preferable to do it yourself. Adriana \\nKovashka et al . wrote a very practical  paper  about crowdsourcing in computer vision. I \\nrecommend you check it out, even if you do not plan to use crowdsourcing.  \\nLet’s suppose you’ve obtained the bounding boxes for every image in the flowers dataset (for \\nnow we will assume there is a single bounding box per image). You then need to create a dataset \\nwhose items will be batches of preprocessed images along with their class labels and their \\nbounding boxes. Each item should be a tuple of the form  (images,  (class_labels,  \\nbounding_boxes)) . Then you are ready to train your model!  \\nTIP  \\nThe bounding boxes should be normalized so that the horizontal and vertical coordinates, as well as the \\nheight and width, all range from 0 to 1. Also, it is common to predict the square root of the height and \\nwidth rather than the height and width directly: this way, a 10 -pixel err or for a large bounding box will \\nnot be penalized as much as a 10 -pixel error for a small bounding box.  \\nThe MSE often works fairly well as a cost function to train the model, but it is not a great metric \\nto evaluate how well the model can predict bounding boxes. The most common metric for this is \\nthe Intersection  over Union  (IoU): the area of overlap between the predicted bounding box and \\nthe target bounding box, divided by the area of their union (see  Figure  23). In tf.keras, it is \\nimplemented by the  tf.keras.metrics.MeanIoU  class.  ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 33}),\n",
       " Document(page_content=' \\nFigure  23. Intersection over Union (IoU) metric for bounding boxes  \\nClassifying and localizing a single object is nice, but what if the images contain multiple objects \\n(as is often the case in the flowers dataset)?  \\nObject Detection  \\nThe task of classifying and localizing multiple objects in an image is called  object  detection . \\nUntil a few years ago, a common approach was to take a CNN that was trained to classify and \\nlocate a single object, then slide it across the image, as shown in  Figure  24. In this example, the \\nimage was chopped into a 6 × 8 grid, and we show a CNN (the thick black rectangle) sliding \\nacross all 3 × 3 regions. When the CNN was looking at the top left of the image, it detected part \\nof the leftmost rose , and then it detected that same rose again when it was first shifted one step to \\nthe right. At the next step, it started detecting part of the topmost rose, and then it detected it \\nagain once it was shifted one more step to the right. You would then conti nue to slide the CNN \\nthrough the whole image, looking at all 3 × 3 regions. Moreover, since objects can have varying \\nsizes, you would also slide the CNN across regions of different sizes. For example, once you are \\ndone with the 3 × 3 regions, you might wan t to slide the CNN across all 4 × 4 regions as well.  \\n \\nFigure  24. Detecting multiple objects by sliding a CNN across the image  \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 34}),\n",
       " Document(page_content='This technique is fairly straightforward, but as you can see it will detect the same object multiple \\ntimes, at slightly differ ent positions. Some post -processing will then be needed to get rid of all \\nthe unnecessary bounding boxes. A  common approach for this is called  non-max suppression . \\nHere’s how you do it:  \\n1. First, you  need to add an extra  objectness  output to your CNN, to esti mate the probability \\nthat a flower is indeed present in the image (alternatively, you could add a “no -flower” \\nclass, but this usually does not work as well). It must use the sigmoid activation function, \\nand you can train it using binary cross -entropy loss.  Then get rid of all the bounding \\nboxes for which the objectness score is below some threshold: this will drop all the \\nbounding boxes that don’t actually contain a flower.  \\n2. Find the bounding box with the highest objectness score, and get rid of all the othe r \\nbounding boxes that overlap a lot with it (e.g., with an IoU greater than 60%). For \\nexamp le, in  Figure  24, the boundin g box with the max objectness score is the thick \\nbounding box over the topmost rose (the objectness score is represented by the thickness \\nof the bounding boxes). The other bounding box over that same rose overlaps a lot with \\nthe max bounding box, so we wil l get rid of it.  \\n3. Repeat step two until there are no more bounding boxes to get rid of.  \\nThis simple approach to object detection works pretty well, but it requires running the CNN \\nmany times, so it is quite slow. Fortunately, there is a much faster way to s lide a CNN across an \\nimage: using a  fully convolutional  network  (FCN).  \\nFully Convolutional Networks  \\nThe idea  of FCNs was first introduced in a  2015  paper  by Jonathan Long et al., for semantic \\nsegmentation (the task of classifying every pixel in an image according to the class of the object \\nit belongs to). The authors pointed out that you c ould replace the dense layers at the top of a \\nCNN by convolutional layers. To understand this, let’s look at an example: suppose a dense layer \\nwith 200 neurons sits on top of a convolutional layer that outputs 100 feature maps, each of size \\n7 × 7 (this is the feature map size, not the kernel size). Each neuron will compute a weighted sum \\nof all 100 × 7 × 7 activations from the convolutional layer (plus a bias term). Now let’s see what \\nhappens if we replace the dense layer with a convolutional layer using 20 0 filters, each of size 7 \\n× 7, and with  \"valid\"  padding. This layer will output 200 feature maps, each 1 × 1 (since the \\nkernel is exactly the size of the input feature maps and we are using  \"valid\"  padding). In other \\nwords, it will output 200 numbers, just  like the dense layer did; and if you look closely at the \\ncomputations performed by a convolutional layer, you will notice that these numbers will be \\nprecisely the same as those the dense layer produced. The only difference is that the dense \\nlayer’s output  was a tensor of shape [ batch  size, 200], while the convolutional layer will output a \\ntensor of shape [ batch  size, 1, 1, 200].  \\nTIP  \\nTo convert a dense layer to a convolutional layer, the number of filters in the convolutional layer must be \\nequal to the numb er of units in the dense layer, the filter size must be equal to the size of the input feature \\nmaps, and you must use  \"valid\"  padding. The stride may be set to 1 or more, as we will see shortly.  ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 35}),\n",
       " Document(page_content='Why is this important? Well, while a dense layer expects a sp ecific input size (since it has one \\nweight per input feature), a convolutional layer will happily process images of any \\nsize (however, it does expect its inputs to have a specific number of channels, since each kernel \\ncontains a different set of weights for each input channel). Since an FCN contains only \\nconvolutional layers (and pooling layers, which have the same property), it can be trained and \\nexecuted on images of any size!  \\nFor example, suppose we’d already trained a CNN for flower classification and localization. It \\nwas trained on 224 × 224 images, and it outputs 10 numbers: outputs 0 to 4 are sent through \\nthe softmax act ivation function, and this gives the class probabilities (one per class); output 5 is \\nsent through the logistic activation function, and this gives the objectness score; outputs 6 to 9 do \\nnot use any activation function, and they represent the bounding box ’s center coordinates, as well \\nas its height and width. We can now convert its dense layers to convolutional layers. In fact, we \\ndon’t even need to retrain it; we can just copy the weights from the dense layers to the \\nconvolutional layers! Alternatively, w e could have converted the CNN into an FCN before \\ntraining.  \\nNow suppose the last convolutional layer before the output layer (also called the bottleneck \\nlayer) outputs 7 × 7 feature maps when the network is fed a 224 × 224 image (see the left side \\nof Figure  25). If we feed the FCN a 448 × 448 image (see the right side of  Figure  25), the \\nbottleneck layer will now output 14 × 14 feature maps.  Since the dense output layer was replaced \\nby a convolutional layer using 10 filters of size 7 × 7, with  \"valid\"  padding and stride 1, the \\noutput will be composed of 10 features maps, each of size 8 × 8 (since 14 – 7 + 1 = 8). In other \\nwords,  the FCN will process the whole image only once, and it will output an 8 × 8 grid where \\neach cell contains 10 numbers (5 class probabilities, 1 objectness score, and 4 bounding box \\ncoordinates). It’s exactly like taking the original CNN and sliding it acro ss the image using 8 \\nsteps per row and 8 steps per column. To visualize this, imagine chopping the original image into \\na 14 × 14 grid, then sliding a 7 × 7 window across this grid; there will be 8 × 8 = 64 possible \\nlocations for the window, hence 8 × 8 pre dictions. However, the FCN approach is  much  more \\nefficient, since the network only looks at the image once. In fact,  You Only  Look  Once  (YOLO) \\nis the name of a very popular object detection architecture, which we’ll look at next.  \\n \\nFigure  14-25. The same f ully convolutional network processing a small image (left) and a large one (right)  \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 36}),\n",
       " Document(page_content='You Only Look Once (YOLO)  \\nYOLO  is an extremely fast and accurate object detection architecture proposed by Joseph \\nRedmon et al. in a  2015 paper  and subsequently improved  in 2016  (YOLOv2) and  in \\n2018  (YOLOv3). It is so fast that it can run in real time on a video, as seen in Redmon’s  demo . \\nYOLOv3’s architecture is quite similar to the one we just discussed, but with a few impor tant \\ndifferences:  \\n\\uf0b7 It outputs five bounding boxes for each grid cell (instead of just one), and each bounding \\nbox comes with an objectness score. It also outputs 20 class probabilities per grid cell, as \\nit was trained on the PASCAL VOC dataset, which contai ns 20 classes. That’s a total of \\n45 numbers per grid cell: 5 bounding boxes, each with 4 coordinates, plus 5 objectness \\nscores, plus 20 class probabilities.  \\n\\uf0b7 Instead of predicting the absolute coordinates of the bounding box centers, YOLOv3 \\npredicts an offs et relative to the coordinates of the grid cell, where (0, 0) means the top \\nleft of that cell and (1, 1) means the bottom right. For each grid cell, YOLOv3 is trained \\nto predict only bounding boxes whose center lies in that cell (but the bounding box itsel f \\ngenerally extends well beyond the grid cell). YOLOv3 applies the logistic activation \\nfunction to the bounding box coordinates to ensure they remain in the 0 to 1 range.  \\n\\uf0b7 Before training the neural net, YOLOv3 finds five representative bounding \\nbox dimensi ons, called  anchor  boxes  (or bounding  box priors ). It does this by ap plying \\nthe K -Means algorithm  to the height and width of the training set bounding boxes. For \\nexample, if the training images contain many pedestrians, then one of the anchor boxes \\nwill likely have the dimensions of a typical pedestrian. Then when the neural net predicts \\nfive bounding  boxes per grid cell, it actually predicts how much to rescale each of the \\nanchor boxes. For example, suppose one anchor box is 100 pixels tall and 50 pixels wide, \\nand the network predicts, say, a vertical rescaling factor of 1.5 and a horizontal rescaling  \\nof 0.9 (for one of the grid cells). This will result in a predicted bounding box of size 150 \\n× 45 pixels. To be more precise, for each grid cell and each anchor box, the network \\npredicts the log of the vertical and horizontal rescaling factors. Having the se priors makes \\nthe network more likely to predict bounding boxes of the appropriate dimensions, and it \\nalso speeds up training because it will more quickly learn what reasonable bounding \\nboxes look like.  \\n\\uf0b7 The network is trained using images of different sc ales: every few batches during \\ntraining, the network randomly chooses a new image dimension (from 330 × 330 to 608 \\n× 608 pixels). This allows the network to learn to detect objects at different scales. \\nMoreover, it makes it possible to use YOLOv3 at differ ent scales: the smaller scale will \\nbe less accurate but faster than the larger scale, so you can choose the right trade -off for \\nyour use case.  \\nThere are a few more innovations you might be interested in, such as the use of skip connections \\nto recover some of the spatial resolution that is lost in the CNN (we will discuss this shortly, \\nwhen we look at semantic segmentation). In the 2016 paper, the authors introduce the \\nYOLO9000 model that uses hierarchical classification: the  model predicts a probability for  each \\nnode in a visual hierarchy called  WordTree . This makes it possible for the network to predict ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 37}),\n",
       " Document(page_content='with high confidence that an image represents, say, a dog, even though it is unsure what specific \\ntype of dog. I encourage you to go ahead and read all thre e papers: they are quite pleasant to \\nread, and they provide excellent examples of how Deep Learning systems can be incrementally \\nimproved.  \\nMEAN AVERAGE PRECISI ON (MAP)  \\nA very common metric used in object detection tasks is the  mean  Average  Precision  (mAP).  \\n“Mean Average” sounds a bit redundant, doesn’t it? To understand this metric, let’s go back to \\ntwo classification metrics : precision and recall. Remember the trade -off: the higher the recall, the \\nlower the precision. You can visualize th is in a precision/recall curve . To summarize this curve \\ninto a single number, we could compute its area under the curve (AUC). But note that the \\nprecision/recall curve may contain a few sections where precision ac tually goes up when recall \\nincreases, e specially at low recall values . This is one of the motivations for the mAP metric.  \\nSuppose the classifier has 90% precision at 10% recall, but 96% precision at 20% recall. There’s \\nreally no trade -off here: it simply makes more sense to use the classifier at 20% recall rather than \\nat 10 % recall, as you will get both higher recall and higher precision. So instead of looking at the \\nprecision  at 10% recall, we should really be looking at the  maximum  precision that the classifier \\ncan offer with  at least  10% recall. It would be 96%, not 90%. Therefore, one way to get a fair \\nidea of the model’s performance is to compute the maximum precision you can get with at least \\n0% recall, then 10% recall, 20%, and so on up to 100%, and then calculate the mean of these \\nmaximum precisions. This  is called th e Average  Precision  (AP) metric. Now when there are \\nmore than two classes, we can compute the AP for each class, and then compute the mean AP \\n(mAP). That’s it!  \\nIn an object detection system, there is an additional level of complexity: what if the system \\ndetected the correct class, but at the wrong location (i.e., the bounding box is completely off)? \\nSurely we should not count this as a positive prediction. One approach is to define an IOU \\nthreshold: for example, we may consider that a prediction is correct only if the IOU is greater \\nthan, say, 0.5, and the predicted class is correct. The corresponding mAP is generally noted \\nmAP@0.5 (or mAP@50%, or sometimes just AP 50). In some competitions (such as the PASCAL \\nVOC challenge), this is what is done. In others ( such as the COCO competition), the mAP is \\ncomputed for different IOU thresholds (0.50, 0.55, 0.60, …, 0.95), and the final metric is the \\nmean of all these mAPs (noted mAP@[.50:.95] or mAP@[.50:0.05:.95]). Yes, that’s a mean \\nmean average.  \\nSeveral YOLO imple mentations built using TensorFlow are available on GitHub. In particular, \\ncheck out  Zihao  Zang’s  TensorFlow  2 implementation . Other object detection models are \\navailable in the TensorFlow Models project, many with pretrained weights; and some have even \\nbeen ported to TF Hub, such as  SSD and Faster -RCNN , which are both quite popular. SSD is \\nalso a “single shot” detection mode l, similar to YOLO. Faster R -CNN is more complex: the \\nimage first goes through a CNN, then the output  is passed to a  Region  Proposal  Network  (RPN) \\nthat proposes bounding boxes that are most likely to contain an object, and a classifier is run for \\neach boun ding box, based on the cropped output of the CNN.  ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 38}),\n",
       " Document(page_content='The choice of detection system depends on many factors: speed, accuracy, available pretrained \\nmodels, training time, complexity, etc. The papers contain tables of metrics, but there is quite a \\nlot of variab ility in the testing environments, and the technologies evolve so fast that it is \\ndifficult to make a fair comparison that will be useful for most people and remain valid for more \\nthan a few months.  \\nSo, we can locate objects by drawing bounding boxes aroun d them. Great! But perhaps you want \\nto be a bit more precise. Let’s see how to go down to the pixel level.  \\nSemantic Segmentation  \\nIn semantic  segmentation , each  pixel is classified according to the class of the object it belongs \\nto (e.g., road, car, pedestr ian, building, etc.), as shown in  Figure  26. Note that different objects of \\nthe same class are  not distinguish ed. For example, all the bicycles on the right side of the \\nsegmented image end up as one big lump of pixels. The main difficulty in this task is that when \\nimages go through a regular CNN, they gradually lose their spatial resolution (due to the layers \\nwith strides greater than 1); so, a regular CNN may end up knowing that there’s a person \\nsomewhere in the bottom left of the image, but it will not be much more precise than that.  \\nJust like for object detection, there are many different approaches to tackle th is problem, some \\nquite complex. However, a fairly simple solution was proposed in the 2015 paper by Jonathan \\nLong et al. we discussed earlier. The authors start by taking a pretrained CNN and turning it into \\nan FCN. The CNN applies an overall stride of 32 to the input image (i.e., if you add up all the \\nstrides greater than 1), meaning the last layer outputs feature maps that are 32 times smaller than \\nthe input image. This is clearly too coarse, so they add a single  upsampling  layer  that multiplies \\nthe resol ution by 32.  \\n \\nFigure  26. Semantic segmentation  \\nThere are  several solutions available for upsampling (increasing the size of an image), such as \\nbilinear interpolation, but that only works reasonably well up to ×4 or ×8. Instead, they  use \\na transposed  convolutional  layer : it is equivalent to first stretching the image by inserting empty \\nrows and columns (full of zeros), then  performing a regular convolution ( see Figure  27). \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 39}),\n",
       " Document(page_content='Alternatively, some people prefer to think of it as a regular co nvolutional layer that uses \\nfractional strides (e.g., 1/2 in  Figure  27). The transposed convolutional layer can be initialized to \\nperform something close to linear interpolation, but since it is a trainable layer, it will learn to do \\nbetter during training. In tf.keras, you can use the  Conv2DTranspose  layer.  \\n \\nFigure  27. Upsampling using a transposed convolutional l ayer \\nNOTE  \\nIn a transposed convolutional layer, the stride defines how much the input will be stretched, not the size \\nof the filter steps, so the larger the stride, the larger the output (unlike for convolutional layers or pooling \\nlayers).  \\nTENSORFLOW CONVOL UTION OPERATIONS  \\nTensorFlow  also offers a few other kinds of convolutional layers:  \\nkeras.layers.Conv1D  \\nCreates a convolutional layer for 1D inputs, such as time series or text (sequences of \\nletters or words), as we will see in next section . \\nkeras.layers.Conv3D  \\nCreates a convolutional layer for 3D inputs, such as 3D PET scans.  \\ndilation_rate  \\nSetting the  dilation_rate  hyperparameter of any  convolutional layer to a value of 2 \\nor more creates an  à-trous  convolutional  layer  (“à trous” is French for “with holes”). This \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 40}),\n",
       " Document(page_content='is equivalent to using a regular convolutional layer with a filter dilated by inserting rows \\nand columns of zeros (i.e., holes) . For example, a 1 × 3 filter equal to  [[1,2,3]]  may \\nbe dilated with a  dilation  rate of 4, resulting in a  dilated  filter  of [[1, 0, 0, 0, 2, \\n0, 0, 0, 3]]. This lets the convolutional layer have a larger receptive field at no \\ncomputational price and using n o extra parameters.  \\ntf.nn.depthwise_conv2d()  \\nCan be used to create a  depthwise  convolutional  layer  (but you need to create the \\nvariables yourself). It applies every filter to every individual input channel independently. \\nThus, if there are  fn filters and  fn′ input channels, then this will output  fn × fn′ feature \\nmaps.  \\nThis solution is OK, but still too imprecise. To do better, the authors added skip connections \\nfrom lower layers: for example, they upsampled the output image by a factor of 2 (instead of 32),  \\nand they added the output of a lower layer that had this double resolution. Then they upsampled \\nthe result by a factor of 16, leading to a total upsampling factor of 3 2 (see  Figure  28). This \\nrecovered some of the spatial resolution that was lost in earlier pooling layers. In their best \\narchitecture, they used a second similar skip connection to recover even finer de tails from an \\neven lower layer. In short, the output of the original CNN goes through the following extra steps: \\nupscale ×2, add the output of a lower layer (of the appropriate scale), upscale ×2, add the output \\nof an even lower layer, and finally upscale ×8. It is even possible to scale up beyond the size of \\nthe original image: this can be used to increase the resolution of an image, which is a technique \\ncalled  super -resolution . \\n \\nFigure  28. Skip layers recover some spatial resolution from lower layers  \\nOnce again, many GitHub repositories provide TensorFlow implementations of semantic \\nsegmentation  (TensorFlow  1 for now), and you will even find pretrained  instance  \\nsegmentation  models in the TensorFlow Models project. Instance segmentation is similar to \\nsemantic segmentation, but instead of merging all objects of the same class into one big lump, \\neach object is distinguished from the others (e.g., it identifies each individual bicycle). At \\npresent,  the instance segmentation models available in the TensorFlo w Models project are based \\non the  Mask  R-CNN  architecture, which was proposed in a  2017  paper : it extends the Faster R -\\nCNN model by additionally producing a pixel mask for each bounding box. So not only do you \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 41}),\n",
       " Document(page_content='get a bounding box around each object, with a set of estimated class probabilities, but you also \\nget a pixel mask that locates  pixels in the bounding box that belong to the object.  \\nAs you can see, the field of Deep Computer Vision is vast and moving fast, with all sorts of \\narchitectures popping out every year, all based on convolutional neural networks. The  progress \\nmade in just a few years has been astounding, and researchers are now focusing on harder and \\nharder problems, such as  adversarial  learning  (which attempts to make the network more \\nresistant to images designed to fool it), explainability (understanding why the network m akes a \\nspecific classification), realistic  image  generation  (which we will come back in later section ), \\nand single -shot learning  (a system that can recognize an object after it has seen it just once). \\nSome even explore completely novel architectures, such as Geoffrey Hinton’s  capsule  \\nnetworks  (I presented them in a couple of  videos , with the corresponding code in a notebook). \\nNow on to the next section , wher e we will look at how to process sequential data such as time \\nseries using recurrent neural networks and convolutional neural networks.  \\n ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 42}),\n",
       " Document(page_content='Processing Sequences Using  RNNs  and CNNs  \\nThe batter hits the ball. The outfielder immediately starts running, anticipating the ball’s \\ntrajectory. He tracks it, adapts his movements, and finally catches it (under a thunder of \\napplause). Predicting the futur e is something you do all the time, whether you are finishing a \\nfriend’s sentence or anticipating the smell of coffee at breakfast. In this section  we will discuss \\nrecurrent neural networks (RNNs), a class of nets that can predict the future (well, up to a  point, \\nof course). They  can analyze time series data such as stock prices, and tell you when to buy or \\nsell. In  autonomous driving systems, they can anticipate car trajectories and help avoid accidents. \\nMore generally, they  can work on sequences of arbitr ary lengths, rather than on fixed -sized \\ninputs like all the nets we have considered so far. For example, they can take sentences, \\ndocuments, or audio samples as input, making  them extremely useful for natural language \\nprocessing applications such as automa tic translation or speech -to-text. \\nIn this section  we will first look at the fundamental concepts underlying RNNs and how to train \\nthem using backpropagation through time, then we will use them to forecast a time series. After \\nthat we’ll explore the two ma in difficulties that RNNs face:  \\n\\uf0b7 Unstable gradients (di scussed before ), which can be alleviated using various techniques, \\nincluding recurrent dropout and recurrent layer normalization  \\n\\uf0b7 A (very) limited short -term memory, which can be extended using LSTM and GRU cells  \\nRNNs are not the only types of neural networks capable of handling sequential data: for small \\nsequences, a regular dense network can do the trick; and for very long sequences, such as audio \\nsamples or text, convolutional neural networks can actua lly work quite well too. We will discuss \\nboth of these possibilities, and we will finish this section  by implementing a  WaveNet : this is a \\nCNN architecture capable of handling sequences of tens  of thousands of time steps. In next \\nsection , we will continue to explore RNNs and see how to use them for natural language \\nprocessing, along with more recent architectures based on attention mechanisms. Let’s get \\nstarted!  \\nRecurrent Neurons and Layers  \\nUp to now we have focused on feedforward neural networks, where the  activations flow only in \\none direction, from the i nput layer to the output layer . A recurrent neural network looks very \\nmuch like a feedforward neural network, except it also has connections pointing backward. Let’s \\nlook at the simplest possible RNN, comp osed of one neuron receiving inputs, producing an \\noutput, and sending that output back to itself, as shown in  Figure  1 (left). At each time \\nstep t (also called a  frame ), this  recurrent  neuron  receives the inputs  x(t) as well as its own output \\nfrom the previous time step,  y(t–1). Since there is no previous output at the first time step, it is \\ngenerally set to 0. We can represent  this tiny network against the time axis, as shown \\nin Figure  1 (right). This  is called  unrolling  the network  through  time (it’s the same recurrent \\nneuron represented once per time step).  ', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 0}),\n",
       " Document(page_content=' \\nFigure  1. A recurrent neuron (left) unrolled through time (right)  \\nYou can easily create a layer of recurrent neurons. At each time step  t, every neuron receives \\nboth the input vector  x(t) and the output vector from the previous time step  y(t–1), as shown \\nin Figure  2. Note that both the inputs and outputs are vector s now (when there was just a single \\nneuron, the output was a scalar).  \\n \\nFigu re 2. A layer of recurrent neurons (left) unrolled through time (right)  \\nEach recurrent neuron has two sets of weights: one for the inputs  x(t) and the other for the outputs \\nof the previous time step,  y(t–1). Let’s call these weight vectors  wx and wy. If we consider the \\nwhole recurrent layer instead of just one recurrent neuron, we can place all the weight vectors in \\ntwo weight matrices,  Wx and Wy. The output vector of the whole recu rrent layer can then be \\ncomputed pretty much as you might expect, as shown in  Equation  1 (b is the bias vector and  ϕ(·) \\nis the activation function (e.g., ReLU).  \\n \\n', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 1}),\n",
       " Document(page_content='Equation  1. Output of a recurrent layer for a single instance  \\n \\nJust as with feedforward neural networks, we can compute a recurrent layer’s output in one shot \\nfor a whole mini -batch by placing all the inputs at time step  t in an input \\nmatrix  X(t) (see Equation  2). \\nEquation  2. Outputs of a la yer of recurrent neurons for all instances in a mini -batch  \\n \\nIn this equation:  \\n\\uf0b7 Y(t) is an m × nneurons matrix containing the layer’s outputs at time step  t for each instance \\nin the mini -batch ( m is the number of instances in the mini -batch and  nneurons is the number \\nof neurons).  \\n\\uf0b7 X(t) is an m × ninputs matrix containing the inputs for all instances ( ninputs is the number of \\ninput features).  \\n\\uf0b7 Wx is an ninputs × nneurons matrix containing the connection weights for the inputs of the \\ncurrent time step.  \\n\\uf0b7 Wy is an nneurons × nneurons matrix containing the connection weights for the outputs of the \\nprevious time step.  \\n\\uf0b7 b is a vector of size  nneurons containing each neuron’s bias term.  \\n\\uf0b7 The weight matrices  Wx and Wy are often concatenated vertically into a single weight \\nmatrix  W of shape ( ninputs + nneurons) × nneurons (see the second line of  Equation  2). \\n\\uf0b7 The notation [ X(t) Y(t–1)] represents  the horizontal concatenation of the \\nmatrices  X(t) and Y(t–1). \\nNotice that  Y(t) is a function of  X(t) and Y(t–1), which is a function of  X(t–1) and Y(t–2), which is a \\nfunction of  X(t–2) and Y(t–3), and so on. This makes  Y(t) a function of all the inputs si nce time  t = 0 \\n(that is,  X(0), X(1), …, X(t)). At the first time step,  t = 0, there are no previous outputs, so they are \\ntypically assumed to be all zeros.  \\nMemory Cells  \\nSince  the output of a recurrent neuron at time step  t is a function of all the inputs f rom previous \\ntime steps, you could say it has a form of  memory . A part of a neural network that preserves \\nsome state across time steps is called a  memory  cell (or simply a  cell). A single recurrent neuron, \\nor a layer of recurrent neurons, is a very basic c ell, capable of learning only short patterns \\n(typically about 10 steps long, but this varies depending on the task). Later in this section , we \\nwill look at some more complex and powerful types of cells capable of learning longer patterns \\n(roughly 10 times longer, but again, this depends on the task).  \\n', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 2}),\n",
       " Document(page_content='In general a cell’s state at time step  t, denoted  h(t) (the “h” stands for “hidden”), is a function of \\nsome inputs at that time step and its state at the previous time step:  h(t) = f(h(t–1), x(t)). Its output a t \\ntime step  t, denoted  y(t), is also a function of the previous state and the current inputs. In the case \\nof the basic cells we have discussed so far, the output is simply equal to the state, but in more \\ncomplex cells this is not always the case, as shown in Figure  3. \\n \\nFigure  3. A cell’s hidden state and its output may be different  \\nInput and Output Sequences  \\nAn RNN can simul taneously take a sequence of inputs and produce a sequence of outputs (see \\nthe top -left network in  Figure  4). This type of  sequence -to-sequence  network  is useful for \\npredicting time series such as stock prices: you feed it the prices over the last  N days, and it must \\noutput the prices shifted by one day into the future (i.e., from  N – 1 days ago to tomorrow).  \\nAlternatively, you  could feed the network a sequence of inputs and ignore all outputs except for \\nthe last one (see the top -right network in  Figure 4). In other words, this  is a sequence -to-vector  \\nnetwork . For example, you could feed the network a sequence of words corresponding to a \\nmovie review, and the network would output a sentiment score (e.g., from –1 [hate] to +1 \\n[love]).  \\nConversely, you could feed the network the same input vector over and over again at each time \\nstep and let it output a sequence (see the bottom -left network of  Figure  4). This  is a vector -to-\\nsequence  network . For example, the input could be an image (or the output of a CNN), and the \\noutput could be a caption for that image.  \\nLastly, you could have a sequence -to-vector network, called  an encoder , followed by a vector -to-\\nsequence network, called a  decoder  (see the bottom -right network of  Figure  4). For example, this \\ncould be used for translating a sentence from one language to another. You would feed the \\nnetwork a sentence in one language, the encoder would convert this sentence into a single vector \\nrepresentation, and then the decoder would decode this vector into a s entence in another \\n', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 3}),\n",
       " Document(page_content='language. This two -step model, called  an Encoder–Decoder , works much better than trying to \\ntranslate on the fly with a single sequence -to-sequence RNN (like the one represented at the top \\nleft): the last words of a sentence can affect th e first words of the translation, so you need to wait \\nuntil you have seen the whole sentence before translating it. We will see how to implement an \\nEncoder –Decoder in next section  (as we will see, it is a bit more complex than \\nin Figure  4 suggests).  \\n \\nFigure  4. Seq-to-seq (top left), seq -to-vector (top right), vector -to-seq (bottom left), and Encoder –Decoder (bottom right) n etworks  \\nSounds promising, but how do you train a recurrent neural network?  \\nTraining RNNs  \\nTo train an RNN, the trick is to unroll it through time (like we just did) and then simply use \\nregular backpropagation ( see Figure  5). This  strategy is called  backpropagation  through  \\ntime (BPTT).  \\nJust like in regular backpropagation, there is a first forward pass through the unrolled network \\n(represented by the dashed arrows). Then the output sequence is evaluated using a cost \\nfunction  C(Y(0), Y(1), …Y(T)) (where  T is the max time step). Note that this cost function may \\nignore some outputs, as shown in  Figure  5 (for example, in a sequence -to-vector RNN, all \\noutputs are ignored except for the very last one). The gradients of that cost function are then \\npropagated backwa rd through the unrolled network (represented by the solid arrows). Finally the \\nmodel parameters are updated using the gradients computed during BPTT. Note that the \\ngradients flow backward through all the outputs used by the cost function, not just through the \\n', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 4}),\n",
       " Document(page_content='final output (for example, in  Figure  5 the cost function is computed using the last three outputs of \\nthe network,  Y(2), Y(3), and  Y(4), so gradients flow through these three outputs, but not \\nthrough  Y(0) and Y(1)). Moreover, since the same parameters  W and b are used at each time step, \\nbackpropagation will do the right thing and sum over all time steps.  \\n \\nFigure  5. Backprop agation through time  \\nFortunately, tf.keras takes care of all of this complexity for you —so let’s start coding!  \\nForecasting a Time Series  \\nSuppose  you are studying the number of active users per hour on your website, or the daily \\ntemperature in your city, or  your company’s financial health, measured quarterly using multiple \\nmetrics. In all these cases, the data will be a sequence of one or more values per time step. This \\nis called a  time series . In the first two examples there is a single value per time step,  so these \\nare univariate  time series , while in the financial example there are multiple values per time step \\n(e.g., the company’s revenue, debt, and so on), so  it is a  multivariate  time series . A typical task is \\nto predict future values, which  is called  forecasting . Another common task is to fill in the blanks: \\nto predict (or rather “postdict”) missing values from the past. This  is called  imputation . For \\nexam ple, Figure  6 shows 3 univariate time series, each of them 50 time steps long, and the goal \\nhere is to forecast the value at the next time step (represented by the X) for each of them.  \\n', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 5}),\n",
       " Document(page_content=' \\nFigure  6. Time series forecasting  \\nFor simplicity, we are using a time series generated by the  generate_time_series()  function, \\nshown here:  \\ndef generate_time_series (batch_size , n_steps): \\n    freq1, freq2, offsets1 , offsets2 = np.random.rand(4, batch_size , 1) \\n    time = np.linspace (0, 1, n_steps) \\n    series = 0.5 * np.sin((time - offsets1 ) * (freq1 * 10 + 10))  #   wave 1 \\n    series += 0.2 * np.sin((time - offsets2 ) * (freq2 * 20 + 20)) # + wave 2 \\n    series += 0.1 * (np.random.rand(batch_size , n_steps) - 0.5)   # + noise \\n    return series[..., np.newaxis].astype(np.float32) \\nThis function creates as many time series as requested (via the  batch_size  argument), each of \\nlength  n_steps , and there is just one value per time step in each series (i.e., all series are \\nunivariate). The function return s a NumPy array of shape [ batch  size, time steps , 1], where each \\nseries is the sum of two sine waves of fixed amplitudes but random frequencies and phases, plus \\na bit of noise.  \\nNOTE  \\nWhen dealing with time series (and other types of sequences such as senten ces), the input features are \\ngenerally represented as 3D arrays of shape [ batch  size, time steps , dimensionality ], \\nwhere  dimensionality  is 1 for univariate time series and more for multivariate time series.  \\nNow let’s create a training set, a validation set , and a test set using this function:  \\nn_steps = 50 \\nseries = generate_time_series (10000, n_steps + 1) \\nX_train, y_train = series[:7000, :n_steps], series[:7000, -1] \\nX_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1] \\nX_test, y_test = series[9000:, :n_steps], series[9000:, -1] \\nX_train  contains 7,000 time series (i.e., its shape is [7000, 50, 1]), while  X_valid  contains 2,000 \\n(from the 7,000th time series to the 8,999th) and  X_test  contains 1,000 (from the 9,000th to the \\n9,999th). Since we wan t to forecast a single value for each series, the targets are column vectors \\n(e.g.,  y_train  has a shape of [7000, 1]).  \\n', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 6}),\n",
       " Document(page_content='Baseline Metrics  \\nBefore  we start using RNNs, it is often a good idea to have a few baseline metrics, or else we \\nmay end up thinking our m odel works great when in fact it is doing worse than basic models. For \\nexample, the simplest approach is to predict the last value in each series. This is called  naive  \\nforecasting , and it is sometimes surprisingly difficult to outperform. In this case, it  gives us a \\nmean squared error of about 0.020:  \\n>>> y_pred = X_valid[:, -1] \\n>>> np.mean(keras.losses.mean_squared_error (y_valid, y_pred)) \\n0.020211367  \\nAnother simple approach is to use a fully connected network. Since it expects a flat list of \\nfeatures for ea ch input, we need to add a  Flatten  layer. Let’s just use a simple Linear Regression \\nmodel so that each prediction will be a linear combination of the values in the time series:  \\nmodel = keras.models.Sequential ([ \\n    keras.layers.Flatten(input_shape =[50, 1]), \\n    keras.layers.Dense(1) \\n]) \\nIf we compile this model using the MSE loss and the default Adam optimizer, then fit it on the \\ntraining set for 20 epochs and evaluate it on the validation set, we get an MSE of about 0.004. \\nThat’s much better than the naive approach!  \\nImplementing a Simple RNN  \\nLet’s  see if we can beat that with a simple RNN:  \\nmodel = keras.models.Sequential ([ \\n  keras.layers.SimpleRNN (1, input_shape =[None, 1]) \\n]) \\nThat’s really the simplest RNN you can build. It just contains a single layer, with  a single \\nneuron, as we saw in  Figure  1. We do not need to specify the length of the input sequences \\n(unlike in the previ ous model), since a recurrent neural network can process any number of time \\nsteps (this is why we set the first input dimension to  None). By default, the  SimpleRNN  layer uses \\nthe hyperbolic tangent activation function. It works exactly as we saw earlier: t he initial \\nstate h(init) is set to 0, and it is passed to a single recurrent neuron, along with the value of the first \\ntime step,  x(0). The neuron computes a weighted sum of these values and applies the hyperbolic \\ntangent activation function to the result,  and this gives the first output,  y0. In a simple RNN, this \\noutput is also the new state  h0. This new state is passed to the same recurrent neuron along with \\nthe next input value,  x(1), and the process is repeated until the last time step. Then the layer j ust \\noutputs the last value,  y49. All of this is performed simultaneously for every time series.  \\nNOTE  ', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 7}),\n",
       " Document(page_content='By default, recurrent layers in Keras only return the final output. To make them return one output per time \\nstep, you must set  return_sequences=True , as we  will see.  \\nIf you compile, fit, and evaluate this model (just like earlier, we train for 20 epochs using Adam), \\nyou will find that its MSE reaches only 0.014, so it is better than the naive approach but it does \\nnot beat a simple linear model. Note that for  each neuron, a linear model has one parameter per \\ninput and per time step, plus a bias term (in the simple linear model we used, that’s a total of 51 \\nparameters). In contrast, for each recurrent neuron in a simple RNN, there is just one parameter \\nper inpu t and per hidden state dimension (in a simple RNN, that’s just the number of recurrent \\nneurons in the layer), plus a bias term. In this simple RNN, that’s a total of just three parameters.  \\nTREND AND SEASONALIT Y \\nThere  are many other models to forecast time series, such as  weighted  moving  \\naverage  models  or autoregressive  integrated  moving  average  (ARIMA) models. Some of them \\nrequire you to first remove the trend and seasonality. For example, if you are studying the \\nnumber of active users on your website, and it is growing by 10% every month, you would have \\nto remove this trend from the time series. Once the model is trained and starts making \\npredictions, you would have to add the trend back to get the final predictions. Similarly, if you \\nare trying to predict the amount of sunscreen lotion sold every month, you will probably observe \\nstrong seasonality: since it sells well every summer, a similar pattern will be repeated every year. \\nYou would have to remove this seasonality from the time series, for example by c omputing the \\ndifference between the value at each time step and the value one year earlier (this technique  is \\ncalled  differencing ). Again, after the model is trained and makes predictions, you would have to \\nadd the seasonal pattern back to get the final pr edictions.  \\nWhen using RNNs, it is generally not necessary to do all this, but it may improve performance in \\nsome cases, since the model will not have to learn the trend or the  seasonality . \\nApparently our simple RNN was too simple to get good performance. S o let’s try to add more \\nrecurrent layers!  \\nDeep RNNs  \\nIt is quite common to stack multiple layers of cells, as shown in Figure  7. This  gives you a  deep  \\nRNN . \\n \\nFigure  7. Deep RNN (left) unrolled through time (right)  \\n', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 8}),\n",
       " Document(page_content='Implementing a deep RNN with tf.keras is quite simple: just stack recurrent layers. In this \\nexample, we use three  SimpleRNN  layers (but we could add any other type  of recurrent layer, such \\nas an  LSTM layer or a  GRU layer, which we will discuss shortly):  \\nmodel = keras.models.Sequential ([ \\n    keras.layers.SimpleRNN (20, return_sequences =True, input_shape =[None, 1]), \\n    keras.layers.SimpleRNN (20, return_sequences =True), \\n    keras.layers.SimpleRNN (1) \\n]) \\nWARNING  \\nMake sure to set  return_sequences=True  for all recurrent layers (except the last one, if you only care \\nabout the last output). If you don’t, they will output a 2D array (containing only the output of the last time  \\nstep) instead of a 3D array (containing outputs for all time steps), and the next recurrent layer will \\ncomplain that you are not feeding it sequences in the expected 3D format.  \\nIf you compile, fit, and evaluate this model, you will find that it reaches an  MSE of 0.003. We \\nfinally managed to beat the linear model!  \\nNote that the last layer is not ideal: it must have a single unit because we want to forecast a \\nunivariate time series, and this means we must have a single output value per time step. \\nHowever, ha ving a single unit means that the hidden state is just a single number. That’s really \\nnot much, and it’s probably not that useful; presumably, the RNN will mostly use the hidden \\nstates of the other recurrent layers to carry over all the information it need s from time step to \\ntime step, and it will not use the final layer’s hidden state very much. Moreover, since \\na SimpleRNN  layer uses the tanh activation function by default, the predicted values must lie \\nwithin the range –1 to 1. But what if you want to use  another activation function? For both these \\nreasons, it might be preferable to replace the output layer with a  Dense  layer: it would run \\nslightly faster, the accuracy would be roughly the same, and it would allow us to choose any \\noutput activation functio n we want. If you make this change, also make sure to \\nremove  return_sequences=True  from the second (now last) recurrent layer:  \\nmodel = keras.models.Sequential ([ \\n    keras.layers.SimpleRNN (20, return_sequences =True, input_shape =[None, 1]), \\n    keras.layers.SimpleRNN (20), \\n    keras.layers.Dense(1) \\n]) \\nIf you train this model, you will see that it converges faster and performs just as well. Plus, you \\ncould change the output activation function if you wanted.  \\nForecasting Several Time Steps Ahead  \\nSo far we have o nly predicted the value at the next time step, but we could just as easily have \\npredicted the value several steps ahead by changing the targets appropriately (e.g., to predict 10 \\nsteps ahead, just change the targets to be the value 10 steps ahead instead o f 1 step ahead). But \\nwhat if we want to predict the next 10 values?  ', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 9}),\n",
       " Document(page_content='The first option is to use the model we already trained, make it predict the next value, then add \\nthat value to the inputs (acting as if this predicted value had actually occurred), and us e the \\nmodel again to predict the following value, and so on, as in the following code:  \\nseries = generate_time_series (1, n_steps + 10) \\nX_new, Y_new = series[:, :n_steps], series[:, n_steps:] \\nX = X_new \\nfor step_ahead  in range(10): \\n    y_pred_one  = model.predict(X[:, step_ahead :])[:, np.newaxis, :] \\n    X = np.concatenate ([X, y_pred_one ], axis=1) \\n \\nY_pred = X[:, n_steps:] \\nAs you might expect, the prediction for the next step will usually be more accurate than the \\npredictions for later time steps, since the error s might accumulate (as you can see in  Figure  8). If \\nyou evaluate this approach on the validation set, you will find an M SE of about 0.029. This is \\nmuch higher than the previous models, but it’s also a much harder task, so the comparison \\ndoesn’t mean much. It’s much more meaningful to compare this performance with naive \\npredictions (just forecasting that the time series will  remain constant for 10 time steps) or with a \\nsimple linear model. The naive approach is terrible (it gives an MSE of about 0.223), but the \\nlinear model gives an MSE of about 0.0188: it’s much better than using our RNN to forecast the \\nfuture one step at a time, and also much faster to train and run. Still, if you only want to forecast \\na few time steps ahead, on more complex tasks, this approach may work well.  \\n \\nFigure  8. Forecasting 10 steps ahead, 1 step at a time  \\nThe second option is to train an RNN to  predict all 10 next values at once. We can still use a \\nsequence -to-vector model, but it will output 10 values instead of 1. However, we first need to \\nchange the targets to be vectors containing the next 10 values:  \\nseries = generate_time_series (10000, n_steps + 10) \\nX_train, Y_train = series[:7000, :n_steps], series[:7000, -10:, 0] \\n', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 10}),\n",
       " Document(page_content='X_valid, Y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0] \\nX_test, Y_test = series[9000:, :n_steps], series[9000:, -10:, 0] \\nNow we just need the output layer to ha ve 10 units instead of 1:  \\nmodel = keras.models.Sequential ([ \\n    keras.layers.SimpleRNN (20, return_sequences =True, input_shape =[None, 1]), \\n    keras.layers.SimpleRNN (20), \\n    keras.layers.Dense(10) \\n]) \\nAfter training this model, you can predict the next 10 v alues at once very easily:  \\nY_pred = model.predict(X_new) \\nThis model works nicely: the MSE for the next 10 time steps is about 0.008. That’s much better \\nthan the linear model. But we can still do better: indeed, instead of training the model to forecast \\nthe next 10 values only at the very last time step, we can train it to forecast the next 10 values at \\neach and every time step. In other words, we can turn this sequence -to-vector RNN into a \\nsequence -to-sequence RNN. The advantage of this technique is that th e loss will contain a term \\nfor the output of the RNN at each and every time step, not just the output at the last time step. \\nThis means there will be many more error gradients flowing through the model, and they won’t \\nhave to flow only through time; they w ill also flow from the output of each time step. This will \\nboth stabilize and speed up training.  \\nTo be clear, at time step 0 the model will output a vector containing the forecasts for time steps 1 \\nto 10, then at time step 1 the model will forecast time st eps 2 to 11, and so on. So each target \\nmust be a sequence of the same length as the input sequence, containing a 10 -dimensional vector \\nat each step. Let’s prepare these target sequences:  \\nY = np.empty((10000, n_steps, 10)) # each target is a sequence  of 10D vectors \\nfor step_ahead  in range(1, 10 + 1): \\n    Y[:, :, step_ahead  - 1] = series[:, step_ahead :step_ahead  + n_steps, 0] \\nY_train = Y[:7000] \\nY_valid = Y[7000:9000] \\nY_test = Y[9000:] \\nNOTE  \\nIt may be surprising that the targets will contain values that appear in the inputs (there is a lot of overlap \\nbetween  X_train  and Y_train ). Isn’t that cheating? Fortunately, not at all: at each time step, the model \\nonly knows about past time steps, so it cannot look ahead. It  is said to be a  causal  model.  \\nTo turn the model into a sequence -to-sequence model, we must set  return_sequences=True  in all \\nrecurrent layers (even the last one), and we must apply the output  Dense  layer at every time step. \\nKeras offers a  TimeDistributed  layer for this very purpose: it wraps any layer (e .g., \\na Dense  layer) and applies it at every time step of its input sequence. It does this efficiently, by \\nreshaping the inputs so that each time step is treated as a separate instance (i.e., it reshapes the \\ninputs from [ batch  size, time steps , input  dimens ions] to [ batch  size × time steps , input  ', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 11}),\n",
       " Document(page_content='dimensions ]; in this example, the number of input dimensions is 20 because the \\nprevious  SimpleRNN  layer has 20 units), then it runs the  Dense  layer, and finally it reshapes the \\noutputs back to sequences (i.e., it re shapes the outputs from [ batch  size × time steps , output  \\ndimensions ] to [ batch  size, time steps , output  dimensions ]; in this example the number of output \\ndimensions is 10, since the  Dense  layer has 10 units).  Here is the updated model:  \\nmodel = keras.models.Sequential ([ \\n    keras.layers.SimpleRNN (20, return_sequences =True, input_shape =[None, 1]), \\n    keras.layers.SimpleRNN (20, return_sequences =True), \\n    keras.layers.TimeDistributed (keras.layers.Dense(10)) \\n]) \\nThe Dense  layer actually supports sequences as inputs (and even higher -dimensional inputs): it \\nhandles them just like  TimeDistributed(Dense(…)) , meaning it is applied to the  last input \\ndimension only (independently across all time steps). Thus, we could replace the last layer with \\njust Dense(10) . For the sake of clarity, however, we will keep \\nusing  TimeDistributed(Dense(10))  because it makes it clear that the  Dense  layer is a pplied \\nindependently at each time step and that the model will output a sequence, not just a single \\nvector.  \\nAll outputs are needed during training, but only the output at the last time step is useful for \\npredictions and for evaluation. So although we will rely on the MSE over all the outputs for \\ntraining, we will use a custom metric for evaluation, to only compute the MSE over the output at \\nthe last time step:  \\ndef last_time_step_mse (Y_true, Y_pred): \\n    return keras.metrics.mean_squared_error (Y_true[:, -1], Y_pred[:, -1]) \\n \\noptimizer  = keras.optimizers .Adam(lr=0.01) \\nmodel.compile(loss=\"mse\", optimizer =optimizer , metrics=[last_time_step_mse ]) \\nWe get a validation MSE of about 0.006, which is 25% better than the previous model. You can \\ncombine this approach with  the first one: just predict the next 10 values using this RNN, then \\nconcatenate these values to the input time series and use the model again to predict the next 10 \\nvalues, and repeat the process as many times as needed. With this approach, you can genera te \\narbitrarily long sequences. It may not be very accurate for long -term predictions, but it may be \\njust fine if your goal is to generate original m usic or text, as we will see in next section.  \\nTIP  \\nWhen forecasting time series, it is often useful to have some error bars along with your predictions. For \\nthis, an efficient techniq ue is MC Dropout, introduced before : add an MC Dropout layer within each \\nmemory cell, dropping part of the inputs and hidden states. After training, to forecast a new time series, \\nuse the model many times and compute the mean and standard deviation of the predictions at each time \\nstep. \\nSimple RNNs can be quite good at forecasting time series or handling other kinds of sequences, \\nbut they do not perform as well on long time series or s equences. Let’s discuss why and see what \\nwe can do about it.  ', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 12}),\n",
       " Document(page_content='Handling Long Sequences  \\nTo train an RNN on long sequences, we must run it over many time steps, making the unrolled \\nRNN a very deep network. Just like any deep neural network it may suffer from t he unstabl e \\ngradients problem, discussed before : it may take forever to train, or training may be unstable. \\nMoreover, when an RNN processes a long sequence, it will gradually forget the first inputs in the \\nsequence. Let’s look at both these problems, starting with the unstable gradients problem.  \\nFighting the Unstable Gradients Problem  \\nMany  of the tricks we used in deep nets to  alleviate the unstable gradients problem can also be \\nused for RNNs: good parameter initialization, faster optimizers, dropout, and so on. However, \\nnonsaturating activation functions (e.g., ReLU) may not help as much here; in fact, they may \\nactually lead t he RNN to be even more unstable during training. Why? Well, suppose Gradient \\nDescent updates the weights in a way that increases the outputs slightly at the first time step. \\nBecause the same weights are used at every time step, the outputs at the second ti me step may \\nalso be slightly increased, and those at the third, and so on until the outputs explode —and a \\nnonsaturating activation function does not prevent that. You can reduce this risk by using a \\nsmaller learning rate, but you can also simply use a satu rating activation function like the \\nhyperbolic tangent (this explains why it is the default). In much the same way, the gradients \\nthemselves can explode. If you notice that training is unstable, you may want to monitor the size \\nof the gradients (e.g., usin g TensorBoard) and perhaps use Gradient Clipping.  \\nMoreover, Batch Normalization cannot be used as efficiently with RNNs as with deep \\nfeedforward nets. In fact, you cannot use it between time steps, only between recurrent layers. To \\nbe more precise, it is t echnically possible to add a BN layer to a memory cell (as we will see \\nshortly) so that it will be applied at each time step (both on the inputs for that time step and on \\nthe hidden state from the previous step). However, the same BN layer will be used at each time \\nstep, with the same parameters, regardless of the actual scale and offset of the inputs and hidden \\nstate. In practice, this does not yield good results, as was demonstrated by César Laurent et al. in \\na 2015  paper : the authors found that BN was slightly beneficial only when it was applied to the \\ninputs, not to the hidden states. In other words, it was slightly better than nothing when applied \\nbetween recurrent layers (i.e., vertically in  Figure  7), but not within recurrent layers (i.e., \\nhorizontally). In Keras this can be done simply by adding a  BatchNormalization  layer before \\neach recurrent layer, but don’t expect too much from it.  \\nAnother  form of normalization often works better with RNNs:  Layer  Normalization . This idea \\nwas introduced by Jimmy Lei Ba et al. in a  2016  paper : it is very similar to Batch Normalization, \\nbut instead of normalizing across the batch dimension, it normalizes across the features \\ndimension. One advantage is that it can compute the required statistics on the fly, at each time \\nstep, independently for each i nstance. This also means that it behaves the same way during \\ntraining and testing (as opposed to BN), and it does not need to use exponential  moving  averages \\nto estimate the feature statistics across all instances in the training set. Like BN, Layer \\nNormal ization learns a scale and an offset parameter for each input. In an RNN, it is typically \\nused right after the linear combination of the inputs and the hidden states.  ', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 13}),\n",
       " Document(page_content='Let’s use tf.keras to implement Layer Normalization within a simple memory cell. For this , we \\nneed to define a custom memory cell. It is just like a regular layer, except its  call()  method \\ntakes two arguments: the  inputs  at the current time step and the hidden  states  from the previous \\ntime step. Note that the  states  argument is a list containi ng one or more tensors. In the case of a \\nsimple RNN cell it contains a single tensor equal to the outputs of the previous time step, but \\nother cells may have multiple state tensors (e.g., an  LSTMCell  has a long -term state and a short -\\nterm state, as we will  see shortly). A cell must also have a  state_size  attribute and \\nan output_size  attribute. In a simple RNN, both are simply equal to the number of units. The \\nfollowing code implements a custom memory cell which will behave like a  SimpleRNNCell , \\nexcept it wi ll also apply Layer Normalization at each time step:  \\nclass LNSimpleRNNCell (keras.layers.Layer): \\n    def __init__ (self, units, activation =\"tanh\", **kwargs): \\n        super().__init__ (**kwargs) \\n        self.state_size  = units \\n        self.output_size  = units \\n        self.simple_rnn_cell  = keras.layers.SimpleRNNCell (units, \\n                                                          activation =None) \\n        self.layer_norm  = keras.layers.LayerNormalization () \\n        self.activation  = keras.activations .get(activati on) \\n    def call(self, inputs, states): \\n        outputs, new_states  = self.simple_rnn_cell (inputs, states) \\n        norm_outputs  = self.activation (self.layer_norm (outputs)) \\n        return norm_outputs , [norm_outputs ] \\nThe code is quite straightforward. 5 Our LNSimpleRNNCell  class inherits from \\nthe keras.layers.Layer  class, just like any custom layer. The constructor takes the numbe r of \\nunits and the desired activation function, and it sets the  state_size  and output_size  attributes, \\nthen creates a  SimpleRNNCell  with no activation function (because we want to perform Layer \\nNormalization after the linear operation but before the activa tion function). Then the constructor \\ncreates the  LayerNormalization  layer, and finally it fetches the desired activation function. \\nThe call()  method starts by applying the simple RNN cell, which computes a linear \\ncombination of the current inputs and the p revious hidden states, and it returns the result twice \\n(indeed, in a  SimpleRNNCell , the outputs are just equal to the hidden states: in other \\nwords,  new_states[0]  is equal to  outputs , so we can safely ignore  new_states  in the rest of \\nthe call()  method). Ne xt, the  call()  method applies Layer Normalization, followed by the \\nactivation function. Finally, it returns the outputs twice (once as the outputs, and once as the new \\nhidden states). To use this custom cell, all we need to do is create a  keras.layers.RNN  layer, \\npassing it a cell instance:  \\nmodel = keras.models.Sequential ([ \\n    keras.layers.RNN(LNSimpleRNNCell (20), return_sequences =True, \\n                     input_shape =[None, 1]), \\n    keras.layers.RNN(LNSimpleRNNCell (20), return_sequences =True), \\n    keras.layers.TimeDistributed (keras.layers.Dense(10)) \\n]) \\nSimilarly, you could create a custom cell to apply dropout between each time step. But there’s a \\nsimpler way: all recurrent layers (except for  keras.layers.RNN ) and all cells provided by Keras ', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 14}),\n",
       " Document(page_content='have a  dropout  hyperparameter and a  recurrent_dropout  hyperparameter: the former defines the \\ndropout rate to apply to the inputs (at each time step), and the latter defines the dropout rate for \\nthe hidden states (also at each time step). No need to create a custom cell to apply dropout at \\neach time step in an RNN.  \\nWith these techniques, you can alleviate the unstable gradients problem and train an RNN much \\nmore efficiently. Now let’s look at how to deal with the short -term memory problem.  \\nTackling the Short -Term Memory P roblem  \\nDue to the transformations that the data goes through when traversing an RNN, some \\ninformation is lost at each time step. After a while, the RNN’s state contains virtually no trace of \\nthe first inputs. This can be a showstopper. Imagine Dory the fis h trying to translate a long \\nsentence; by the time she’s finished reading it, she has no clue how it started. To tackle this  \\nproblem, various types of cells with long -term memory have been introduced. They have proven \\nso successful that the basic cells are not used much anymore. Let’s first look at the most popular \\nof these long -term memory cells: the LSTM cell.  \\nLSTM cells  \\nThe Long  Short -Term  Memory  (LSTM) cell  was proposed  in 1997  by Sepp Hochreiter and \\nJürgen S chmidhuber and gradually improved over the years by several researchers, such as  Alex  \\nGraves , Haşim  Sak, and Wojciech  Zaremba . If you consider the LSTM cell as a black box, it can \\nbe used very much like a basic cell, except it will perform much better; training will converge \\nfaster, and it will detect l ong-term dependencies in the data. In Keras, you can simply use \\nthe LSTM layer instead of the  SimpleRNN  layer:  \\nmodel = keras.models.Sequential ([ \\n    keras.layers.LSTM(20, return_sequences =True, input_shape =[None, 1]), \\n    keras.layers.LSTM(20, return_seque nces=True), \\n    keras.layers.TimeDistributed (keras.layers.Dense(10)) \\n]) \\nAlternatively, you could use the general -purpose  keras.layers.RNN  layer, giving it \\nan LSTMCell  as an argument:  \\nmodel = keras.models.Sequential ([ \\n    keras.layers.RNN(keras.layers.LSTMCell(20), return_sequences =True, \\n                     input_shape =[None, 1]), \\n    keras.layers.RNN(keras.layers.LSTMCell (20), return_sequences =True), \\n    keras.layers.TimeDistributed (keras.layers.Dense(10)) \\n]) \\nHowever, the  LSTM layer uses an optimized imple menta tion when running on a GPU  (we will see \\nlater), so in general it is preferable to use it (the  RNN layer is mostly useful w hen you define \\ncustom cells, as we did earlier).  ', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 15}),\n",
       " Document(page_content='So how does an LSTM cell work? Its architecture is shown in  Figure  9. \\nIf you don’t look at what’s inside the box, the LSTM cell looks exactly like a regular cell, except \\nthat its state is split into two vectors:  h(t) and c(t) (“c” stands for “cell”). You can think of  h(t) as \\nthe short -term state and  c(t) as the long -term state.  \\n \\nFigure  9. LSTM cell  \\nNow let’s open the box! The key idea is that the network can learn what to store in the long -term \\nstate, what to throw away, and what to read from it. As the long -term state  c(t–1) traverses the \\nnetwork from left to right, you can  see that it first goes through  a forget  gate, dropping some \\nmemories, and then it adds some new memories via the addition operation (which adds the \\nmemories that were selected by an  input  gate). The result  c(t) is sent straight out, without any \\nfurther tr ansformation. So, at each time step, some memories are dropped and some memories \\nare added. Moreover, after the addition operation, the long -term state is copied and passed \\nthrough the tanh function, and then the result is filtered by  the output  gate. This  produces the \\nshort -term state  h(t) (which is equal to the cell’s output for this time step,  y(t)). Now let’s look at \\nwhere new memories come from and how the gates work.  \\nFirst, the current input vector  x(t) and the previous short -term state  h(t–1) are fed  to four different \\nfully connected layers. They all serve a different purpose:  \\n\\uf0b7 The main layer is the one that outputs  g(t). It has the usual role of analyzing the current \\ninputs  x(t) and the previous (short -term) state  h(t–1). In a basic cell, there is not hing other \\nthan this layer, and its output goes straight out to  y(t) and h(t). In contrast, in an LSTM cell \\nthis layer’s output does not go straight out, but instead its most important parts are stored \\nin the long -term state (and the rest is  dropped ). \\n', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 16}),\n",
       " Document(page_content='\\uf0b7 The three other layers  are gate controllers . Since they use the logistic activation function, \\ntheir outputs range from 0 to 1. As you can see, their outputs are fed to element -wise \\nmultiplication operations, so if they output 0s they close the gate, and if the y output 1s \\nthey open it. Specifically:  \\n\\uf0b7 The forget  gate (controlled by  f(t)) controls which parts of the long -term state \\nshould be erased.  \\n\\uf0b7 The input  gate (controlled by  i(t)) controls which parts of  g(t) should be added to \\nthe long -term state.  \\n\\uf0b7 Finally, the  output  gate (controlled by  o(t)) controls which parts of the long -term \\nstate should be read and output at this time step, both to  h(t) and to  y(t). \\nIn short, an LSTM cell can learn to recognize an important input (that’s the role of the input \\ngate), store  it in the long -term state, preserve it for as long as it is needed (that’s the role of the \\nforget gate), and extract it whenever it is needed. This explains why these cells have been \\namazingly successful at capturing long -term patterns in time series, lon g texts, audio recordings, \\nand more.  \\nEquation  3 summarizes how to compute the cell’s long -term state, its short -term state, an d its \\noutput at each time step for a single instance (the equations for a whole mini -batch are very \\nsimilar).  \\nEquation  3. LSTM computations  \\n \\nIn this equation:  \\n\\uf0b7 Wxi, Wxf, Wxo, Wxg are the weight matrices of each of the four layers for their \\nconnection to the input vector  x(t). \\n\\uf0b7 Whi, Whf, Who, and  Whg are the weight matrices of each of t he four layers for their \\nconnection to the previous short -term state  h(t–1). \\n\\uf0b7 bi, bf, bo, and  bg are the bias terms for each of the four layers. Note that TensorFlow \\ninitializes  bf to a vector full of 1s instead of 0s. This prevents forgetting everything at  the \\nbeginning of training.  \\nPeephole connections  \\nIn a regular LSTM cell, the gate controllers can look only at the input  x(t) and the previous short -\\nterm state  h(t–1). It may be a good idea to give them a bit more context by letting them peek at the \\nlong-term state as well. This idea was  proposed  by Felix  Gers  and Jürgen  Schmidhuber  in \\n', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 17}),\n",
       " Document(page_content='2000 . They proposed an LSTM variant with  extra connections called  peephole  connections : the \\nprevious long -term state  c(t–1) is added as an input to the controllers of the forget gate and the \\ninput gate, and the current long -term state  c(t) is added as input to  the controller of the output \\ngate. This often improves performance, but not always, and there is no clear pattern for which \\ntasks are better off with or without them: you will have to try it on your task and see if it helps.  \\nIn Keras, the  LSTM layer is ba sed on the  keras.layers.LSTMCell  cell, which does not support \\npeepholes. The experimental  tf.keras.experimental.PeepholeLSTMCell  does, however, so you \\ncan create a  keras.layers.RNN  layer and pass a  PeepholeLSTMCell  to its constructor.  \\nThere are many other variants of the LSTM cell. One particularly popular variant is the GRU \\ncell, which we will look at now.  \\nGRU cells  \\nThe Gated  Recurrent  Unit (GRU) cell  (see Figure  10) was proposed by Kyunghyun Cho et al. in \\na 2014  paper  that also introduced the Encoder –Decoder network we discussed earlier.  \\n \\nFigure  10. GRU cell  \\nThe GRU cell is a simplified version of the LSTM cell, and it seems to perform just as \\nwell (which explains its growing popularity). These are the main simplifications:  \\n\\uf0b7 Both state vectors are merged into a single vector  h(t). \\n\\uf0b7 A single gate controller  z(t) controls both the  forget gate and the input gate. If the gate \\ncontroller outputs a 1, the forget gate is open (=  1) and the input gate is closed (1  – 1 = 0). \\nIf it outputs a 0, the opposite happens. In other words, whenever a memory must be \\nstored, the location where it wi ll be stored is erased first. This is actually a frequent \\nvariant to the LSTM cell in and of itself.  \\n\\uf0b7 There is no output gate; the full state vector is output at every time step. However, there \\nis a new gate controller  r(t) that controls which part of the p revious state will be shown to \\nthe main layer ( g(t)). \\n', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 18}),\n",
       " Document(page_content='Equation  4 summarizes how to compute the cell’s state at each time step f or a single instance.  \\nEquation  4. GRU computations  \\n \\nKeras provides a  keras.layers.GRU  layer (based on the  keras.layers.GRUCell  memory cell); \\nusing it is just a matter of replacing  SimpleRNN  or LSTM with GRU. \\nLSTM and GRU cells are one of the main reasons behind the success of RNNs. Yet while they \\ncan tackle much longer sequences than simple RNNs, they still have a fairly limited short -term \\nmemory, and they have a hard time learning  long-term patterns in sequences of 100 time steps or \\nmore, such as audio samples, long time series, or long sentences. One way to solve this is to \\nshorten the input sequences, for example using 1D convolutional layers.  \\nUsing 1D convolutional layers to pro cess sequences  \\nBefore  we saw that a 2D convolutional layer works by sliding several fairly small kernels (or \\nfilters) acros s an image, producing multiple 2D feature maps (one per kernel). Similarly, a 1D \\nconvolutional layer slides several kernels across a sequence, producing a 1D feature map per \\nkernel. Each kernel will learn to detect a single very short sequential pattern (n o longer than the \\nkernel size). If you use 10 kernels, then the layer’s output will be composed of 10 1 -dimensional \\nsequences (all of the same length), or equivalently you can view this output as a single 10 -\\ndimensional sequence. This means that you can bu ild a neural network composed of a mix of \\nrecurrent layers and 1D convolutional layers (or even 1D pooling layers). If you use a 1D \\nconvolutional layer with a stride of 1 and  \"same\"  padding, then the output sequence will have the \\nsame length as the input s equence. But if you use  \"valid\"  padding or a stride greater than 1, then \\nthe output sequence will be shorter than the input sequence, so make sure you adjust the targets \\naccordingly. For example, the following model is the same as earlier, except it starts  with a 1D \\nconvolutional layer that downsamples the input sequence by a factor of 2, using a stride of 2. The \\nkernel size is larger than the stride, so all inputs will be used to compute the layer’s output, and \\ntherefore the model can learn to preserve the  useful information, dropping only the unimportant \\ndetails. By shortening the sequences, the convolutional layer may help the  GRU layers detect \\nlonger patterns. Note that we must also crop off the first three time steps in the targets (since the \\nkernel’s s ize is 4, the first output of the convolutional layer will be based on the input time steps \\n0 to 3), and downsample the targets by a factor of 2:  \\nmodel = keras.models.Sequential ([ \\n    keras.layers.Conv1D(filters=20, kernel_size =4, strides=2, padding=\"valid\", \\n                        input_shape =[None, 1]), \\n    keras.layers.GRU(20, return_sequences =True), \\n    keras.layers.GRU(20, return_sequences =True), \\n    keras.layers.TimeDistributed (keras.layers.Dense(10)) \\n', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 19}),\n",
       " Document(page_content=']) \\n \\nmodel.compile(loss=\"mse\", optimizer =\"adam\", metrics=[last_time_step_mse ]) \\nhistory = model.fit(X_train, Y_train[:, 3::2], epochs=20, \\n                    validation_data =(X_valid, Y_valid[:, 3::2])) \\nIf you train and evaluate this model, you will find that it is the best model so far. The \\nconvolutional l ayer really helps. In fact, it is actually possible to use only 1D convolutional \\nlayers and drop the recurrent layers entirely!  \\nWaveNet  \\nIn a 2016  paper  Aaron van den Oord and other DeepMind researchers introduced an  architecture \\ncalled  WaveNet . They stacked 1D convolutional layers, doubling the dilation rate (how spread \\napart each neuron’s inputs are) at every layer: the first convolutional layer gets a glimpse of just \\ntwo time steps at a time, while the next one see s four time steps (its receptive field is four time \\nsteps long), the next one sees eight time steps, and so on (see  Figure  11). This way, the lower \\nlayers learn short -term patterns, while the higher layers learn long -term patterns. Thanks to the \\ndoubling dilation rate, the network can process extremely large sequences very efficiently.  \\n \\nFigure  11. WaveNet architecture  \\nIn the W aveNet paper, the authors actually stacked 10 convolutional layers with dilation rates of \\n1, 2, 4, 8, …, 256, 512, then they stacked another group of 10 identical layers (also with dilation \\nrates 1, 2, 4, 8, …, 256, 512), then again another identical group  of 10 layers. They justified this \\narchitecture by pointing out that a single stack of 10 convolutional layers with these dilation rates \\nwill act like a super -efficient convolutional layer with a kernel of size 1,024 (except way faster, \\nmore powerful, and using significantly fewer parameters), which is why they stacked 3 such \\nblocks. They also left -padded the input sequences with a number of zeros equal to the dilation \\nrate before every layer, to preserve the same sequence length throughout the network. Her e is \\nhow to implement a simplified WaveNet to tackle the same sequences as  earlier : \\nmodel = keras.models.Sequential () \\nmodel.add(keras.layers.InputLayer (input_shape =[None, 1])) \\nfor rate in (1, 2, 4, 8) * 2: \\n', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 20}),\n",
       " Document(page_content='    model.add(keras.layers.Conv1D(filters=20, kernel_size =2, padding=\"causal\" , \\n                                  activation =\"relu\", dilation_rate =rate)) \\nmodel.add(keras.layers.Conv1D(filters=10, kernel_size =1)) \\nmodel.compile(loss=\"mse\", optimizer =\"adam\", metrics=[last_time_step_mse ]) \\nhistory = model.fit(X_train, Y_train, epochs=20, \\n                    validation_data =(X_valid, Y_valid)) \\nThis Sequential  model starts with an explicit input layer (this is simpler than trying to \\nset input_shape  only on the first layer), then continues with a 1D convolution al layer \\nusing  \"causal\"  padding: this ensures that the convolutional layer does not peek into the future \\nwhen making predictions (it is equivalent to padding the inputs with the right amount of zeros on \\nthe left and using  \"valid\"  padding). We then add  similar pairs of layers using growing dilation \\nrates: 1, 2, 4, 8, and again 1, 2, 4, 8. Finally, we add the output layer: a convolutional layer with \\n10 filters of size 1 and without any activation function. Thanks to the padding layers, every \\nconvolutional lay er outputs a sequence of the same length as the input sequences, so the targets \\nwe use during training can be the full sequences: no need to crop them or downsample them.  \\nThe last two models offer the best performance so far in forecasting our time series!  In the \\nWaveNet paper, the authors achieved state -of-the-art performance on various audio tasks (hence \\nthe name of the architecture), including text -to-speech tasks, producing incredibly realistic \\nvoices across several languages. They also used the model t o generate music, one audio sample \\nat a time. This feat is all the more impressive when you realize that a single second of audio can \\ncontain tens of thousands of time steps —even LSTMs and GRUs cannot handle such long \\nsequences.  \\nIn next section  we will con tinue to explore RNNs, and we will see how they can tackle various \\nNLP tasks.  \\n ', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 21})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = read_doc(\"documents/\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(docs,chunk_size=800,chunk_overlap = 50):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap)\n",
    "    docs = text_splitter.split_documents(docs)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Deep Computer Vision Using  Convolutional  \\nNeural  Networks  \\nAlthough  IBM’s Deep Blue supercomputer beat the chess world champion Garry Kasparov back \\nin 1996, it wasn’t until fairly recently that computers were able to reliably perform seemingly \\ntrivial tasks such as detecting a puppy in a picture or recognizing spoken words. Why are these \\ntasks so effortless to us humans? The answer lies in the fact that perception largely takes place \\noutside the realm of our consciousness, within specialized visual, auditory,  and other sensory \\nmodules in our brains. By the time sensory information reaches our consciousness, it is already \\nadorned with high -level features; for example, when you look at a picture of a cute puppy, you', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 0}),\n",
       " Document(page_content='cannot choose  not to see the puppy,  not to not ice its cuteness. Nor can you explain  how you \\nrecognize a cute puppy; it’s just obvious to you. Thus, we cannot trust our subjective experience: \\nperception is not trivial at all, and to understand it we must look at how the sensory modules \\nwork.  \\nConvolutio nal neural networks (CNNs) emerged from the study of the brain’s visual cortex, and \\nthey have been used in image recognition since the 1980s. In the last few years, thanks to the \\nincrease in computational power, the amount of available training da ta, and t he tricks presented \\nbefore  for training deep nets, CNNs have managed to achieve superhuman performance on some \\ncomplex visual tasks. They power image search services, self -driving cars, automatic video', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 0}),\n",
       " Document(page_content='classification systems, and more. Moreover, CNNs  are n ot restricted to visual perception: they \\nare also successful at many other tasks, such as voice recognition and natural language \\nprocessing. However, we will focus on visual applications for now.  \\nIn this section  we will explore where CNNs came from, what t heir building blocks look like, and \\nhow to implement them using TensorFlow and Keras. Then we will discuss some of the best \\nCNN architectures, as well as other visual tasks, including object detection (classifying multiple \\nobjects in an image and placing b ounding boxes around them) and semantic segmentation \\n(classifying each pixel according to the class of the object it belongs to).  \\nThe Architecture of the Visual Cortex', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 0}),\n",
       " Document(page_content='The Architecture of the Visual Cortex  \\nDavid H. Hubel  and Torsten Wiesel performed a series of experiments on cats in  1958  and 1959  \\n(and a  few years  later on monkeys ), giving crucial insights into the structure of the visual cortex \\n(the authors received the Nobel Prize in  Physiology or Medicine in 1981 for their work). In \\nparticular, they showed that many neurons in the visual cortex have a small  local  receptive  field, \\nmeaning they react only to visual stimuli located in a limited region of the visual field \\n(see Figure  1, in which the local receptive fields of five neurons are represented by dashed \\ncircles). The receptive fields of diffe rent neurons may overlap, and together they tile the whole \\nvisual field.', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 0}),\n",
       " Document(page_content='visual field.  \\nMoreover, the authors showed that some neurons react only to images of horizontal lines, while \\nothers react only to lines with different orientations (two neurons may have the same re ceptive', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 0}),\n",
       " Document(page_content='field but react to different line orientations). They also noticed that some neurons have larger \\nreceptive fields, and they react to more complex patterns that are combinations of the lower -level \\npatterns. These observations led to the idea that th e higher -level neurons are based on the outputs \\nof neighboring lower -level neurons (in  Figure  1, notice that each neu ron is connected only to a \\nfew neurons from the previous layer). This powerful architecture is able to detect all sorts of \\ncomplex patterns in any area of the visual field.  \\n \\nFigure  1. Biological neurons in the visual cortex respond to specific patterns in  small regions of the visual field called receptive fields; as the visual', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 1}),\n",
       " Document(page_content='signal makes its way through consecutive brain modules, neurons respond to more complex patterns in larger receptive fields.  \\nThese studies of the visual cortex inspired the  neocognitron , introduced in 1980, which gradually \\nevolved into what we now call  convolutional  neural  networks . An important milestone was \\na 1998  paper  by Yann LeCun et al. that introduced the  famous  LeNet -5 architecture, widely used \\nby banks to recognize handwritten check numbers. This architecture has some building blocks \\nthat you already know, such as fully connected layers and sigmoid activation functions, but it \\nalso introduces two new bui lding blocks:  convolutional  layers  and pooling  layers . Let’s look at \\nthem now.  \\nNOTE', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 1}),\n",
       " Document(page_content='them now.  \\nNOTE  \\nWhy not simply use a deep neural network with fully connected layers for image recognition tasks? \\nUnfortunately, although this works fine for small images (e.g., MNIST), it  breaks down for larger images \\nbecause of the huge number of parameters it requires. For example, a 100 × 100 –pixel image has 10,000 \\npixels, and if the first layer has just 1,000 neurons (which already severely restricts the amount of \\ninformation transmitt ed to the next layer), this means a total of 10 million connections. And that’s just the \\nfirst layer. CNNs solve this problem using partially connected layers and weight sharing.  \\nConvolutional Layers  \\nThe most important building block of a CNN is the  convol utional  layer : neurons in the first', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 1}),\n",
       " Document(page_content='convolutional layer are not connected to every single pixel in the input image (like they were in \\nthe layers discussed in previous section s), but only to pixels in their receptive fields \\n(see Figure  2). In turn, each neuron in the second convolutional layer is connected only to \\nneurons located within a small rectangle in the first layer. This  architecture allows the network to \\nconcentrate on small low -level features in the first hidden layer, then assemble them into larger', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 1}),\n",
       " Document(page_content='higher -level features in the next hidden layer, and so on. This hierarchical structure is common in \\nreal-world images, whi ch is one of the reasons why CNNs work so well for image recognition.  \\n \\nFigure  2. CNN layers with rectangular local receptive fields  \\nNOTE  \\nAll the multilayer neural networks we’ve looked at so far had layers composed of a long line of neurons, \\nand we had to  flatten input images to 1D before feeding them to the neural network. In a CNN each layer \\nis represented in 2D, which makes it easier to match neurons with their corresponding inputs.  \\nA neuron located in row  i, column  j of a given layer is connected to th e outputs of the neurons in \\nthe previous layer located in rows  i to i + fh – 1, columns  j to j + fw – 1, where  fh and fw are the', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 2}),\n",
       " Document(page_content='height and width of the receptive field (see  Figure  3). In order for a layer to have the same height \\nand width as the previous layer, it is common to add zeros around the inputs, as shown in the \\ndiagram. This  is called  zero padding . \\n \\nFigure  3. Connections between layers and zero padding', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 2}),\n",
       " Document(page_content='It is also possible to connect a large input layer to a much smaller layer by spacing out the \\nreceptive fields, as shown in  Figure  4. This dramatically reduces the model’s computational \\ncomplexity. The shift from one receptive field to  the next is called the  stride . In the diagram, a 5 \\n× 7 input layer (plus zero padding) is connected to a 3 × 4 layer, using 3 × 3 receptive fields and \\na stride of 2 (in this example the stride is the same in both directions, but it does not have to be \\nso). A neuron located in row  i, column  j in the upper layer is connected to the outputs of the \\nneurons in t he previous layer located in rows  i × sh to i × sh + fh – 1, \\ncolumns  j × sw to j × sw + fw – 1, where  sh and sw are the vertical and horizontal strides.', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 3}),\n",
       " Document(page_content='Figure  4. Reducing dimensionality using a stride of 2  \\nFilters  \\nA neuron’s weights can be represented a s a small image the size of the receptive field. For \\nexample,  Figure  5 shows two possible sets of weights, called  filters  (or convolution  kernels ). The \\nfirst one is represented as a black square with a vertical white line in the middle (it is a 7 × 7 \\nmatrix full of 0s except for the central column, which is full of 1s); neurons using these weights \\nwill ignore everything in their r eceptive field except for the central vertical line (since all inputs \\nwill get multiplied by 0, except for the ones located in the central vertical line). The second filter \\nis a black square with a horizontal white line in the middle. Once again, neurons u sing these', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 3}),\n",
       " Document(page_content='weights will ignore everything in their receptive field except for the central horizontal line.  \\nNow if all neurons in a layer use the same vertical line filter (and the same bias term), and you \\nfeed the network the input image shown in  Figure  5 (the bottom image), the layer will output the \\ntop-left image. Notice that the vertical white lines get enhanced while the rest gets blurred. \\nSimilarly, the upper -right image is what you get if all neurons use the same horizontal line filter; \\nnotice that the horizontal white lines get enhanced while the rest is blurred out. Thus, a layer  full \\nof neurons using the same filter output s a feature  map, which highlights the areas in an image', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 3}),\n",
       " Document(page_content='that activate the filter the most. Of course, you do not have to define the filters manually: instead,', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 3}),\n",
       " Document(page_content='during training the convolutional layer will automatically learn the most useful filters for its task, \\nand the layers above will learn to combine them into more complex  patterns . \\n \\nFigure  5. Applying two different filters to get two feature maps  \\nStacking Multiple Feature Maps  \\nUp to now, for simplicity, I have represented the output of each convolution al layer as a 2D \\nlayer, but in reality a convolutional layer has multiple filters (you decide how many) and outputs \\none feature map per filter, so it is more accurately represented in 3D (see  Figure  6). It has one \\nneuron per pixel in each feature map, and all neurons within a given feature map share the same \\nparameters (i.e., the same weights and bias term). Neurons in d ifferent feature maps use different', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 4}),\n",
       " Document(page_content='parameters. A neuron’s receptive field is the same as described earlier, but it extends across all \\nthe previous layers’ feature maps. In short, a convolutional layer simultaneously applies multiple \\ntrainable filters to i ts inputs, making it capable of detecting multiple features anywhere in its \\ninputs.  \\nNOTE  \\nThe fact that all neurons in a feature map share the same parameters dramatically reduces the number of \\nparameters in the model. Once the CNN has learned to recognize a pattern in one location, it can \\nrecognize it in any other location. In contrast, once a regular DNN has learned to recognize a pattern in \\none location, it can recognize it only in that particular location.  \\nInput images are also composed of multiple subla yers: one  per color  channel . There are typically', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 4}),\n",
       " Document(page_content='three: red, green, and blue (RGB). Grayscale images have just one  channel , but some images \\nmay have much more —for example, satellite images that capture extra light frequencies (such as \\ninfrared).', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 4}),\n",
       " Document(page_content='Figure  6. Convolutional layers with multiple feature maps, and images with three color channels  \\nSpecifically, a neuron located in row  i, column  j of the feature map  k in a given convolutional \\nlayer  l is connected to the outputs of the neurons in the previous layer  l – 1, located in \\nrows  i × sh to i × sh + fh – 1 and columns  j × sw to j × sw + fw – 1, across all feature maps (in \\nlayer  l – 1). Note that all neurons located in the same row  i and column  j but in different feature \\nmaps are connected to the outputs of th e exact same neurons in the previous layer.  \\nEquation  1 summarizes the preceding explanations in one big mathematic al equation: it shows \\nhow to compute the output of a given neuron in a convolutional layer. It is a bit ugly due to all', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 5}),\n",
       " Document(page_content='the different indices, but all it does is calculate the weighted sum of all the inputs, plus the bias \\nterm.  \\nEquation  1. Computing the ou tput of a neuron in a convolutional layer  \\n \\nIn this equation:  \\n\\uf0b7 zi, j, k is the output of the neuron located in row  i, column  j in feature map  k of the \\nconvolutional layer (layer  l). \\n\\uf0b7 As explained earlier,  sh and sw are the v ertical and horizontal strides,  fh and fw are the \\nheight and width of the receptive field, and  fn′ is the number of feature maps in the \\nprevious layer (layer  l – 1).', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 5}),\n",
       " Document(page_content='\\uf0b7 xi′, j′, k′ is the output of the neuron located in layer  l – 1, row  i′, column  j′, feature  map k′ (or \\nchannel  k′ if the previous layer is the input layer).  \\n\\uf0b7 bk is the bias term for feature map  k (in layer  l). You can think of it as a knob that tweaks \\nthe overall brightness of the feature map  k. \\n\\uf0b7 wu, v, k′ ,k is the connection weight between any n euron in feature map  k of the layer  l and \\nits input located at row  u, column  v (relative to the neuron’s receptive field), and feature \\nmap k′. \\nTensorFlow Implementation  \\nIn TensorFlow, each input image is typically represented as a 3D tensor of shape [ heigh t, width,  \\nchannels ]. A mini -batch is represented as a 4D tensor of shape [ mini-batch  size, height,  width,', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 6}),\n",
       " Document(page_content='channels ]. The weights of a convolutional layer are represented as a 4D tensor of shape \\n[fh, fw, fn′, fn]. The bias terms of a convolutional layer are  simply represented as a 1D tensor of \\nshape [ fn]. \\nLet’s look at a simple example. The following code loads two sample images, using Scikit -\\nLearn’s  load_sample_image()  (which loads two color images, one of a Chinese temple, and \\nthe other of a flower), then it creates two filters and applies them to both images, and finally it \\ndisplays one of the resulting feature maps. Note that you must pip install the  Pillow  package to \\nuse load_sample_image() . \\nfrom sklearn.datasets  import load_sample_image  \\n \\n# Load sample images \\nchina = load_sample_image (\"china.jpg\" ) / 255 \\nflower = load_sample_image (\"flower.jpg\" ) / 255', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 6}),\n",
       " Document(page_content='flower = load_sample_image (\"flower.jpg\" ) / 255 \\nimages = np.array([china, flower]) \\nbatch_size , height, width, channels = images.shape \\n \\n# Create 2 filters \\nfilters = np.zeros(shape=(7, 7, channels , 2), dtype=np.float32) \\nfilters[:, 3, :, 0] = 1  # vertical  line \\nfilters[3, :, :, 1] = 1  # horizontal  line \\n \\noutputs = tf.nn.conv2d(images, filters, strides=1, padding=\"SAME\") \\n \\nplt.imshow(outputs[0, :, :, 1], cmap=\"gray\") # plot 1st image\\'s 2nd feature map \\nplt.show() \\nLet’s go through this code:  \\n\\uf0b7 The pixel intensity for each color channel is represented as a byte from 0 to 255, so we \\nscale these features simply by dividing by 255, to get floats ranging from 0 to 1.  \\n\\uf0b7 Then we create two 7 × 7 filters (one with a vertical wh ite line in the middle, and the', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 6}),\n",
       " Document(page_content='other with a horizontal white line in the middle).', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 6}),\n",
       " Document(page_content='\\uf0b7 We apply them to both images using the  tf.nn.conv2d()  function, which is part of \\nTensorFlow’s low -level Deep Learning API. In this example, we use zero padding \\n(padding=\"SAM E\") and a stride of 1.  \\n\\uf0b7 Finally, we plot one of the resulting feature maps (similar to the top -right image \\nin Figure  5). \\nThe tf.nn.conv2d()  line deserves a bit more explanation:  \\n\\uf0b7 images  is the input mini -batch (a 4D tensor, as explained earlier).  \\n\\uf0b7 filters  is the set of filters to apply (also a 4D tensor, as explained earlier).  \\n\\uf0b7 strides  is equal to  1, but it could also be a 1D array  with four elements, where the two \\ncentral elements are the vertical and horizontal strides ( sh and sw). The first and last', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 7}),\n",
       " Document(page_content='elements must currently be equal to  1. They may one day be used to specify a batch stride \\n(to skip some instances) and a channel str ide (to skip some of the previous layer’s feature \\nmaps or channels).  \\n\\uf0b7 padding  must be either  \"SAME\"  or \"VALID\" : \\n\\uf0b7 If set to  \"SAME\" , the convolutional layer uses zero padding if necessary. The \\noutput size is set to the number of input neurons divided by the st ride, rounded up. \\nFor example, if the input size is 13 and the stri de is 5 (see  Figure  7), then the \\noutput size is 3  (i.e., 13 / 5 = 2.6, rounded up to 3). Then zeros are added as \\nevenly as possible around the inputs, as needed. When  strides=1 , the layer’s \\noutputs will have the same spatial dimensions (width and height) as its inputs, \\nhence the name  same .', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 7}),\n",
       " Document(page_content='hence the name  same . \\n\\uf0b7 If set to  \"VALID\", the convolutional layer does  not use zero padding and may \\nignore some rows and columns at the bottom and right of the input image, \\ndepending on the stride, as shown in  Figure  7 (for simplicity , only the horizontal \\ndimension is shown here, but of course the same logic applies to the vertical \\ndimension). This means that every neuron’s receptive field lies strictly wi thin \\nvalid positions inside the input (it does not go out of bounds), hence the \\nname  valid . \\n \\nFigure  7. “SAME” or “VALID” padding (with input width 13, filter width 6, stride 5)', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 7}),\n",
       " Document(page_content='In this example we manually defined the filters, but in a real CNN you woul d normally define \\nfilters as trainable variables so the neural net can learn which filters work best, as explained \\nearlier. Instead of manually creating the variables, use the  keras.layers.Conv2D  layer:  \\nconv = keras.layers.Conv2D(filters=32, kernel_size =3, strides=1, \\n                           padding=\"same\", activation =\"relu\") \\nThis code creates a  Conv2D  layer with 32 filters, each 3 × 3, using a stride of 1 (both \\nhorizontally and vertically) and  \"same\"  padding, and applying the ReLU activation function to \\nits outputs. As you can see, convolutional layers have quite a few hyperparameters: you must \\nchoose the number of filters, their height and width, the strides, and the padding type. As always,', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 8}),\n",
       " Document(page_content='you can use cross -validation to find the right hyperparameter v alues, but this is very time -\\nconsuming. We will discuss common CNN architectures later, to give you some idea of which \\nhyperparameter values work best in practice.  \\nMemory Requirements  \\nAnother  problem with CNNs is that the convolutional layers require a hug e amount of RAM. \\nThis is especially true during training, because the reverse pass of backpropagation requires all \\nthe intermediate values computed during the forward pass.  \\nFor example, consider a convolutional layer with 5 × 5 filters, outputting 200 feat ure maps of \\nsize 150 × 100, with stride 1 and  \"same\"  padding. If the input is a 150 × 100 RGB image (three \\nchannels), then the number of parameters is (5 × 5 × 3 + 1) × 200 = 15,200 (the + 1 corresponds', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 8}),\n",
       " Document(page_content='to the bias terms), which is fairly small compared to  a fully connected layer. 7 However, each of \\nthe 200 feature maps contains 150 × 100 neurons, and each of these neurons needs to compute a \\nweighted sum of its 5 × 5 × 3 = 75 inputs: that’s a total of 225 million float multiplications. Not \\nas bad as a fully connected layer, but still quite computationally intensive. Moreover, if the \\nfeature maps are represented using 32 -bit floats , then the convolutional layer’s output will \\noccupy 200 × 150 × 100 × 32 = 96 million bits (12 MB) of RAM. 8 And that’s just f or one \\ninstance —if a training batch contains 100 instances, then this layer will use up 1.2 GB of RAM!  \\nDuring inference (i.e., when making a prediction for a new instance) the RAM occupied by one', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 8}),\n",
       " Document(page_content='layer can be released as soon as the next layer has been com puted, so you only need as much \\nRAM as required by two consecutive layers. But during training everything computed during the \\nforward pass needs to be preserved for the reverse pass, so the amount of RAM needed is (at \\nleast) the total amount of RAM require d by all layers.  \\nTIP  \\nIf training crashes because of an out -of-memory error, you can try reducing the mini -batch size. \\nAlternatively, you can try reducing dimensionality using a stride, or removing a few layers. Or you can \\ntry using 16 -bit floats instead of  32-bit floats. Or you could distribute the CNN across multiple devices.  \\nNow let’s look at the second common building block of CNNs: the  pooling  layer .', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 8}),\n",
       " Document(page_content='Pooling Layers  \\nOnce  you understand how convolutional layers work, the pooling layers are quite easy to g rasp. \\nTheir  goal is to  subsample  (i.e., shrink) the input image in order to reduce the computational \\nload, the memory usage, and the number of parameters  (thereby limiting the risk of overfitting).  \\nJust like in convolutional layers, each neuron in a poolin g layer is connected to the outputs of a \\nlimited number of neurons in the previous layer, located within a small rectangular receptive \\nfield. You must define its size, the stride, and the padding type, just like before. However, a \\npooling neuron has no weights; all it does is aggregate the inputs using an aggregation function', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 9}),\n",
       " Document(page_content='such as the max or mean.  Figure  8 shows  a max pooling  layer , which is the most common type \\nof pooling layer. In this example, we use a 2 × 2  pooling  kernel , with a stride o f 2 and no \\npadding. Only the max input value in each receptive field makes it to the next layer, while the \\nother inputs are dropped. For example, in the lower -left receptive field in  Figure  8, the input \\nvalues are 1, 5, 3, 2, so only the max value, 5, is propagated to the next layer. Because of the \\nstride of 2, the output image has half the height and half the width of the input image (rounded \\ndown since we use no padding).  \\n \\nFigure  8. Max pooling layer (2 × 2 pooling kernel, stride 2, no padding)  \\nNOTE', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 9}),\n",
       " Document(page_content='NOTE  \\nA pooling layer typically works on every input channel independently, so the output depth is the same as \\nthe input depth.  \\nOther  than reducing computations, memory usage, and the number of parameters, a max pooling \\nlayer also introduces some level of  invariance  to small translations, as shown in  Figure  9. Here \\nwe assume that the bright pixels have a lower value than dark pixels, and we consider three \\nimages (A, B, C) going through a max pooling layer with a 2 × 2 kernel and stride 2. Im ages B \\nand C are the same as image A, but shifted by one and two pixels to the right. As you can see, \\nthe outputs of the max pooling layer for images A and B are identical. This is what translation', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 9}),\n",
       " Document(page_content='invariance means. For image C, the output is different: it  is shifted one pixel to the right (but', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 9}),\n",
       " Document(page_content='there is still 50% invariance). By inserting a max pooling layer every few layers in a CNN, it is \\npossible to get some level of translation invariance at a larger scale. Moreover, max pooling \\noffers a small amount of  rotational invariance and a slight scale invariance. Such invariance \\n(even if it is limited) can be useful in cases where the prediction should not depend on these \\ndetails, such as in classification tasks.  \\n \\nFigure  9. Invariance to small translations  \\nHowever, max pooling has some downsides too. Firstly, it is obviously very destructive: even \\nwith a tiny 2 × 2 kernel and a stride of 2, the output will be two times smaller in both directions \\n(so its area will be four times smaller), simply dropping 75% of  the input values. And in some', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 10}),\n",
       " Document(page_content='applications, invariance is not desirable. Take  semantic segmentation (the task of classifying \\neach pixel in an image according to the object that pixel belongs to, which we’ll explore later in \\nthis section ): obviously, if th e input image is translated by one pixel to the right, the output \\nshould also be translated by one pixel to the right. The goal in this case is  equivariance , not \\ninvariance: a small change to the inputs should lead to a corresponding small change in the \\noutput. \\nTensorFlow Implementation  \\nImplementing  a max pooling layer in TensorFlow is quite easy. The following code creates a \\nmax pooling layer using a 2 × 2 kernel. The strides default to the kernel size, so this layer will', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 10}),\n",
       " Document(page_content='use a stride of 2 (both horizontal ly and vertically). By default, it uses  \"valid\"  padding (i.e., no \\npadding at all):  \\nmax_pool = keras.layers.MaxPool2D (pool_size =2)', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 10}),\n",
       " Document(page_content='To create an  average  pooling  layer , just use  AvgPool2D  instead of  MaxPool2D . As you might \\nexpect, it works exactly like a max pooling layer, except it computes the mean rather than the \\nmax. Average pooling layers used to be very popular, but people mostly use max pooling layers \\nnow, as they generally perform better. This may seem surprising, since computing the mean \\ngenerally los es less information than computing the max. But on the other hand, max pooling \\npreserves only the strongest features, getting rid of all the meaningless ones, so the next layers \\nget a cleaner signal to work with. Moreover, max pooling offers stronger trans lation invariance \\nthan average pooling, and it requires slightly less compute.', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 11}),\n",
       " Document(page_content='Note that max pooling and average pooling can be performed along the depth dimension rather \\nthan the spatial dimensions, although this is not as common. This can allow the CNN t o learn to \\nbe invariant to various features. For example, it could learn multiple filters, each detecting a \\ndifferent rotation of the same pattern (such as hand -written digits; see  Figure  10), and the \\ndepthwise max pooling layer would ensure that the output is the same regardless of the rotation. \\nThe CNN could similarly learn to be invariant to anything else: thickne ss, brightness, skew, \\ncolor, and so on.  \\n \\nFigure  10. Depthwise max pooling can help the CNN learn any invariance  \\nKeras does not include a depthwise max pooling layer, but TensorFlow’s low -level Deep', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 11}),\n",
       " Document(page_content='Learning API does: just use the  tf.nn.max_pool()  funct ion, and specify the kernel size and \\nstrides as 4 -tuples (i.e., tuples of size 4). The first three values of each should be 1: this indicates \\nthat the kernel size and stride along the batch, height, and width dimensions should be 1. The last \\nvalue should b e whatever kernel size and stride you want along the depth dimension —for', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 11}),\n",
       " Document(page_content='example, 3 (this must be a divisor of the input depth; it will not work if the previous layer \\noutputs 20 feature maps, since 20 is not a multiple of 3):  \\noutput = tf.nn.max_pool (images, \\n                        ksize=(1, 1, 1, 3), \\n                        strides=(1, 1, 1, 3), \\n                        padding=\"VALID\") \\nIf you want to include this as a layer in your Keras models, wrap it in a  Lambda  layer (or create a \\ncustom Keras layer):  \\ndepth_pool = keras.layers.Lambda( \\n    lambda X: tf.nn.max_pool (X, ksize=(1, 1, 1, 3), strides=(1, 1, 1, 3), \\n                             padding=\"VALID\")) \\nOne last type of pooling layer that you will often see in modern architectures is the  global', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 12}),\n",
       " Document(page_content='average  pooling  layer . It works very differently: all it does is compute the mean of each entire \\nfeature map (it’s like an average pooling layer using a pooling kernel with the same spatial \\ndimensions as the inputs). This means that it just outputs a single number p er feature map and \\nper instance. Although this is of course extremely destructive (most of the information in the \\nfeature map is lost), it can be useful as the output layer, as we will see later in this section . To \\ncreate such a layer, simply use the  keras.layers.GlobalAvgPool2D  class:  \\nglobal_avg_pool  = keras.layers.GlobalAvgPool2D () \\nIt’s equivalent to this simple  Lambda  layer, which computes the mean over the spatial \\ndimensions (height and width):', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 12}),\n",
       " Document(page_content='dimensions (height and width):  \\nglobal_avg_pool  = keras.layers.Lambda(lambda X: tf.reduce_m ean(X, axis=[1, 2])) \\nNow you know all the building blocks to create convolutional neural networks. Let’s see how to \\nassemble them.  \\nCNN Architectures  \\nTypical CNN  architectures stack a few convolutional layers (each one generally followed by a \\nReLU layer), t hen a pooling layer, then another few convolutional layers (+ReLU), then another \\npooling layer, and so on. The image gets smaller and smaller as it progresses through the \\nnetwork, but it also typically gets deeper and deeper (i.e., with more feature maps),  thanks to the \\nconvolutional layers ( see Figure  11). At the top of the stack, a regular feedforward neural', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 12}),\n",
       " Document(page_content='network is added, composed of a few fully connected layers (+ReLUs), and the final layer \\noutputs the prediction (e.g., a softmax layer that outputs estimated class probabilities).', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 12}),\n",
       " Document(page_content='Figure  11. Typical CNN architecture  \\nTIP  \\nA common mistake is to use convolution kernels that are too large. For example, instead of using a \\nconvolutional layer with a 5 × 5 kernel, stack two layers with 3 × 3 kernels: it will use fewer parameters \\nand require fewer computations, and it will usually perform better. One exception is for the first \\nconvolutional layer: it can typically have a large kernel (e.g., 5 × 5), usually with a stride of 2 or more: \\nthis will reduce the spatial dimension of the image without losing too much information, and since the \\ninput image only has three channel s in general, it will not be too costly.  \\nHere is how you can implement a simple CNN to tackle the Fashion MNIST dataset : \\nmodel = keras.models.Sequential ([', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 13}),\n",
       " Document(page_content='model = keras.models.Sequential ([ \\n    keras.layers.Conv2D(64, 7, activation =\"relu\", padding=\"same\", \\n                        input_shape =[28, 28, 1]), \\n    keras.layers.MaxPooling2D (2), \\n    keras.layers.Conv2D(128, 3, activation =\"relu\", padding=\"same\"), \\n    keras.layers.Conv2D(128, 3, activation =\"relu\", padding=\"same\"), \\n    keras.layers.MaxPooling2D (2), \\n    keras.layers.Conv2D(256, 3, activation =\"relu\", padding=\"same\"), \\n    keras.layers.Conv2D(256, 3, activation =\"relu\", padding=\"same\"), \\n    keras.layers.MaxPooling2D (2), \\n    keras.layers.Flatten(), \\n    keras.layers.Dense(128, activation =\"relu\"), \\n    keras.layers.Dropout(0.5), \\n    keras.layers.Dense(64, activation =\"relu\"), \\n    keras.layers.Dropout(0.5), \\n    keras.layers.Dense(10, activation =\"softmax\") \\n])', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 13}),\n",
       " Document(page_content=']) \\nLet’s go through this model:  \\n\\uf0b7 The first layer uses 64 fairly large filters (7 × 7) but only stride 1 because the input \\nimages are not very large. It also sets  input_shape=[28,  28, 1], because the images are \\n28 × 28 pixels, with a single color chan nel (i.e., grayscale).  \\n\\uf0b7 Next we have a max pooling layer which uses a pool size of 2, so it divides each spatial \\ndimension by a factor of 2.  \\n\\uf0b7 Then we repeat the same structure twice: two convolutional layers followed by a max \\npooling layer. For larger images , we could repeat this structure several more times (the \\nnumber of repetitions is a hyperparameter you can tune).', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 13}),\n",
       " Document(page_content='\\uf0b7 Note that the number of filters grows as we climb up the CNN toward the output layer (it \\nis initially 64, then 128, then 256): it makes sense for it to grow, since the number of low -\\nlevel features is often fairly low (e.g., small circles, horizontal lines), but there are many \\ndifferent ways to combine them into higher -level features. It is a common practice to \\ndouble the number of filters after each pooling layer: since a pooling layer divides each \\nspatial dimension by a factor of 2, we can afford to double the number of feature maps in \\nthe next layer without fear of exploding the number of parameters, memory usage, or \\ncomputational load.  \\n\\uf0b7 Next is  the fully connected network, composed of two hidden dense layers and a dense', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 14}),\n",
       " Document(page_content='output layer. Note that we must flatten its inputs, since a dense network expects a 1D \\narray of features for each instance. We also add two dropout layers, with a dropout rate of  \\n50% each, to reduce overfitting.  \\nThis CNN reaches over 92% accuracy on the test set. It’s not state of the art, but it is pretty good, \\nand clearly much better than what we achieved with dense networks in previous section . \\nOver the years, variants of this fundamental architecture have been developed, leading to \\namazing advances in the field. A good measure of this progress is the error rate  in competitions \\nsuch as the ILSVRC  ImageNet  challenge . In this competition the top -five error rate for image', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 14}),\n",
       " Document(page_content='classification fell from over 26% to less than 2.3% in just six years. The top -five error rate is the \\nnumber  of test images for which the system’s top five predictions did not include the correct \\nanswer. The images are large (256 pixels high) and there are 1,000 classes, some of which are \\nreally subtle (try distinguishing 120 dog breeds). Looking at the evolutio n of the winning entries \\nis a good way to understand how CNNs work.  \\nWe will first look at the classical LeNet -5 architecture (1998), then three of the winners of the \\nILSVRC challenge: AlexNet (2012), GoogLeNet (2014), and ResNet (2015).  \\nLeNet -5 \\nThe LeNet -5 architecture  is perhaps the most widely known CNN architecture. As mentioned', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 14}),\n",
       " Document(page_content='earlier, it was  created by Yann LeCun in 1998 and has been widely used for handwritten digit \\nrecognition (MNIST). It is composed of the layers shown in  Table  1. \\nLayer  Type  Maps  Size Kernel size  Stride  Activation  \\nOut Fully connected  – 10 – – RBF  \\nF6 Fully connected  – 84 – – tanh \\nC5 Convolution  120 1 × 1  5 × 5  1 tanh', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 14}),\n",
       " Document(page_content='Layer  Type  Maps  Size Kernel size  Stride  Activation  \\nS4 Avg pooling  16 5 × 5  2 × 2  2 tanh \\nC3 Convolution  16 10 × 10  5 × 5  1 tanh \\nS2 Avg pooling  6 14 × 14  2 × 2  2 tanh \\nC1 Convolution  6 28 × 28  5 × 5  1 tanh \\nIn Input  1 32 × 32  – – – \\nTable  1. LeNet -5 architecture  \\nThere are a few extra details to be noted:  \\n\\uf0b7 MNIST images are 28 × 28 pixels, but they are zero -padded to 32 × 3 2 pixels and \\nnormalized before being fed to the network. The rest of the network does not use any \\npadding, which is why the size keeps shrinking as the image progresses through the \\nnetwork.  \\n\\uf0b7 The average pooling layers are slightly more complex than usual: e ach neuron computes \\nthe mean of its inputs, then multiplies the result by a learnable coefficient (one per map)', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 15}),\n",
       " Document(page_content='and adds a learnable bias term (again, one per map), then finally applies the activation \\nfunction.  \\n\\uf0b7 Most neurons in C3 maps are connected to neur ons in only three or four S2 maps (instead \\nof all six S2 maps). See table  1 (page 8) in the original paper  for details.  \\n\\uf0b7 The output lay er is a bit special: instead of computing the matrix multiplication of the \\ninputs and the weight vector, each neuron outputs the square of the Euclidian distance \\nbetween its input vector and its weight vector. Each output measures how much the \\nimage belong s to a particular digit class. The cross -entropy cost function is now \\npreferred, as it penalizes bad predictions much more, producing larger gradients and \\nconverging faster.', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 15}),\n",
       " Document(page_content='converging faster.  \\nYann LeCun’s  website  features great demos of LeNet -5 classifying digits.  \\nAlexNet  \\nThe AlexNet  CNN  architecture  won the 2012 ImageNet ILSVRC challenge by a large margin: it \\nachieved a top -five error rate of 17%, while the second best achieved only 26%! It was \\ndeveloped by Alex Krizhevsky (hence the name), Ilya Sutskever, and Geoffrey Hinton. It is \\nsimilar to LeNet -5, only much larger and deeper, and it was the first to stack convolutional layers', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 15}),\n",
       " Document(page_content='directly on top of one another, instead of stacking a pooling layer on top of each convolutional \\nlayer.  Table  2 presents this architecture.  \\nLaye\\nr Type  Maps  Size Kerne\\nl size  Strid\\ne Paddin\\ng Activatio\\nn \\nOut Fully \\nconnected  – 1,00\\n0 – – – Softmax  \\nF10 Fully \\nconnected  – 4,09\\n6 – – – ReLU  \\nF9 Fully  \\nconnected  – 4,09\\n6 – – – ReLU  \\nS8 Max \\npooling  256 6 × 6  3 × 3  2 valid  – \\nC7 Convolutio\\nn 256 13 × \\n13 3 × 3  1 same  ReLU  \\nC6 Convolutio\\nn 384 13 × \\n13 3 × 3  1 same  ReLU  \\nC5 Convolutio\\nn 384 13 × \\n13 3 × 3  1 same  ReLU  \\nS4 Max \\npooling  256 13 × \\n13 3 × 3  2 valid  – \\nC3 Convolutio\\nn 256 27 × \\n27 5 × 5  1 same  ReLU  \\nS2 Max \\npooling  96 27 × \\n27 3 × 3  2 valid  – \\nC1 Convolutio\\nn 96 55 × \\n55 11 × \\n11 4 valid  ReLU', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 16}),\n",
       " Document(page_content='Laye\\nr Type  Maps  Size Kerne\\nl size  Strid\\ne Paddin\\ng Activatio\\nn \\nIn Input  3 \\n(RGB\\n) 227 \\n× \\n227 – – – – \\nTable  2. AlexNet architecture  \\nTo reduce overfitting, the authors used two regula rization techniques . First, they applied dropout  \\nwith a 50% dropout rate during training to the outputs  of layers F9 and F10. Second, they \\nperformed  data augmentation  by randomly shifting the training images by various offsets, \\nflipping them horizontally, and changing the lighting conditions.  \\nDATA AUGMENTATION  \\nData augmentation artificially increases the si ze of the training set by generating many realistic \\nvariants of each training instance. This reduces overfitting, making this a regularization', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 17}),\n",
       " Document(page_content='technique. The generated instances should be as realistic as possible: ideally, given an image \\nfrom the augmented  training set, a human should not be able to tell whether it was augmented or \\nnot. Simply adding white noise will not help; the modifications should be learnable (white noise \\nis not).  \\nFor example, you can slightly shift, rotate, and resize every picture in  the training set by various \\namounts and add the resulting pictures to the training set (see  Figure  12). This forc es the model \\nto be more tolerant to variations in the position, orientation, and size of the objects in the \\npictures. For a model that’s more tolerant of different lighting conditions, you can similarly \\ngenerate many images with various contrasts. In gener al, you can also flip the pictures', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 17}),\n",
       " Document(page_content='horizontally (except for text, and other asymmetrical objects). By combining these \\ntransformations, you can greatly increase the size of your training set.  \\n \\nFIGURE  12. GENERATING NEW TRAIN ING INSTANCES FROM E XISTING O NES', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 17}),\n",
       " Document(page_content='AlexNet  also uses a competitive normalization step immediately after the ReLU step of layers \\nC1 and C3, called  local  response  normalization  (LRN): the most strongly activated neurons \\ninhibit other neurons located at the same position in neighboring fea ture maps (such competitive \\nactivation has been observed in biological neurons). This encourages different feature maps to \\nspecialize, pushing them apart and forcing them to explore a wider range of features, ultimately \\nimproving generalization.  Equation  2 shows  how to apply LRN.  \\nEquation  2. Local response normalization (LRN)  \\n \\nIn this equation:  \\n\\uf0b7 bi is the normalized output of the neuron located in feature map  i, at some row  u and \\ncolumn  v (note that in this equation we consider only neurons located at this row and', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 18}),\n",
       " Document(page_content='column, so  u and v are not shown).  \\n\\uf0b7 ai is the activation of that neuron after the ReLU step, but before normalization.  \\n\\uf0b7 k, α, β, and  r are hyperparameters.  k is called the  bias, and  r is called the  depth  radius . \\n\\uf0b7 fn is the number of feature maps.  \\nFor example, if  r = 2 and a neuron has a strong activation, it will inhibit the activation of the \\nneurons located in the feature maps immediately above and below its own.  \\nIn AlexNet, the hyperparameters are set as follows:  r = 5, α = 0.0001,  β = 0.75, and  k = 2. This \\nstep can be implemented using the  tf.nn.local_response_normalization()  function \\n(which you can wrap in a  Lambda  layer if you want to use it in a Keras model).  \\nA variant of AlexNet called  ZF Net was developed by Matthew Zeiler and Rob Fergus and won', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 18}),\n",
       " Document(page_content='the 2013 ILSVRC challenge. It is essentially AlexNet with a few tweaked hyperparameters \\n(numbe r of feature maps, kernel size, stride, etc.).  \\nGoogLeNet  \\nThe GoogLeNet  architecture  was developed by Christian Szegedy et al. from Google \\nResearch,  and it won the ILSVRC 2014 challenge by pushing the top -five error rate below 7%. \\nThis great performance came in large part from the fact that the network was much deeper than \\nprevious CNNs (as you’ll see in  Figure  14). This was made possible by subnetworks \\ncalled  inception  modules , which allow GoogLeNet to use parameters much more efficiently than \\nprevious architectures: GoogLeNet actually has 10 times fewer parameters than AlexNet \\n(roughly 6 million instead of 60 million).', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 18}),\n",
       " Document(page_content='(roughly 6 million instead of 60 million).  \\nFigure  13 shows the architecture of an inception module. The notation “3 ×  3 + 1(S)” means that \\nthe layer uses a 3 × 3 kernel, stride 1, and  \"same\"  padding. The input signal is first copied and \\nfed to four different layers. All convolutional layers use the ReLU activation function. Note that', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 18}),\n",
       " Document(page_content='the second set of convolutional layer s uses different kernel sizes (1 × 1, 3 × 3, and 5 × 5), \\nallowing them to capture patterns at different scales. Also note that every single layer uses a \\nstride of 1 and  \"same\"  padding (even the max pooling layer), so their outputs all have the same \\nheight and width as their inputs. This makes it possible to concatenate all the outputs along the \\ndepth dimension  in the final  depth  concatenation  layer  (i.e., stack the feature maps from all four \\ntop convolutional layers). This concatenation layer can be impleme nted in TensorFlow using \\nthe tf.concat()  operation, with  axis=3  (the axis is the depth).  \\n \\nFigure  13. Inception module  \\nYou may wonder why inception modules have convolutional layers with 1 × 1 kernels. Surely', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 19}),\n",
       " Document(page_content='these layers cannot capture any features bec ause they look at only one pixel at a time? In fact, the \\nlayers serve three purposes:  \\n\\uf0b7 Although they cannot capture spatial patterns, they can capture patterns along the depth \\ndimension.  \\n\\uf0b7 They  are configured to output fewer feature maps than their inputs, so  they serve \\nas bottleneck  layers , meaning they reduce dimensionality. This cuts the computational \\ncost and the number of parameters, speeding up training and improving generalization.  \\n\\uf0b7 Each pair of convolutional layers ([1 × 1, 3 × 3] and [1 × 1, 5 × 5]) ac ts like a single \\npowerful convolutional layer, capable of capturing more complex patterns. Indeed, \\ninstead of sweeping a simple linear classifier across the image (as a single convolutional', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 19}),\n",
       " Document(page_content='layer does), this pair of convolutional layers sweeps a two -layer neural network across \\nthe image.  \\nIn short, you can think of the whole inception module as a convolutional layer on steroids, able \\nto output feature maps that capture complex patterns at various scales.  \\nWARNING  \\nThe number of convolutional kernels for each c onvolutional layer is a hyperparameter. Unfortunately, this \\nmeans that you have six more hyperparameters to tweak for every inception layer you add.', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 19}),\n",
       " Document(page_content='Now let’s look at the architecture of the GoogLeNet CNN ( see Figure  14). The number of \\nfeature maps output by each convolutional layer and each pooling layer is shown before the \\nkernel size. The architecture is so deep that it ha s to be represented in three columns, but \\nGoogLeNet is actually one tall stack, including nine inception modules (the boxes with the \\nspinning tops). The six numbers in the inception modules represent the number of feature maps \\noutput by each convolutional layer in the module (in the same order as in  Figure  13). Note that \\nall the convolutional layers use the ReLU activa tion function.  \\n \\nFigure  14. GoogLeNet architecture  \\nLet’s go through this network:  \\n\\uf0b7 The first two layers divide the image’s height and width by 4 (so its area is divided by', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 20}),\n",
       " Document(page_content='16), to reduce the computational load. The first layer uses a large kernel size so  that much \\nof the information is preserved.  \\n\\uf0b7 Then the local response normalization layer ensures that the previous layers learn a wide \\nvariety of features (as discussed earlier).  \\n\\uf0b7 Two convolutional layers follow, where the first acts like a bottleneck layer.  As \\nexplained earlier, you can think of this pair as a single smarter convolutional layer.  \\n\\uf0b7 Again, a local response normalization layer ensures that the previous layers capture a \\nwide variety of patterns.  \\n\\uf0b7 Next, a max pooling layer reduces the image height a nd width by 2, again to speed up \\ncomputations.', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 20}),\n",
       " Document(page_content='\\uf0b7 Then comes the tall stack of nine inception modules, interleaved with a couple max \\npooling layers to reduce dimensionality and speed up the net.  \\n\\uf0b7 Next, the global average pooling layer outputs the mean of each feature map: this drops \\nany remaining spatial information, which is fine because there was not much spatial \\ninformation left at that point. Indeed, GoogLeNet input images are typically expected to \\nbe 224 × 224 pixels, so after 5 max pooling layers, each di viding the height and width by \\n2, the feature maps are down to 7 × 7. Moreover, it is a classification task, not \\nlocalization, so it does not matter where the object is. Thanks to the dimensionality \\nreduction brought by this layer, there is no need to have  several fully connected layers at', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 21}),\n",
       " Document(page_content='the top of the CNN (like in AlexNet), and this considerably reduces the number of \\nparameters in the network and limits the risk of overfitting.  \\n\\uf0b7 The last layers are self -explanatory: dropout for regularization, then a full y connected \\nlayer with 1,000 units (since there are 1,000 classes) and a softmax activation function  to \\noutput estimated class probabilities.  \\nThis diagram is slightly simplified: the original GoogLeNet architecture also included two \\nauxiliary classifiers p lugged on top of the third and sixth inception modules. They were both \\ncomposed of one average pooling layer, one convolutional layer, two fully connected layers, and \\na softmax activation layer. During training, their loss (scaled down by 70%) was added to  the', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 21}),\n",
       " Document(page_content='overall loss. The goal was to fight the vanishing gradients problem and regularize the network. \\nHowever, it was later shown that their effect was relatively minor.  \\nSeveral variants of the GoogLeNet architecture were later proposed by Google researcher s, \\nincluding Inception -v3 and Inception -v4, using slightly different inception modules and reaching \\neven better performance.  \\nVGGNet  \\nThe runner -up in the ILSVRC 2014 challenge was  VGGNet , developed by Karen Simonyan and \\nAndrew Zisserman from the Visual Geometry Group (VGG) research lab at Oxford University. \\nIt had a very simple and classical  architecture, with 2 or 3 convolutional layers and a pooling \\nlayer, then again 2 or 3 convolutional layers and a pooling layer, and so on (reaching a total of', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 21}),\n",
       " Document(page_content='just 16 or 19 convolutional layers, depending on the VGG variant), plus a final dense network \\nwith 2 hidden layers and the output layer. It used only 3 × 3 filters, but many filters.  \\nResNet  \\nKaiming He et al. won  the ILSVRC 2015 challenge using a  Residual  Network  (or ResNet ), that \\ndelivered an astounding top -five error rate under 3.6%. The winning variant used an extremely \\ndeep CNN composed of 152 layers (other variants had 34, 50, and  101 layers). It confirmed the \\ngeneral trend: models are getting deeper and deeper, with fewer and fewer parameters. The key \\nto being able to train such a deep network is to  use skip connections  (also called  shortcut  \\nconnections ): the signal feeding into a  layer is also added to the output of a layer located a bit', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 21}),\n",
       " Document(page_content='higher up the stack. Let’s see why this is useful.', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 21}),\n",
       " Document(page_content='When training a neural network, the goal is to make it model a target function  h(x). If you add \\nthe input  x to the output of the network (i.e., y ou add a skip connection), then the network will be \\nforced to model  f(x) = h(x) – x rather than  h(x). This  is called  residual  learning  (see Figure  15). \\n \\nFigure  15. Residual learning  \\nWhen you initialize a regular neural network, its weights are close to zero, so the network just \\noutputs values close to zero. If you add a skip connection, the resulting network just outputs a \\ncopy of its inputs; in other words, it initially models the identity function. If the target function is \\nfairly close to the identity function (which is often the case), this will speed up training \\nconsiderably.', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 22}),\n",
       " Document(page_content='considerably.  \\nMoreover, if you add many skip con nections, the network can start making progress even if \\nseveral layers have not started learning yet (see  Figure  16). Thanks to skip connections, the \\nsignal can easily make its way across the whole network. The deep residual network can be seen \\nas a stack of  residual  units  (RUs), where each residual unit is a small neural network with a skip \\nconnection.  \\n \\nFigure  16. Regular deep neural network (left) and deep residual network (right)', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 22}),\n",
       " Document(page_content='Now let’s look at ResNet’s architecture ( see Figure  17). It is surprisingly simple. It starts and \\nends exactly like GoogLeNet (except without a dropout layer), and in between is just a very deep \\nstack of simple residual units. Each residual unit is composed of two convolutional layers (and \\nno pooling laye r!), with Batch Normalization (BN) and ReLU activation, using 3 × 3 kernels and \\npreserving spatial dimensions (stride 1,  \"same\"  padding).  \\n \\nFigure  17. ResNet architecture  \\nNote that the number of feature maps is doubled every few residual units, at the s ame time as \\ntheir height and width are halved (using a convolutional layer with stride 2). When this happens, \\nthe inputs cannot be added directly to the outputs of the residual unit because they don’t have the', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 23}),\n",
       " Document(page_content='same shape (for example, this problem affects the skip connection represented by the dashed \\narrow in  Figure  17). To solve this problem, the inputs are passed through a 1 ×  1 convolutional \\nlayer with stride 2 and the right number of output feature maps (see  Figure  18). \\n \\nFigure  18. Skip connection when changing feature map size and depth', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 23}),\n",
       " Document(page_content='ResNet -34 is the ResNet with 34 layers (only counting the convolutional layers and the fully \\nconnected layer)  containing 3 residual units that output 64 feature maps, 4 RUs with 128 maps, 6 \\nRUs with 256 maps, and 3 RUs with 512 maps. We will implement this architecture later in this \\nsection . \\nResNets deeper than that , such as ResNet -152, use slightly different residual units. Instead of \\ntwo 3 × 3 convolutional layers with, say, 256 feature maps, they use three convolutional layers: \\nfirst a 1 × 1 convolutional layer with just 64 feature maps (4 times less), which acts as a \\nbottleneck layer (as discussed already), then a 3 × 3 layer with 64 feature maps, and finally \\nanother 1 × 1 convolutional layer with 256 feature maps (4 times 64) that restores the original', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 24}),\n",
       " Document(page_content='depth. ResNet -152 contains 3 such RUs that output 256 maps, t hen 8 RUs with 512 maps, a \\nwhopping 36 RUs with 1,024 maps, and finally 3 RUs with 2,048 maps.  \\nNOTE  \\nGoogle’s  Inception -v4 architecture merged the ideas of GoogLeNet and ResNet and achieved a top -five \\nerror rate of close to 3% on ImageNet classification.  \\nXception  \\nAnother  variant of the GoogLeNet architecture is worth noting:  Xception  (which stands \\nfor Extreme  Inception ) was proposed in 2016 by François Chollet (the auth or of Keras), and it \\nsignificantly outperformed Inception -v3 on a huge vision task (350 million images and 17,000 \\nclasses). Just like Inception -v4, it merges the ideas of GoogLeNet and ResNet, but it replaces', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 24}),\n",
       " Document(page_content='the inception modules with a special type of la yer called a  depthwise  separable  convolution  \\nlayer  (or separable  convolution  layer  for short ). These layers had been used b efore in some CNN \\narchitectures, but they were not as central as in the Xception architecture. While a regular \\nconvolutional layer uses filters that try to simultaneously capture spatial patterns (e.g., an oval) \\nand cross -channel patterns (e.g., mouth + no se + eyes = face), a separable convolutional layer \\nmakes the strong assumption that spatial patterns and cross -channel patterns can be modeled \\nseparately (see  Figure  19). Thus, it is composed of two parts: the first part applies a single spatial \\nfilter for each input feature map, then the second part looks exclusively for cross -channel', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 24}),\n",
       " Document(page_content='patterns —it is just a regula r convolutional layer with 1 × 1 filters.  \\n \\nFigure  19. Depthwise separable convolutional layer', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 24}),\n",
       " Document(page_content='Since separable convolutional layers only have one spatial filter per input channel, you should \\navoid using them after layers that have too few channels, such  as the input layer (granted, that’s \\nwhat  Figure  19 represents, but it is just for illustration purposes). For  this reason, the Xception \\narchitecture starts with 2 regular convolutional layers, but then the rest of the architecture uses \\nonly separable convolutions (34 in all), plus a few max pooling layers and the usual final layers \\n(a global average pooling layer  and a dense output layer).  \\nYou might wonder why Xception is considered a variant of GoogLeNet, since it contains no \\ninception module at all. Well, as we discussed earlier, an inception module contains', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 25}),\n",
       " Document(page_content='convolutional layers with 1 × 1 filters: these look ex clusively for cross -channel patterns. \\nHowever, the convolutional layers that sit on top of them are regular convolutional layers that \\nlook both for spatial and cross -channel patterns. So you can think of an inception module as an \\nintermediate between a reg ular convolutional layer (which considers spatial patterns and cross -\\nchannel patterns jointly) and a separable convolutional layer (which considers them separately). \\nIn practice, it seems that separable convolutional layers generally perform better.  \\nTIP  \\nSeparable convolutional layers use fewer parameters, less memory, and fewer computations than regular \\nconvolutional layers, and in general they even perform better, so you should consider using them by', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 25}),\n",
       " Document(page_content='default (except after layers with few channels).  \\nThe ILS VRC 2016 challenge was won by the CUImage team from the Chinese University of \\nHong Kong. They used an ensemble of many different techniques, including a sophisticated \\nobject -detection system called  GBD -Net, to achieve a top -five error rate below 3%. Although \\nthis result is unquestionably impressive, the complexity of the solution contra sted with the \\nsimplicity of ResNets. Moreover, one year later another fairly simple architecture performed \\neven better, as we will see now.  \\nSENet  \\nThe winning architecture in the ILSVRC 2017 challenge was the  Squeeze -and-Excitation  \\nNetwork  (SENet) . This architecture extends existing architectures such as inception networks', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 25}),\n",
       " Document(page_content='and ResNets, an d boosts their performance. This allowed SENet to win the competition with an \\nastonishing 2.25% top -five error rate! The extended versions of inception networks  and ResNets \\nare called  SE-Inception  and SE-ResNet , respectively. The boost comes from the fact that a SENet \\nadds a small neural network, called an  SE block , to every unit in the original architecture (i.e., \\nevery inception module or every residual unit), as shown in  Figure  20.', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 25}),\n",
       " Document(page_content='Figure  20. SE-Inception module (left) and SE -ResNet unit (right)  \\nAn SE block analyzes the output of the unit it is attached to, focusing exclusively on the depth \\ndimension (it does not look for any spatial pattern), and it learns which features are usually most \\nactive together. It then uses this information to recalibrate the feature maps, as shown \\nin Figure  21. For example, an SE block may learn that mouths, noses, and eyes usually appear \\ntogether in pictures: if you see a mouth and a nose, you should expect to see eyes as well. So if \\nthe block sees a strong ac tivation in the mouth and nose feature maps, but only mild activation in \\nthe eye feature map, it will boost the eye feature map (more accurately, it will reduce irrelevant', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 26}),\n",
       " Document(page_content='feature maps). If the eyes were somewhat confused with something else, this feature map \\nrecalibration will help resolve the ambiguity.  \\n \\nFigure  21. An SE block performs feature map recalibration  \\nAn SE block is composed of just three layers: a global average pooling layer, a hidden dense \\nlayer using the ReLU activation function, and a d ense output layer using the sigmoid activation \\nfunction (see  Figure  22).', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 26}),\n",
       " Document(page_content='Figure  22. SE block architecture  \\nAs earlier, t he global average pooling layer computes the mean activation for each feature map: \\nfor example, if its input contains 256 feature maps, it will output 256  numbers  representing the \\noverall level of response for each filter. The next layer is where the “sque eze” happens: this layer \\nhas significantly fewer than 256 neurons —typically 16 times fewer than the number of feature \\nmaps (e.g., 16 neurons) —so the 256 numbers get compressed into a small vector (e.g., 16 \\ndimensions). This is a low -dimensional vector repr esentation (i.e., an embedding) of the \\ndistribution of feature responses. This bottleneck step forces the SE block to learn a general', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 27}),\n",
       " Document(page_content='representation of the feature combinations (we will see this principle in action again when we \\ndiscuss autoencoders in later  section ). Finally, the output layer takes the embedding and outputs \\na recalibration vector containing one number per featu re map (e.g., 256), each between 0 and 1. \\nThe feature maps are then multiplied by this recalibration vector, so irrelevant features (with a \\nlow recalibration score) get scaled down while relevant features (with a recalibration score close \\nto 1) are left al one. \\nImplementing a ResNet -34 CNN Using Keras  \\nMost  CNN architectures described so far are fairly straightforward to implement (although \\ngenerally you would load a pretrained network instead, as we will see). To illustrate the process,', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 27}),\n",
       " Document(page_content='let’s implement a Res Net-34 from scratch using Keras. First, let’s create \\na ResidualUnit  layer:  \\nclass ResidualUnit (keras.layers.Layer): \\n    def __init__ (self, filters, strides=1, activation =\"relu\", **kwargs): \\n        super().__init__ (**kwargs) \\n        self.activation  = keras.activations .get(activation ) \\n        self.main_layers  = [ \\n            keras.layers.Conv2D(filters, 3, strides=strides, \\n                                padding=\"same\", use_bias =False), \\n            keras.layers.BatchNormalization (), \\n            self.activation , \\n            keras.layers.Conv2D(filters, 3, strides=1, \\n                                padding=\"same\", use_bias =False), \\n            keras.layers.BatchNormalization ()] \\n        self.skip_layers  = [] \\n        if strides > 1:', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 27}),\n",
       " Document(page_content='self.skip_layers  = [ \\n                keras.layers.Conv2D(filters, 1, strides=strides, \\n                                    padding=\"same\", use_bias =False), \\n                keras.layers.BatchNormalization ()] \\n \\n    def call(self, inputs): \\n        Z = inputs \\n        for layer in self.main_layers : \\n            Z = layer(Z) \\n        skip_Z = inputs \\n        for layer in self.skip_layers : \\n            skip_Z = layer(skip_Z) \\n        return self.activation (Z + skip_Z) \\nAs you can see, this code matches  Figure  18 pretty  closely. In the constructor, we create all the \\nlayers we will need: the main layers are the ones on the right side of the diagram, a nd the skip \\nlayers are the ones on the left (only needed if the stride is greater than 1). Then in', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 28}),\n",
       " Document(page_content='the call()  method, we make the inputs go through the main layers and the skip layers (if any), \\nthen we add both outputs and apply the activation function.  \\nNext, we can build the ResNet -34 using a  Sequential  model, since it’s really just a long \\nsequence of layers (we can treat each residual unit as a single layer now that we have \\nthe ResidualUnit  class):  \\nmodel = keras.models.Sequential () \\nmodel.add(keras.layers.Conv2D(64, 7, strides=2, input_shape =[224, 224, 3], \\n                              padding=\"same\", use_bias =False)) \\nmodel.add(keras.layers.BatchNormalization ()) \\nmodel.add(keras.layers.Activation (\"relu\")) \\nmodel.add(keras.layers.MaxPool2D (pool_size =3, strides=2, padding=\"same\")) \\nprev_filters  = 64 \\nfor filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 28}),\n",
       " Document(page_content='strides = 1 if filters == prev_filters  else 2 \\n    model.add(ResidualUnit (filters, strides=strides)) \\n    prev_filters  = filters \\nmodel.add(keras.layers.GlobalAvgPool2D ()) \\nmodel.add(keras.layers.Flatten()) \\nmodel.add(keras.layers.Dense(10, activation =\"softmax\" )) \\nThe only slightly tricky part in this code is the loop that adds the  ResidualUnit  layers to the \\nmodel: as explained earlier, the first 3 RUs have 64 filters, then the next 4 RUs have 128 filters, \\nand so on. We then set the stride to 1 when the number of filters is the same as in the previous \\nRU, or else we set it to 2. Then we add the  ResidualUnit , and finally we \\nupdate  prev_filters . \\nIt is amazing that  in fewer than 40 lines of code, we can build the model that won the ILSVRC', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 28}),\n",
       " Document(page_content='2015 challenge! This demonstrates both the elegance of the ResNet model and the \\nexpressiveness of the Keras API. Implementing the other CNN architectures is not much harder. \\nHoweve r, Keras comes with several of these architectures built in, so why not use them instead?', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 28}),\n",
       " Document(page_content='Using Pretrained Models from Keras  \\nIn general, you won’t have to implement standard models like GoogLeNet or ResNet manually, \\nsince pretrained networks are readily av ailable with a single line of code in \\nthe keras.applications  package. For example, you can load the ResNet -50 model, \\npretrained on ImageNet, with the following line of code:  \\nmodel = keras.applications .resnet50 .ResNet50 (weights=\"imagenet\" ) \\nThat’s all! This will create a ResNet -50 model and download weights pretrained on the \\nImageNet dataset. To use it, you first need to ensure that the images have the right size. A \\nResNet -50 model expects 224 × 224 -pixel images (other models may expect other sizes, such as \\n299 × 299), so let’s use TensorFlow’s  tf.image.resize()  function to resize the images we', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 29}),\n",
       " Document(page_content='loaded earlier:  \\nimages_resized  = tf.image.resize(images, [224, 224]) \\nTIP  \\nThe tf.image.resize()  will not preserve the aspect ratio. If this is a problem, try cropping th e images \\nto the appropriate aspect ratio before resizing. Both operations can be done in one shot \\nwith tf.image.crop_and_resize() . \\nThe pretrained models assume that the images are preprocessed in a specific way. In some cases \\nthey may expect the inputs to be scaled from 0 to 1, or –1 to 1, and so on. Each model provides \\na preprocess_input()  function that you can use to preprocess your images. These functions \\nassume that the pixel values range from 0 to 255, so we must multiply them by 255 (since earlier \\nwe scaled them to the 0 –1 range):', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 29}),\n",
       " Document(page_content='we scaled them to the 0 –1 range):  \\ninputs = keras.applications .resnet50 .preprocess_input (images_resized  * 255) \\nNow we can use the pretrained model to make predictions:  \\nY_proba = model.predict(inputs) \\nAs usual, the output  Y_proba  is a matrix with one row per im age and one column per class (in \\nthis case, there are 1,000 classes). If you want to display the top  K predictions, including the \\nclass name and the estimated probability of each predicted class, use \\nthe decode_predictions()  function. For each image, it re turns an array containing the \\ntop K predictions, where each prediction is represented as an array containing the class \\nidentifier,  its name, and the corresponding confidence score:  \\ntop_K = keras.applications .resnet50 .decode_predictions (Y_proba, top=3)', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 29}),\n",
       " Document(page_content='for image_index  in range(len(images)): \\n    print(\"Image #{}\".format(image_index )) \\n    for class_id , name, y_proba in top_K[image_index]: \\n        print(\"  {} - {:12s} {:.2f}%\" .format(class_id , name, y_proba * 100))', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 29}),\n",
       " Document(page_content='print() \\nThe output looks like this:  \\nImage #0  \\n  n03877845 - palace       42.87%  \\n  n02825657 - bell_cote    40.57%  \\n  n03781244 - monastery    14.56%  \\n \\nImage #1  \\n  n04522168 - vase         46.83%  \\n  n07930864 - cup          7.78%  \\n  n11939491 - daisy        4.87%  \\nThe correct classes (monastery and daisy) appear in the top three results for both images. That’s \\npretty good, considering that the model had to choose from among 1 ,000 classes.  \\nAs you can see, it is very easy to create a pretty good image classifier using a pretrained model. \\nOther vision models are available in  keras.applications , including several ResNet variants, \\nGoogLeNet variants like Inception -v3 and Xception, VGGNet variants, and MobileNet and', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 30}),\n",
       " Document(page_content='MobileNetV2 (lightweight models for use in mobile applications).  \\nBut what if you want to use an image classifier for classes of images that are not part of \\nImageNet? In that case, you may still benefit from the pretrained  models to perform transfer \\nlearning.  \\nPretrained Models for Transfer Learning  \\nIf you want to build an image classifier but you do not have enough training data, then it is often \\na good idea to reuse the low er layers of a pretrained model . For example, let’s train a model to \\nclassify pictures of flowers, reusing a pretrained Xception model. First, let’s load the dat aset \\nusing TensorFlow Datasets : \\nimport tensorflow_datasets  as tfds \\n \\ndataset, info = tfds.load(\"tf_flowers\", as_supervised =True, with_info =True)', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 30}),\n",
       " Document(page_content='dataset_size  = info.splits[\"train\"].num_examples  # 3670 \\nclass_names  = info.features [\"label\"].names # [\"dandelion\",  \"daisy\",  ...] \\nn_classes  = info.features [\"label\"].num_classes  # 5 \\nNote that you can get information  about the dataset by setting  with_info=True . Here, we get \\nthe dataset size and the names of the classes. Unfortunately, there is only a  \"train\"  dataset, no \\ntest set or validation set, so we need to split the training set. The TF Datasets project provides an \\nAPI for this. For example, let’s take the first 10% of the dataset for testing, the next 15% for \\nvalidation, and the remaining 75% for  training : \\ntest_set_raw , valid_set_raw , train_set_raw  = tfds.load( \\n    \"tf_flowers\" ,', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 30}),\n",
       " Document(page_content='split=[\"train[:10%]\" , \"train[1 0%:25%]\" , \"train[25%:]\" ], \\n    as_supervised =True) \\nNext we must preprocess the images. The CNN expects 224 × 224 images, so we need to resize \\nthem. We also need to run the images through Xception’s  preprocess_input()  function:  \\ndef preprocess (image, label): \\n    resized_image  = tf.image.resize(image, [224, 224]) \\n    final_image  = keras.applications .xception .preprocess_input (resized_image ) \\n    return final_image , label \\nLet’s apply this preprocessing function to all three datasets, shuffle the training set, and add \\nbatching and prefetching to all the datasets:  \\nbatch_size  = 32 \\ntrain_set  = train_set .shuffle(1000) \\ntrain_set  = train_set .map(preprocess ).batch(batch_size ).prefetch (1)', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 31}),\n",
       " Document(page_content='valid_set  = valid_set .map(preprocess ).batch(batch_size ).prefetch (1) \\ntest_set = test_set.map(preprocess ).batch(batch_size ).prefetch (1) \\nIf you want to perform some data augmentation, change the preprocessing function for the \\ntraining set, adding some random transformations to the training images. For example, \\nuse tf.image.random_crop()  to ran domly crop the images, \\nuse tf.image.random_flip_left_right()  to randomly flip the images horizontally, and \\nso on (see the “Pretrained Models for Transfer Learning” section of the notebook for an \\nexample).  \\nTIP  \\nThe keras.preprocessing.image.ImageDataGenerato r class makes it easy to load images from \\ndisk and augment them in various ways: you can shift each image, rotate it, rescale it, flip it horizontally', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 31}),\n",
       " Document(page_content='or vertically, shear it, or apply any transformation function you want to it. This is very convenient for  \\nsimple projects. However, building a tf.data pipeline has many advantages: it can read the images \\nefficiently (e.g., in parallel) from any source, not just the local disk; you can manipulate the  Dataset  as \\nyou wish; and if you write a preprocessing functi on based on  tf.image  operations, this function can be \\nused both in the tf.data pipeline and in the model you will deplo y to production . \\nNext let’s load an Xception model, pretrained on ImageNet. We exclude the top of the network \\nby setting  include_top=False : this excludes the global average pooling layer and the dense \\noutput layer. We then add our own global av erage pooling layer, based on the output of the base', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 31}),\n",
       " Document(page_content='model, followed by a dense output layer with one unit per class, using the  softmax activation \\nfunction. Finally, we create the Keras  Model : \\nbase_model  = keras.applications .xception .Xception (weights=\"imagenet\", \\n                                                  include_top =False) \\navg = keras.layers.GlobalAveragePooling2D ()(base_model .output) \\noutput = keras.layers.Dense(n_classes , activation =\"softmax\" )(avg) \\nmodel = keras.Model(inputs=base_model .input, outputs=output)', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 31}),\n",
       " Document(page_content='As explained before , it’s usually a good idea to freeze the weights of the pretrained layers, at \\nleast at the beg inning of training:  \\nfor layer in base_model .layers: \\n    layer.trainable  = False \\nNOTE  \\nSince our model uses the base model’s layers directly, rather than the  base_model  object itself, \\nsetting  base_model.trainable=False  would have no effect.  \\nFinally, we can c ompile the model and start training:  \\noptimizer  = keras.optimizers .SGD(lr=0.2, momentum =0.9, decay=0.01) \\nmodel.compile(loss=\"sparse_categorical_crossentropy\" , optimizer =optimizer , \\n              metrics=[\"accuracy\" ]) \\nhistory = model.fit(train_set , epochs=5, validation_data =valid_set ) \\nWARNING  \\nThis will be very slow, unless you have a GPU. If you do not, then you should run this code in Colab,', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 32}),\n",
       " Document(page_content='using a GPU runtime (it’s free!).  \\nAfter training the model for a few epochs, its validation accuracy should reach about 75 –80% \\nand stop making much progress. This means that the top layers are now pretty well trained, so \\nwe are ready to unfreez e all the layers (or you could try unfreezing just the top ones) and \\ncontinue training (don’t forget to compile the model when you freeze or unfreeze layers). This \\ntime we use a much lower learning rate to avoid damaging the pretrained weights:  \\nfor layer in base_model .layers: \\n    layer.trainable  = True \\n \\noptimizer  = keras.optimizers .SGD(lr=0.01, momentum =0.9, decay=0.001) \\nmodel.compile(...) \\nhistory = model.fit(...) \\nIt will take a while, but this model should reach around 95% accuracy on the test set. With th at,', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 32}),\n",
       " Document(page_content='you can start training amazing image classifiers! But there’s more to computer vision than just \\nclassification. For example, what if you also want to know  where  the flower is in the picture? \\nLet’s look at this now.  \\nClassification and Localization  \\nLocal izing  an object in a picture can be expressed as a regression task, as discussed  before : to \\npredict a bounding box around th e object, a common approach is to predict the horizontal and \\nvertical coordinates of the object’s center, as well as its height and width. This means we have \\nfour numbers to predict. It does not require much change to the model; we just need to add a', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 32}),\n",
       " Document(page_content='secon d dense output layer with four units (typically on top of the global average pooling layer), \\nand it can be trained using the MSE loss:  \\nbase_model  = keras.applications .xception .Xception (weights=\"imagenet\" , \\n                                                  include_top =False) \\navg = keras.layers.GlobalAveragePooling2D ()(base_model .output) \\nclass_output  = keras.layers.Dense(n_classes , activation =\"softmax\" )(avg) \\nloc_output  = keras.layers.Dense(4)(avg) \\nmodel = keras.Model(inputs=base_model .input, \\n                    outputs=[class_output , loc_output ]) \\nmodel.compile(loss=[\"sparse_categorical_crossentropy\" , \"mse\"], \\n              loss_weights =[0.8, 0.2], # depends on what you care most about \\n              optimizer =optimizer , metrics=[\"accuracy\" ])', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 33}),\n",
       " Document(page_content='But now we have a pro blem: the flowers dataset does not have bounding boxes around the \\nflowers. So, we need to add them ourselves. This is often one of the hardest and most costly parts \\nof a Machine Learning project: getting the labels. It’s a good idea to spend time looking f or the \\nright tools. To annotate images with bounding boxes, you may want to use an open source image \\nlabeling tool like VGG Image Annotator, LabelImg, OpenLabeler, or ImgLab, or perhaps a \\ncommercial tool like LabelBox or Supervisely. You may also want to c onsider crowdsourcing \\nplatforms such as Amazon Mechanical Turk if you have a very large number of images to \\nannotate. However, it is quite a lot of work to set up a crowdsourcing platform, prepare the form', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 33}),\n",
       " Document(page_content='to be sent to the workers, supervise them, and ens ure that the quality of the bounding boxes they \\nproduce is good, so make sure it is worth the effort. If there are just a few thousand images to \\nlabel, and you don’t plan to do this frequently, it may be preferable to do it yourself. Adriana \\nKovashka et al . wrote a very practical  paper  about crowdsourcing in computer vision. I \\nrecommend you check it out, even if you do not plan to use crowdsourcing.  \\nLet’s suppose you’ve obtained the bounding boxes for every image in the flowers dataset (for \\nnow we will assume there is a single bounding box per image). You then need to create a dataset \\nwhose items will be batches of preprocessed images along with their class labels and their', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 33}),\n",
       " Document(page_content='bounding boxes. Each item should be a tuple of the form  (images,  (class_labels,  \\nbounding_boxes)) . Then you are ready to train your model!  \\nTIP  \\nThe bounding boxes should be normalized so that the horizontal and vertical coordinates, as well as the \\nheight and width, all range from 0 to 1. Also, it is common to predict the square root of the height and \\nwidth rather than the height and width directly: this way, a 10 -pixel err or for a large bounding box will \\nnot be penalized as much as a 10 -pixel error for a small bounding box.  \\nThe MSE often works fairly well as a cost function to train the model, but it is not a great metric \\nto evaluate how well the model can predict bounding boxes. The most common metric for this is', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 33}),\n",
       " Document(page_content='the Intersection  over Union  (IoU): the area of overlap between the predicted bounding box and \\nthe target bounding box, divided by the area of their union (see  Figure  23). In tf.keras, it is \\nimplemented by the  tf.keras.metrics.MeanIoU  class.', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 33}),\n",
       " Document(page_content='Figure  23. Intersection over Union (IoU) metric for bounding boxes  \\nClassifying and localizing a single object is nice, but what if the images contain multiple objects \\n(as is often the case in the flowers dataset)?  \\nObject Detection  \\nThe task of classifying and localizing multiple objects in an image is called  object  detection . \\nUntil a few years ago, a common approach was to take a CNN that was trained to classify and \\nlocate a single object, then slide it across the image, as shown in  Figure  24. In this example, the \\nimage was chopped into a 6 × 8 grid, and we show a CNN (the thick black rectangle) sliding \\nacross all 3 × 3 regions. When the CNN was looking at the top left of the image, it detected part', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 34}),\n",
       " Document(page_content='of the leftmost rose , and then it detected that same rose again when it was first shifted one step to \\nthe right. At the next step, it started detecting part of the topmost rose, and then it detected it \\nagain once it was shifted one more step to the right. You would then conti nue to slide the CNN \\nthrough the whole image, looking at all 3 × 3 regions. Moreover, since objects can have varying \\nsizes, you would also slide the CNN across regions of different sizes. For example, once you are \\ndone with the 3 × 3 regions, you might wan t to slide the CNN across all 4 × 4 regions as well.  \\n \\nFigure  24. Detecting multiple objects by sliding a CNN across the image', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 34}),\n",
       " Document(page_content='This technique is fairly straightforward, but as you can see it will detect the same object multiple \\ntimes, at slightly differ ent positions. Some post -processing will then be needed to get rid of all \\nthe unnecessary bounding boxes. A  common approach for this is called  non-max suppression . \\nHere’s how you do it:  \\n1. First, you  need to add an extra  objectness  output to your CNN, to esti mate the probability \\nthat a flower is indeed present in the image (alternatively, you could add a “no -flower” \\nclass, but this usually does not work as well). It must use the sigmoid activation function, \\nand you can train it using binary cross -entropy loss.  Then get rid of all the bounding \\nboxes for which the objectness score is below some threshold: this will drop all the', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 35}),\n",
       " Document(page_content='bounding boxes that don’t actually contain a flower.  \\n2. Find the bounding box with the highest objectness score, and get rid of all the othe r \\nbounding boxes that overlap a lot with it (e.g., with an IoU greater than 60%). For \\nexamp le, in  Figure  24, the boundin g box with the max objectness score is the thick \\nbounding box over the topmost rose (the objectness score is represented by the thickness \\nof the bounding boxes). The other bounding box over that same rose overlaps a lot with \\nthe max bounding box, so we wil l get rid of it.  \\n3. Repeat step two until there are no more bounding boxes to get rid of.  \\nThis simple approach to object detection works pretty well, but it requires running the CNN', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 35}),\n",
       " Document(page_content='many times, so it is quite slow. Fortunately, there is a much faster way to s lide a CNN across an \\nimage: using a  fully convolutional  network  (FCN).  \\nFully Convolutional Networks  \\nThe idea  of FCNs was first introduced in a  2015  paper  by Jonathan Long et al., for semantic \\nsegmentation (the task of classifying every pixel in an image according to the class of the object \\nit belongs to). The authors pointed out that you c ould replace the dense layers at the top of a \\nCNN by convolutional layers. To understand this, let’s look at an example: suppose a dense layer \\nwith 200 neurons sits on top of a convolutional layer that outputs 100 feature maps, each of size \\n7 × 7 (this is the feature map size, not the kernel size). Each neuron will compute a weighted sum', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 35}),\n",
       " Document(page_content='of all 100 × 7 × 7 activations from the convolutional layer (plus a bias term). Now let’s see what \\nhappens if we replace the dense layer with a convolutional layer using 20 0 filters, each of size 7 \\n× 7, and with  \"valid\"  padding. This layer will output 200 feature maps, each 1 × 1 (since the \\nkernel is exactly the size of the input feature maps and we are using  \"valid\"  padding). In other \\nwords, it will output 200 numbers, just  like the dense layer did; and if you look closely at the \\ncomputations performed by a convolutional layer, you will notice that these numbers will be \\nprecisely the same as those the dense layer produced. The only difference is that the dense \\nlayer’s output  was a tensor of shape [ batch  size, 200], while the convolutional layer will output a', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 35}),\n",
       " Document(page_content='tensor of shape [ batch  size, 1, 1, 200].  \\nTIP  \\nTo convert a dense layer to a convolutional layer, the number of filters in the convolutional layer must be \\nequal to the numb er of units in the dense layer, the filter size must be equal to the size of the input feature \\nmaps, and you must use  \"valid\"  padding. The stride may be set to 1 or more, as we will see shortly.', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 35}),\n",
       " Document(page_content='Why is this important? Well, while a dense layer expects a sp ecific input size (since it has one \\nweight per input feature), a convolutional layer will happily process images of any \\nsize (however, it does expect its inputs to have a specific number of channels, since each kernel \\ncontains a different set of weights for each input channel). Since an FCN contains only \\nconvolutional layers (and pooling layers, which have the same property), it can be trained and \\nexecuted on images of any size!  \\nFor example, suppose we’d already trained a CNN for flower classification and localization. It \\nwas trained on 224 × 224 images, and it outputs 10 numbers: outputs 0 to 4 are sent through \\nthe softmax act ivation function, and this gives the class probabilities (one per class); output 5 is', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 36}),\n",
       " Document(page_content='sent through the logistic activation function, and this gives the objectness score; outputs 6 to 9 do \\nnot use any activation function, and they represent the bounding box ’s center coordinates, as well \\nas its height and width. We can now convert its dense layers to convolutional layers. In fact, we \\ndon’t even need to retrain it; we can just copy the weights from the dense layers to the \\nconvolutional layers! Alternatively, w e could have converted the CNN into an FCN before \\ntraining.  \\nNow suppose the last convolutional layer before the output layer (also called the bottleneck \\nlayer) outputs 7 × 7 feature maps when the network is fed a 224 × 224 image (see the left side \\nof Figure  25). If we feed the FCN a 448 × 448 image (see the right side of  Figure  25), the', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 36}),\n",
       " Document(page_content='bottleneck layer will now output 14 × 14 feature maps.  Since the dense output layer was replaced \\nby a convolutional layer using 10 filters of size 7 × 7, with  \"valid\"  padding and stride 1, the \\noutput will be composed of 10 features maps, each of size 8 × 8 (since 14 – 7 + 1 = 8). In other \\nwords,  the FCN will process the whole image only once, and it will output an 8 × 8 grid where \\neach cell contains 10 numbers (5 class probabilities, 1 objectness score, and 4 bounding box \\ncoordinates). It’s exactly like taking the original CNN and sliding it acro ss the image using 8 \\nsteps per row and 8 steps per column. To visualize this, imagine chopping the original image into \\na 14 × 14 grid, then sliding a 7 × 7 window across this grid; there will be 8 × 8 = 64 possible', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 36}),\n",
       " Document(page_content='locations for the window, hence 8 × 8 pre dictions. However, the FCN approach is  much  more \\nefficient, since the network only looks at the image once. In fact,  You Only  Look  Once  (YOLO) \\nis the name of a very popular object detection architecture, which we’ll look at next.  \\n \\nFigure  14-25. The same f ully convolutional network processing a small image (left) and a large one (right)', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 36}),\n",
       " Document(page_content='You Only Look Once (YOLO)  \\nYOLO  is an extremely fast and accurate object detection architecture proposed by Joseph \\nRedmon et al. in a  2015 paper  and subsequently improved  in 2016  (YOLOv2) and  in \\n2018  (YOLOv3). It is so fast that it can run in real time on a video, as seen in Redmon’s  demo . \\nYOLOv3’s architecture is quite similar to the one we just discussed, but with a few impor tant \\ndifferences:  \\n\\uf0b7 It outputs five bounding boxes for each grid cell (instead of just one), and each bounding \\nbox comes with an objectness score. It also outputs 20 class probabilities per grid cell, as \\nit was trained on the PASCAL VOC dataset, which contai ns 20 classes. That’s a total of \\n45 numbers per grid cell: 5 bounding boxes, each with 4 coordinates, plus 5 objectness', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 37}),\n",
       " Document(page_content='scores, plus 20 class probabilities.  \\n\\uf0b7 Instead of predicting the absolute coordinates of the bounding box centers, YOLOv3 \\npredicts an offs et relative to the coordinates of the grid cell, where (0, 0) means the top \\nleft of that cell and (1, 1) means the bottom right. For each grid cell, YOLOv3 is trained \\nto predict only bounding boxes whose center lies in that cell (but the bounding box itsel f \\ngenerally extends well beyond the grid cell). YOLOv3 applies the logistic activation \\nfunction to the bounding box coordinates to ensure they remain in the 0 to 1 range.  \\n\\uf0b7 Before training the neural net, YOLOv3 finds five representative bounding \\nbox dimensi ons, called  anchor  boxes  (or bounding  box priors ). It does this by ap plying', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 37}),\n",
       " Document(page_content='the K -Means algorithm  to the height and width of the training set bounding boxes. For \\nexample, if the training images contain many pedestrians, then one of the anchor boxes \\nwill likely have the dimensions of a typical pedestrian. Then when the neural net predicts \\nfive bounding  boxes per grid cell, it actually predicts how much to rescale each of the \\nanchor boxes. For example, suppose one anchor box is 100 pixels tall and 50 pixels wide, \\nand the network predicts, say, a vertical rescaling factor of 1.5 and a horizontal rescaling  \\nof 0.9 (for one of the grid cells). This will result in a predicted bounding box of size 150 \\n× 45 pixels. To be more precise, for each grid cell and each anchor box, the network', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 37}),\n",
       " Document(page_content='predicts the log of the vertical and horizontal rescaling factors. Having the se priors makes \\nthe network more likely to predict bounding boxes of the appropriate dimensions, and it \\nalso speeds up training because it will more quickly learn what reasonable bounding \\nboxes look like.  \\n\\uf0b7 The network is trained using images of different sc ales: every few batches during \\ntraining, the network randomly chooses a new image dimension (from 330 × 330 to 608 \\n× 608 pixels). This allows the network to learn to detect objects at different scales. \\nMoreover, it makes it possible to use YOLOv3 at differ ent scales: the smaller scale will \\nbe less accurate but faster than the larger scale, so you can choose the right trade -off for \\nyour use case.', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 37}),\n",
       " Document(page_content='your use case.  \\nThere are a few more innovations you might be interested in, such as the use of skip connections \\nto recover some of the spatial resolution that is lost in the CNN (we will discuss this shortly, \\nwhen we look at semantic segmentation). In the 2016 paper, the authors introduce the \\nYOLO9000 model that uses hierarchical classification: the  model predicts a probability for  each \\nnode in a visual hierarchy called  WordTree . This makes it possible for the network to predict', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 37}),\n",
       " Document(page_content='with high confidence that an image represents, say, a dog, even though it is unsure what specific \\ntype of dog. I encourage you to go ahead and read all thre e papers: they are quite pleasant to \\nread, and they provide excellent examples of how Deep Learning systems can be incrementally \\nimproved.  \\nMEAN AVERAGE PRECISI ON (MAP)  \\nA very common metric used in object detection tasks is the  mean  Average  Precision  (mAP).  \\n“Mean Average” sounds a bit redundant, doesn’t it? To understand this metric, let’s go back to \\ntwo classification metrics : precision and recall. Remember the trade -off: the higher the recall, the \\nlower the precision. You can visualize th is in a precision/recall curve . To summarize this curve', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 38}),\n",
       " Document(page_content='into a single number, we could compute its area under the curve (AUC). But note that the \\nprecision/recall curve may contain a few sections where precision ac tually goes up when recall \\nincreases, e specially at low recall values . This is one of the motivations for the mAP metric.  \\nSuppose the classifier has 90% precision at 10% recall, but 96% precision at 20% recall. There’s \\nreally no trade -off here: it simply makes more sense to use the classifier at 20% recall rather than \\nat 10 % recall, as you will get both higher recall and higher precision. So instead of looking at the \\nprecision  at 10% recall, we should really be looking at the  maximum  precision that the classifier \\ncan offer with  at least  10% recall. It would be 96%, not 90%. Therefore, one way to get a fair', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 38}),\n",
       " Document(page_content='idea of the model’s performance is to compute the maximum precision you can get with at least \\n0% recall, then 10% recall, 20%, and so on up to 100%, and then calculate the mean of these \\nmaximum precisions. This  is called th e Average  Precision  (AP) metric. Now when there are \\nmore than two classes, we can compute the AP for each class, and then compute the mean AP \\n(mAP). That’s it!  \\nIn an object detection system, there is an additional level of complexity: what if the system \\ndetected the correct class, but at the wrong location (i.e., the bounding box is completely off)? \\nSurely we should not count this as a positive prediction. One approach is to define an IOU \\nthreshold: for example, we may consider that a prediction is correct only if the IOU is greater', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 38}),\n",
       " Document(page_content='than, say, 0.5, and the predicted class is correct. The corresponding mAP is generally noted \\nmAP@0.5 (or mAP@50%, or sometimes just AP 50). In some competitions (such as the PASCAL \\nVOC challenge), this is what is done. In others ( such as the COCO competition), the mAP is \\ncomputed for different IOU thresholds (0.50, 0.55, 0.60, …, 0.95), and the final metric is the \\nmean of all these mAPs (noted mAP@[.50:.95] or mAP@[.50:0.05:.95]). Yes, that’s a mean \\nmean average.  \\nSeveral YOLO imple mentations built using TensorFlow are available on GitHub. In particular, \\ncheck out  Zihao  Zang’s  TensorFlow  2 implementation . Other object detection models are \\navailable in the TensorFlow Models project, many with pretrained weights; and some have even', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 38}),\n",
       " Document(page_content='been ported to TF Hub, such as  SSD and Faster -RCNN , which are both quite popular. SSD is \\nalso a “single shot” detection mode l, similar to YOLO. Faster R -CNN is more complex: the \\nimage first goes through a CNN, then the output  is passed to a  Region  Proposal  Network  (RPN) \\nthat proposes bounding boxes that are most likely to contain an object, and a classifier is run for \\neach boun ding box, based on the cropped output of the CNN.', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 38}),\n",
       " Document(page_content='The choice of detection system depends on many factors: speed, accuracy, available pretrained \\nmodels, training time, complexity, etc. The papers contain tables of metrics, but there is quite a \\nlot of variab ility in the testing environments, and the technologies evolve so fast that it is \\ndifficult to make a fair comparison that will be useful for most people and remain valid for more \\nthan a few months.  \\nSo, we can locate objects by drawing bounding boxes aroun d them. Great! But perhaps you want \\nto be a bit more precise. Let’s see how to go down to the pixel level.  \\nSemantic Segmentation  \\nIn semantic  segmentation , each  pixel is classified according to the class of the object it belongs', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 39}),\n",
       " Document(page_content='to (e.g., road, car, pedestr ian, building, etc.), as shown in  Figure  26. Note that different objects of \\nthe same class are  not distinguish ed. For example, all the bicycles on the right side of the \\nsegmented image end up as one big lump of pixels. The main difficulty in this task is that when \\nimages go through a regular CNN, they gradually lose their spatial resolution (due to the layers \\nwith strides greater than 1); so, a regular CNN may end up knowing that there’s a person \\nsomewhere in the bottom left of the image, but it will not be much more precise than that.  \\nJust like for object detection, there are many different approaches to tackle th is problem, some \\nquite complex. However, a fairly simple solution was proposed in the 2015 paper by Jonathan', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 39}),\n",
       " Document(page_content='Long et al. we discussed earlier. The authors start by taking a pretrained CNN and turning it into \\nan FCN. The CNN applies an overall stride of 32 to the input image (i.e., if you add up all the \\nstrides greater than 1), meaning the last layer outputs feature maps that are 32 times smaller than \\nthe input image. This is clearly too coarse, so they add a single  upsampling  layer  that multiplies \\nthe resol ution by 32.  \\n \\nFigure  26. Semantic segmentation  \\nThere are  several solutions available for upsampling (increasing the size of an image), such as \\nbilinear interpolation, but that only works reasonably well up to ×4 or ×8. Instead, they  use \\na transposed  convolutional  layer : it is equivalent to first stretching the image by inserting empty', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 39}),\n",
       " Document(page_content='rows and columns (full of zeros), then  performing a regular convolution ( see Figure  27).', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 39}),\n",
       " Document(page_content='Alternatively, some people prefer to think of it as a regular co nvolutional layer that uses \\nfractional strides (e.g., 1/2 in  Figure  27). The transposed convolutional layer can be initialized to \\nperform something close to linear interpolation, but since it is a trainable layer, it will learn to do \\nbetter during training. In tf.keras, you can use the  Conv2DTranspose  layer.  \\n \\nFigure  27. Upsampling using a transposed convolutional l ayer \\nNOTE  \\nIn a transposed convolutional layer, the stride defines how much the input will be stretched, not the size \\nof the filter steps, so the larger the stride, the larger the output (unlike for convolutional layers or pooling \\nlayers).  \\nTENSORFLOW CONVOL UTION OPERATIONS  \\nTensorFlow  also offers a few other kinds of convolutional layers:', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 40}),\n",
       " Document(page_content='keras.layers.Conv1D  \\nCreates a convolutional layer for 1D inputs, such as time series or text (sequences of \\nletters or words), as we will see in next section . \\nkeras.layers.Conv3D  \\nCreates a convolutional layer for 3D inputs, such as 3D PET scans.  \\ndilation_rate  \\nSetting the  dilation_rate  hyperparameter of any  convolutional layer to a value of 2 \\nor more creates an  à-trous  convolutional  layer  (“à trous” is French for “with holes”). This', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 40}),\n",
       " Document(page_content='is equivalent to using a regular convolutional layer with a filter dilated by inserting rows \\nand columns of zeros (i.e., holes) . For example, a 1 × 3 filter equal to  [[1,2,3]]  may \\nbe dilated with a  dilation  rate of 4, resulting in a  dilated  filter  of [[1, 0, 0, 0, 2, \\n0, 0, 0, 3]]. This lets the convolutional layer have a larger receptive field at no \\ncomputational price and using n o extra parameters.  \\ntf.nn.depthwise_conv2d()  \\nCan be used to create a  depthwise  convolutional  layer  (but you need to create the \\nvariables yourself). It applies every filter to every individual input channel independently. \\nThus, if there are  fn filters and  fn′ input channels, then this will output  fn × fn′ feature \\nmaps.', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 41}),\n",
       " Document(page_content='maps.  \\nThis solution is OK, but still too imprecise. To do better, the authors added skip connections \\nfrom lower layers: for example, they upsampled the output image by a factor of 2 (instead of 32),  \\nand they added the output of a lower layer that had this double resolution. Then they upsampled \\nthe result by a factor of 16, leading to a total upsampling factor of 3 2 (see  Figure  28). This \\nrecovered some of the spatial resolution that was lost in earlier pooling layers. In their best \\narchitecture, they used a second similar skip connection to recover even finer de tails from an \\neven lower layer. In short, the output of the original CNN goes through the following extra steps: \\nupscale ×2, add the output of a lower layer (of the appropriate scale), upscale ×2, add the output', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 41}),\n",
       " Document(page_content='of an even lower layer, and finally upscale ×8. It is even possible to scale up beyond the size of \\nthe original image: this can be used to increase the resolution of an image, which is a technique \\ncalled  super -resolution . \\n \\nFigure  28. Skip layers recover some spatial resolution from lower layers  \\nOnce again, many GitHub repositories provide TensorFlow implementations of semantic \\nsegmentation  (TensorFlow  1 for now), and you will even find pretrained  instance  \\nsegmentation  models in the TensorFlow Models project. Instance segmentation is similar to \\nsemantic segmentation, but instead of merging all objects of the same class into one big lump, \\neach object is distinguished from the others (e.g., it identifies each individual bicycle). At', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 41}),\n",
       " Document(page_content='present,  the instance segmentation models available in the TensorFlo w Models project are based \\non the  Mask  R-CNN  architecture, which was proposed in a  2017  paper : it extends the Faster R -\\nCNN model by additionally producing a pixel mask for each bounding box. So not only do you', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 41}),\n",
       " Document(page_content='get a bounding box around each object, with a set of estimated class probabilities, but you also \\nget a pixel mask that locates  pixels in the bounding box that belong to the object.  \\nAs you can see, the field of Deep Computer Vision is vast and moving fast, with all sorts of \\narchitectures popping out every year, all based on convolutional neural networks. The  progress \\nmade in just a few years has been astounding, and researchers are now focusing on harder and \\nharder problems, such as  adversarial  learning  (which attempts to make the network more \\nresistant to images designed to fool it), explainability (understanding why the network m akes a \\nspecific classification), realistic  image  generation  (which we will come back in later section ),', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 42}),\n",
       " Document(page_content='and single -shot learning  (a system that can recognize an object after it has seen it just once). \\nSome even explore completely novel architectures, such as Geoffrey Hinton’s  capsule  \\nnetworks  (I presented them in a couple of  videos , with the corresponding code in a notebook). \\nNow on to the next section , wher e we will look at how to process sequential data such as time \\nseries using recurrent neural networks and convolutional neural networks.', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 42}),\n",
       " Document(page_content='Processing Sequences Using  RNNs  and CNNs  \\nThe batter hits the ball. The outfielder immediately starts running, anticipating the ball’s \\ntrajectory. He tracks it, adapts his movements, and finally catches it (under a thunder of \\napplause). Predicting the futur e is something you do all the time, whether you are finishing a \\nfriend’s sentence or anticipating the smell of coffee at breakfast. In this section  we will discuss \\nrecurrent neural networks (RNNs), a class of nets that can predict the future (well, up to a  point, \\nof course). They  can analyze time series data such as stock prices, and tell you when to buy or \\nsell. In  autonomous driving systems, they can anticipate car trajectories and help avoid accidents.', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 0}),\n",
       " Document(page_content='More generally, they  can work on sequences of arbitr ary lengths, rather than on fixed -sized \\ninputs like all the nets we have considered so far. For example, they can take sentences, \\ndocuments, or audio samples as input, making  them extremely useful for natural language \\nprocessing applications such as automa tic translation or speech -to-text. \\nIn this section  we will first look at the fundamental concepts underlying RNNs and how to train \\nthem using backpropagation through time, then we will use them to forecast a time series. After \\nthat we’ll explore the two ma in difficulties that RNNs face:  \\n\\uf0b7 Unstable gradients (di scussed before ), which can be alleviated using various techniques, \\nincluding recurrent dropout and recurrent layer normalization', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 0}),\n",
       " Document(page_content='\\uf0b7 A (very) limited short -term memory, which can be extended using LSTM and GRU cells  \\nRNNs are not the only types of neural networks capable of handling sequential data: for small \\nsequences, a regular dense network can do the trick; and for very long sequences, such as audio \\nsamples or text, convolutional neural networks can actua lly work quite well too. We will discuss \\nboth of these possibilities, and we will finish this section  by implementing a  WaveNet : this is a \\nCNN architecture capable of handling sequences of tens  of thousands of time steps. In next \\nsection , we will continue to explore RNNs and see how to use them for natural language \\nprocessing, along with more recent architectures based on attention mechanisms. Let’s get \\nstarted!  \\nRecurrent Neurons and Layers', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 0}),\n",
       " Document(page_content='started!  \\nRecurrent Neurons and Layers  \\nUp to now we have focused on feedforward neural networks, where the  activations flow only in \\none direction, from the i nput layer to the output layer . A recurrent neural network looks very \\nmuch like a feedforward neural network, except it also has connections pointing backward. Let’s \\nlook at the simplest possible RNN, comp osed of one neuron receiving inputs, producing an \\noutput, and sending that output back to itself, as shown in  Figure  1 (left). At each time \\nstep t (also called a  frame ), this  recurrent  neuron  receives the inputs  x(t) as well as its own output \\nfrom the previous time step,  y(t–1). Since there is no previous output at the first time step, it is', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 0}),\n",
       " Document(page_content='generally set to 0. We can represent  this tiny network against the time axis, as shown \\nin Figure  1 (right). This  is called  unrolling  the network  through  time (it’s the same recurrent \\nneuron represented once per time step).', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 0}),\n",
       " Document(page_content='Figure  1. A recurrent neuron (left) unrolled through time (right)  \\nYou can easily create a layer of recurrent neurons. At each time step  t, every neuron receives \\nboth the input vector  x(t) and the output vector from the previous time step  y(t–1), as shown \\nin Figure  2. Note that both the inputs and outputs are vector s now (when there was just a single \\nneuron, the output was a scalar).  \\n \\nFigu re 2. A layer of recurrent neurons (left) unrolled through time (right)  \\nEach recurrent neuron has two sets of weights: one for the inputs  x(t) and the other for the outputs \\nof the previous time step,  y(t–1). Let’s call these weight vectors  wx and wy. If we consider the \\nwhole recurrent layer instead of just one recurrent neuron, we can place all the weight vectors in', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 1}),\n",
       " Document(page_content='two weight matrices,  Wx and Wy. The output vector of the whole recu rrent layer can then be \\ncomputed pretty much as you might expect, as shown in  Equation  1 (b is the bias vector and  ϕ(·) \\nis the activation function (e.g., ReLU).', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 1}),\n",
       " Document(page_content='Equation  1. Output of a recurrent layer for a single instance  \\n \\nJust as with feedforward neural networks, we can compute a recurrent layer’s output in one shot \\nfor a whole mini -batch by placing all the inputs at time step  t in an input \\nmatrix  X(t) (see Equation  2). \\nEquation  2. Outputs of a la yer of recurrent neurons for all instances in a mini -batch  \\n \\nIn this equation:  \\n\\uf0b7 Y(t) is an m × nneurons matrix containing the layer’s outputs at time step  t for each instance \\nin the mini -batch ( m is the number of instances in the mini -batch and  nneurons is the number \\nof neurons).  \\n\\uf0b7 X(t) is an m × ninputs matrix containing the inputs for all instances ( ninputs is the number of \\ninput features).', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 2}),\n",
       " Document(page_content='input features).  \\n\\uf0b7 Wx is an ninputs × nneurons matrix containing the connection weights for the inputs of the \\ncurrent time step.  \\n\\uf0b7 Wy is an nneurons × nneurons matrix containing the connection weights for the outputs of the \\nprevious time step.  \\n\\uf0b7 b is a vector of size  nneurons containing each neuron’s bias term.  \\n\\uf0b7 The weight matrices  Wx and Wy are often concatenated vertically into a single weight \\nmatrix  W of shape ( ninputs + nneurons) × nneurons (see the second line of  Equation  2). \\n\\uf0b7 The notation [ X(t) Y(t–1)] represents  the horizontal concatenation of the \\nmatrices  X(t) and Y(t–1). \\nNotice that  Y(t) is a function of  X(t) and Y(t–1), which is a function of  X(t–1) and Y(t–2), which is a', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 2}),\n",
       " Document(page_content='function of  X(t–2) and Y(t–3), and so on. This makes  Y(t) a function of all the inputs si nce time  t = 0 \\n(that is,  X(0), X(1), …, X(t)). At the first time step,  t = 0, there are no previous outputs, so they are \\ntypically assumed to be all zeros.  \\nMemory Cells  \\nSince  the output of a recurrent neuron at time step  t is a function of all the inputs f rom previous \\ntime steps, you could say it has a form of  memory . A part of a neural network that preserves \\nsome state across time steps is called a  memory  cell (or simply a  cell). A single recurrent neuron, \\nor a layer of recurrent neurons, is a very basic c ell, capable of learning only short patterns \\n(typically about 10 steps long, but this varies depending on the task). Later in this section , we', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 2}),\n",
       " Document(page_content='will look at some more complex and powerful types of cells capable of learning longer patterns \\n(roughly 10 times longer, but again, this depends on the task).', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 2}),\n",
       " Document(page_content='In general a cell’s state at time step  t, denoted  h(t) (the “h” stands for “hidden”), is a function of \\nsome inputs at that time step and its state at the previous time step:  h(t) = f(h(t–1), x(t)). Its output a t \\ntime step  t, denoted  y(t), is also a function of the previous state and the current inputs. In the case \\nof the basic cells we have discussed so far, the output is simply equal to the state, but in more \\ncomplex cells this is not always the case, as shown in Figure  3. \\n \\nFigure  3. A cell’s hidden state and its output may be different  \\nInput and Output Sequences  \\nAn RNN can simul taneously take a sequence of inputs and produce a sequence of outputs (see \\nthe top -left network in  Figure  4). This type of  sequence -to-sequence  network  is useful for', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 3}),\n",
       " Document(page_content='predicting time series such as stock prices: you feed it the prices over the last  N days, and it must \\noutput the prices shifted by one day into the future (i.e., from  N – 1 days ago to tomorrow).  \\nAlternatively, you  could feed the network a sequence of inputs and ignore all outputs except for \\nthe last one (see the top -right network in  Figure 4). In other words, this  is a sequence -to-vector  \\nnetwork . For example, you could feed the network a sequence of words corresponding to a \\nmovie review, and the network would output a sentiment score (e.g., from –1 [hate] to +1 \\n[love]).  \\nConversely, you could feed the network the same input vector over and over again at each time \\nstep and let it output a sequence (see the bottom -left network of  Figure  4). This  is a vector -to-', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 3}),\n",
       " Document(page_content='sequence  network . For example, the input could be an image (or the output of a CNN), and the \\noutput could be a caption for that image.  \\nLastly, you could have a sequence -to-vector network, called  an encoder , followed by a vector -to-\\nsequence network, called a  decoder  (see the bottom -right network of  Figure  4). For example, this \\ncould be used for translating a sentence from one language to another. You would feed the \\nnetwork a sentence in one language, the encoder would convert this sentence into a single vector \\nrepresentation, and then the decoder would decode this vector into a s entence in another', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 3}),\n",
       " Document(page_content='language. This two -step model, called  an Encoder–Decoder , works much better than trying to \\ntranslate on the fly with a single sequence -to-sequence RNN (like the one represented at the top \\nleft): the last words of a sentence can affect th e first words of the translation, so you need to wait \\nuntil you have seen the whole sentence before translating it. We will see how to implement an \\nEncoder –Decoder in next section  (as we will see, it is a bit more complex than \\nin Figure  4 suggests).  \\n \\nFigure  4. Seq-to-seq (top left), seq -to-vector (top right), vector -to-seq (bottom left), and Encoder –Decoder (bottom right) n etworks  \\nSounds promising, but how do you train a recurrent neural network?  \\nTraining RNNs', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 4}),\n",
       " Document(page_content='Training RNNs  \\nTo train an RNN, the trick is to unroll it through time (like we just did) and then simply use \\nregular backpropagation ( see Figure  5). This  strategy is called  backpropagation  through  \\ntime (BPTT).  \\nJust like in regular backpropagation, there is a first forward pass through the unrolled network \\n(represented by the dashed arrows). Then the output sequence is evaluated using a cost \\nfunction  C(Y(0), Y(1), …Y(T)) (where  T is the max time step). Note that this cost function may \\nignore some outputs, as shown in  Figure  5 (for example, in a sequence -to-vector RNN, all \\noutputs are ignored except for the very last one). The gradients of that cost function are then \\npropagated backwa rd through the unrolled network (represented by the solid arrows). Finally the', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 4}),\n",
       " Document(page_content='model parameters are updated using the gradients computed during BPTT. Note that the \\ngradients flow backward through all the outputs used by the cost function, not just through the', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 4}),\n",
       " Document(page_content='final output (for example, in  Figure  5 the cost function is computed using the last three outputs of \\nthe network,  Y(2), Y(3), and  Y(4), so gradients flow through these three outputs, but not \\nthrough  Y(0) and Y(1)). Moreover, since the same parameters  W and b are used at each time step, \\nbackpropagation will do the right thing and sum over all time steps.  \\n \\nFigure  5. Backprop agation through time  \\nFortunately, tf.keras takes care of all of this complexity for you —so let’s start coding!  \\nForecasting a Time Series  \\nSuppose  you are studying the number of active users per hour on your website, or the daily \\ntemperature in your city, or  your company’s financial health, measured quarterly using multiple', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 5}),\n",
       " Document(page_content='metrics. In all these cases, the data will be a sequence of one or more values per time step. This \\nis called a  time series . In the first two examples there is a single value per time step,  so these \\nare univariate  time series , while in the financial example there are multiple values per time step \\n(e.g., the company’s revenue, debt, and so on), so  it is a  multivariate  time series . A typical task is \\nto predict future values, which  is called  forecasting . Another common task is to fill in the blanks: \\nto predict (or rather “postdict”) missing values from the past. This  is called  imputation . For \\nexam ple, Figure  6 shows 3 univariate time series, each of them 50 time steps long, and the goal', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 5}),\n",
       " Document(page_content='here is to forecast the value at the next time step (represented by the X) for each of them.', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 5}),\n",
       " Document(page_content='Figure  6. Time series forecasting  \\nFor simplicity, we are using a time series generated by the  generate_time_series()  function, \\nshown here:  \\ndef generate_time_series (batch_size , n_steps): \\n    freq1, freq2, offsets1 , offsets2 = np.random.rand(4, batch_size , 1) \\n    time = np.linspace (0, 1, n_steps) \\n    series = 0.5 * np.sin((time - offsets1 ) * (freq1 * 10 + 10))  #   wave 1 \\n    series += 0.2 * np.sin((time - offsets2 ) * (freq2 * 20 + 20)) # + wave 2 \\n    series += 0.1 * (np.random.rand(batch_size , n_steps) - 0.5)   # + noise \\n    return series[..., np.newaxis].astype(np.float32) \\nThis function creates as many time series as requested (via the  batch_size  argument), each of \\nlength  n_steps , and there is just one value per time step in each series (i.e., all series are', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 6}),\n",
       " Document(page_content='univariate). The function return s a NumPy array of shape [ batch  size, time steps , 1], where each \\nseries is the sum of two sine waves of fixed amplitudes but random frequencies and phases, plus \\na bit of noise.  \\nNOTE  \\nWhen dealing with time series (and other types of sequences such as senten ces), the input features are \\ngenerally represented as 3D arrays of shape [ batch  size, time steps , dimensionality ], \\nwhere  dimensionality  is 1 for univariate time series and more for multivariate time series.  \\nNow let’s create a training set, a validation set , and a test set using this function:  \\nn_steps = 50 \\nseries = generate_time_series (10000, n_steps + 1) \\nX_train, y_train = series[:7000, :n_steps], series[:7000, -1]', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 6}),\n",
       " Document(page_content='X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1] \\nX_test, y_test = series[9000:, :n_steps], series[9000:, -1] \\nX_train  contains 7,000 time series (i.e., its shape is [7000, 50, 1]), while  X_valid  contains 2,000 \\n(from the 7,000th time series to the 8,999th) and  X_test  contains 1,000 (from the 9,000th to the \\n9,999th). Since we wan t to forecast a single value for each series, the targets are column vectors \\n(e.g.,  y_train  has a shape of [7000, 1]).', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 6}),\n",
       " Document(page_content='Baseline Metrics  \\nBefore  we start using RNNs, it is often a good idea to have a few baseline metrics, or else we \\nmay end up thinking our m odel works great when in fact it is doing worse than basic models. For \\nexample, the simplest approach is to predict the last value in each series. This is called  naive  \\nforecasting , and it is sometimes surprisingly difficult to outperform. In this case, it  gives us a \\nmean squared error of about 0.020:  \\n>>> y_pred = X_valid[:, -1] \\n>>> np.mean(keras.losses.mean_squared_error (y_valid, y_pred)) \\n0.020211367  \\nAnother simple approach is to use a fully connected network. Since it expects a flat list of \\nfeatures for ea ch input, we need to add a  Flatten  layer. Let’s just use a simple Linear Regression', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 7}),\n",
       " Document(page_content='model so that each prediction will be a linear combination of the values in the time series:  \\nmodel = keras.models.Sequential ([ \\n    keras.layers.Flatten(input_shape =[50, 1]), \\n    keras.layers.Dense(1) \\n]) \\nIf we compile this model using the MSE loss and the default Adam optimizer, then fit it on the \\ntraining set for 20 epochs and evaluate it on the validation set, we get an MSE of about 0.004. \\nThat’s much better than the naive approach!  \\nImplementing a Simple RNN  \\nLet’s  see if we can beat that with a simple RNN:  \\nmodel = keras.models.Sequential ([ \\n  keras.layers.SimpleRNN (1, input_shape =[None, 1]) \\n]) \\nThat’s really the simplest RNN you can build. It just contains a single layer, with  a single', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 7}),\n",
       " Document(page_content='neuron, as we saw in  Figure  1. We do not need to specify the length of the input sequences \\n(unlike in the previ ous model), since a recurrent neural network can process any number of time \\nsteps (this is why we set the first input dimension to  None). By default, the  SimpleRNN  layer uses \\nthe hyperbolic tangent activation function. It works exactly as we saw earlier: t he initial \\nstate h(init) is set to 0, and it is passed to a single recurrent neuron, along with the value of the first \\ntime step,  x(0). The neuron computes a weighted sum of these values and applies the hyperbolic \\ntangent activation function to the result,  and this gives the first output,  y0. In a simple RNN, this \\noutput is also the new state  h0. This new state is passed to the same recurrent neuron along with', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 7}),\n",
       " Document(page_content='the next input value,  x(1), and the process is repeated until the last time step. Then the layer j ust \\noutputs the last value,  y49. All of this is performed simultaneously for every time series.  \\nNOTE', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 7}),\n",
       " Document(page_content='By default, recurrent layers in Keras only return the final output. To make them return one output per time \\nstep, you must set  return_sequences=True , as we  will see.  \\nIf you compile, fit, and evaluate this model (just like earlier, we train for 20 epochs using Adam), \\nyou will find that its MSE reaches only 0.014, so it is better than the naive approach but it does \\nnot beat a simple linear model. Note that for  each neuron, a linear model has one parameter per \\ninput and per time step, plus a bias term (in the simple linear model we used, that’s a total of 51 \\nparameters). In contrast, for each recurrent neuron in a simple RNN, there is just one parameter \\nper inpu t and per hidden state dimension (in a simple RNN, that’s just the number of recurrent', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 8}),\n",
       " Document(page_content='neurons in the layer), plus a bias term. In this simple RNN, that’s a total of just three parameters.  \\nTREND AND SEASONALIT Y \\nThere  are many other models to forecast time series, such as  weighted  moving  \\naverage  models  or autoregressive  integrated  moving  average  (ARIMA) models. Some of them \\nrequire you to first remove the trend and seasonality. For example, if you are studying the \\nnumber of active users on your website, and it is growing by 10% every month, you would have \\nto remove this trend from the time series. Once the model is trained and starts making \\npredictions, you would have to add the trend back to get the final predictions. Similarly, if you \\nare trying to predict the amount of sunscreen lotion sold every month, you will probably observe', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 8}),\n",
       " Document(page_content='strong seasonality: since it sells well every summer, a similar pattern will be repeated every year. \\nYou would have to remove this seasonality from the time series, for example by c omputing the \\ndifference between the value at each time step and the value one year earlier (this technique  is \\ncalled  differencing ). Again, after the model is trained and makes predictions, you would have to \\nadd the seasonal pattern back to get the final pr edictions.  \\nWhen using RNNs, it is generally not necessary to do all this, but it may improve performance in \\nsome cases, since the model will not have to learn the trend or the  seasonality . \\nApparently our simple RNN was too simple to get good performance. S o let’s try to add more \\nrecurrent layers!  \\nDeep RNNs', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 8}),\n",
       " Document(page_content='recurrent layers!  \\nDeep RNNs  \\nIt is quite common to stack multiple layers of cells, as shown in Figure  7. This  gives you a  deep  \\nRNN . \\n \\nFigure  7. Deep RNN (left) unrolled through time (right)', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 8}),\n",
       " Document(page_content='Implementing a deep RNN with tf.keras is quite simple: just stack recurrent layers. In this \\nexample, we use three  SimpleRNN  layers (but we could add any other type  of recurrent layer, such \\nas an  LSTM layer or a  GRU layer, which we will discuss shortly):  \\nmodel = keras.models.Sequential ([ \\n    keras.layers.SimpleRNN (20, return_sequences =True, input_shape =[None, 1]), \\n    keras.layers.SimpleRNN (20, return_sequences =True), \\n    keras.layers.SimpleRNN (1) \\n]) \\nWARNING  \\nMake sure to set  return_sequences=True  for all recurrent layers (except the last one, if you only care \\nabout the last output). If you don’t, they will output a 2D array (containing only the output of the last time', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 9}),\n",
       " Document(page_content='step) instead of a 3D array (containing outputs for all time steps), and the next recurrent layer will \\ncomplain that you are not feeding it sequences in the expected 3D format.  \\nIf you compile, fit, and evaluate this model, you will find that it reaches an  MSE of 0.003. We \\nfinally managed to beat the linear model!  \\nNote that the last layer is not ideal: it must have a single unit because we want to forecast a \\nunivariate time series, and this means we must have a single output value per time step. \\nHowever, ha ving a single unit means that the hidden state is just a single number. That’s really \\nnot much, and it’s probably not that useful; presumably, the RNN will mostly use the hidden', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 9}),\n",
       " Document(page_content='states of the other recurrent layers to carry over all the information it need s from time step to \\ntime step, and it will not use the final layer’s hidden state very much. Moreover, since \\na SimpleRNN  layer uses the tanh activation function by default, the predicted values must lie \\nwithin the range –1 to 1. But what if you want to use  another activation function? For both these \\nreasons, it might be preferable to replace the output layer with a  Dense  layer: it would run \\nslightly faster, the accuracy would be roughly the same, and it would allow us to choose any \\noutput activation functio n we want. If you make this change, also make sure to \\nremove  return_sequences=True  from the second (now last) recurrent layer:  \\nmodel = keras.models.Sequential ([', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 9}),\n",
       " Document(page_content='model = keras.models.Sequential ([ \\n    keras.layers.SimpleRNN (20, return_sequences =True, input_shape =[None, 1]), \\n    keras.layers.SimpleRNN (20), \\n    keras.layers.Dense(1) \\n]) \\nIf you train this model, you will see that it converges faster and performs just as well. Plus, you \\ncould change the output activation function if you wanted.  \\nForecasting Several Time Steps Ahead  \\nSo far we have o nly predicted the value at the next time step, but we could just as easily have \\npredicted the value several steps ahead by changing the targets appropriately (e.g., to predict 10 \\nsteps ahead, just change the targets to be the value 10 steps ahead instead o f 1 step ahead). But \\nwhat if we want to predict the next 10 values?', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 9}),\n",
       " Document(page_content='The first option is to use the model we already trained, make it predict the next value, then add \\nthat value to the inputs (acting as if this predicted value had actually occurred), and us e the \\nmodel again to predict the following value, and so on, as in the following code:  \\nseries = generate_time_series (1, n_steps + 10) \\nX_new, Y_new = series[:, :n_steps], series[:, n_steps:] \\nX = X_new \\nfor step_ahead  in range(10): \\n    y_pred_one  = model.predict(X[:, step_ahead :])[:, np.newaxis, :] \\n    X = np.concatenate ([X, y_pred_one ], axis=1) \\n \\nY_pred = X[:, n_steps:] \\nAs you might expect, the prediction for the next step will usually be more accurate than the \\npredictions for later time steps, since the error s might accumulate (as you can see in  Figure  8). If', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 10}),\n",
       " Document(page_content='you evaluate this approach on the validation set, you will find an M SE of about 0.029. This is \\nmuch higher than the previous models, but it’s also a much harder task, so the comparison \\ndoesn’t mean much. It’s much more meaningful to compare this performance with naive \\npredictions (just forecasting that the time series will  remain constant for 10 time steps) or with a \\nsimple linear model. The naive approach is terrible (it gives an MSE of about 0.223), but the \\nlinear model gives an MSE of about 0.0188: it’s much better than using our RNN to forecast the \\nfuture one step at a time, and also much faster to train and run. Still, if you only want to forecast \\na few time steps ahead, on more complex tasks, this approach may work well.', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 10}),\n",
       " Document(page_content='Figure  8. Forecasting 10 steps ahead, 1 step at a time  \\nThe second option is to train an RNN to  predict all 10 next values at once. We can still use a \\nsequence -to-vector model, but it will output 10 values instead of 1. However, we first need to \\nchange the targets to be vectors containing the next 10 values:  \\nseries = generate_time_series (10000, n_steps + 10) \\nX_train, Y_train = series[:7000, :n_steps], series[:7000, -10:, 0]', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 10}),\n",
       " Document(page_content='X_valid, Y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0] \\nX_test, Y_test = series[9000:, :n_steps], series[9000:, -10:, 0] \\nNow we just need the output layer to ha ve 10 units instead of 1:  \\nmodel = keras.models.Sequential ([ \\n    keras.layers.SimpleRNN (20, return_sequences =True, input_shape =[None, 1]), \\n    keras.layers.SimpleRNN (20), \\n    keras.layers.Dense(10) \\n]) \\nAfter training this model, you can predict the next 10 v alues at once very easily:  \\nY_pred = model.predict(X_new) \\nThis model works nicely: the MSE for the next 10 time steps is about 0.008. That’s much better \\nthan the linear model. But we can still do better: indeed, instead of training the model to forecast', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 11}),\n",
       " Document(page_content='the next 10 values only at the very last time step, we can train it to forecast the next 10 values at \\neach and every time step. In other words, we can turn this sequence -to-vector RNN into a \\nsequence -to-sequence RNN. The advantage of this technique is that th e loss will contain a term \\nfor the output of the RNN at each and every time step, not just the output at the last time step. \\nThis means there will be many more error gradients flowing through the model, and they won’t \\nhave to flow only through time; they w ill also flow from the output of each time step. This will \\nboth stabilize and speed up training.  \\nTo be clear, at time step 0 the model will output a vector containing the forecasts for time steps 1', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 11}),\n",
       " Document(page_content='to 10, then at time step 1 the model will forecast time st eps 2 to 11, and so on. So each target \\nmust be a sequence of the same length as the input sequence, containing a 10 -dimensional vector \\nat each step. Let’s prepare these target sequences:  \\nY = np.empty((10000, n_steps, 10)) # each target is a sequence  of 10D vectors \\nfor step_ahead  in range(1, 10 + 1): \\n    Y[:, :, step_ahead  - 1] = series[:, step_ahead :step_ahead  + n_steps, 0] \\nY_train = Y[:7000] \\nY_valid = Y[7000:9000] \\nY_test = Y[9000:] \\nNOTE  \\nIt may be surprising that the targets will contain values that appear in the inputs (there is a lot of overlap \\nbetween  X_train  and Y_train ). Isn’t that cheating? Fortunately, not at all: at each time step, the model', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 11}),\n",
       " Document(page_content='only knows about past time steps, so it cannot look ahead. It  is said to be a  causal  model.  \\nTo turn the model into a sequence -to-sequence model, we must set  return_sequences=True  in all \\nrecurrent layers (even the last one), and we must apply the output  Dense  layer at every time step. \\nKeras offers a  TimeDistributed  layer for this very purpose: it wraps any layer (e .g., \\na Dense  layer) and applies it at every time step of its input sequence. It does this efficiently, by \\nreshaping the inputs so that each time step is treated as a separate instance (i.e., it reshapes the \\ninputs from [ batch  size, time steps , input  dimens ions] to [ batch  size × time steps , input', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 11}),\n",
       " Document(page_content='dimensions ]; in this example, the number of input dimensions is 20 because the \\nprevious  SimpleRNN  layer has 20 units), then it runs the  Dense  layer, and finally it reshapes the \\noutputs back to sequences (i.e., it re shapes the outputs from [ batch  size × time steps , output  \\ndimensions ] to [ batch  size, time steps , output  dimensions ]; in this example the number of output \\ndimensions is 10, since the  Dense  layer has 10 units).  Here is the updated model:  \\nmodel = keras.models.Sequential ([ \\n    keras.layers.SimpleRNN (20, return_sequences =True, input_shape =[None, 1]), \\n    keras.layers.SimpleRNN (20, return_sequences =True), \\n    keras.layers.TimeDistributed (keras.layers.Dense(10)) \\n])', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 12}),\n",
       " Document(page_content=']) \\nThe Dense  layer actually supports sequences as inputs (and even higher -dimensional inputs): it \\nhandles them just like  TimeDistributed(Dense(…)) , meaning it is applied to the  last input \\ndimension only (independently across all time steps). Thus, we could replace the last layer with \\njust Dense(10) . For the sake of clarity, however, we will keep \\nusing  TimeDistributed(Dense(10))  because it makes it clear that the  Dense  layer is a pplied \\nindependently at each time step and that the model will output a sequence, not just a single \\nvector.  \\nAll outputs are needed during training, but only the output at the last time step is useful for \\npredictions and for evaluation. So although we will rely on the MSE over all the outputs for', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 12}),\n",
       " Document(page_content='training, we will use a custom metric for evaluation, to only compute the MSE over the output at \\nthe last time step:  \\ndef last_time_step_mse (Y_true, Y_pred): \\n    return keras.metrics.mean_squared_error (Y_true[:, -1], Y_pred[:, -1]) \\n \\noptimizer  = keras.optimizers .Adam(lr=0.01) \\nmodel.compile(loss=\"mse\", optimizer =optimizer , metrics=[last_time_step_mse ]) \\nWe get a validation MSE of about 0.006, which is 25% better than the previous model. You can \\ncombine this approach with  the first one: just predict the next 10 values using this RNN, then \\nconcatenate these values to the input time series and use the model again to predict the next 10 \\nvalues, and repeat the process as many times as needed. With this approach, you can genera te', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 12}),\n",
       " Document(page_content='arbitrarily long sequences. It may not be very accurate for long -term predictions, but it may be \\njust fine if your goal is to generate original m usic or text, as we will see in next section.  \\nTIP  \\nWhen forecasting time series, it is often useful to have some error bars along with your predictions. For \\nthis, an efficient techniq ue is MC Dropout, introduced before : add an MC Dropout layer within each \\nmemory cell, dropping part of the inputs and hidden states. After training, to forecast a new time series, \\nuse the model many times and compute the mean and standard deviation of the predictions at each time \\nstep. \\nSimple RNNs can be quite good at forecasting time series or handling other kinds of sequences,', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 12}),\n",
       " Document(page_content='but they do not perform as well on long time series or s equences. Let’s discuss why and see what \\nwe can do about it.', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 12}),\n",
       " Document(page_content='Handling Long Sequences  \\nTo train an RNN on long sequences, we must run it over many time steps, making the unrolled \\nRNN a very deep network. Just like any deep neural network it may suffer from t he unstabl e \\ngradients problem, discussed before : it may take forever to train, or training may be unstable. \\nMoreover, when an RNN processes a long sequence, it will gradually forget the first inputs in the \\nsequence. Let’s look at both these problems, starting with the unstable gradients problem.  \\nFighting the Unstable Gradients Problem  \\nMany  of the tricks we used in deep nets to  alleviate the unstable gradients problem can also be \\nused for RNNs: good parameter initialization, faster optimizers, dropout, and so on. However,', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 13}),\n",
       " Document(page_content='nonsaturating activation functions (e.g., ReLU) may not help as much here; in fact, they may \\nactually lead t he RNN to be even more unstable during training. Why? Well, suppose Gradient \\nDescent updates the weights in a way that increases the outputs slightly at the first time step. \\nBecause the same weights are used at every time step, the outputs at the second ti me step may \\nalso be slightly increased, and those at the third, and so on until the outputs explode —and a \\nnonsaturating activation function does not prevent that. You can reduce this risk by using a \\nsmaller learning rate, but you can also simply use a satu rating activation function like the \\nhyperbolic tangent (this explains why it is the default). In much the same way, the gradients', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 13}),\n",
       " Document(page_content='themselves can explode. If you notice that training is unstable, you may want to monitor the size \\nof the gradients (e.g., usin g TensorBoard) and perhaps use Gradient Clipping.  \\nMoreover, Batch Normalization cannot be used as efficiently with RNNs as with deep \\nfeedforward nets. In fact, you cannot use it between time steps, only between recurrent layers. To \\nbe more precise, it is t echnically possible to add a BN layer to a memory cell (as we will see \\nshortly) so that it will be applied at each time step (both on the inputs for that time step and on \\nthe hidden state from the previous step). However, the same BN layer will be used at each time \\nstep, with the same parameters, regardless of the actual scale and offset of the inputs and hidden', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 13}),\n",
       " Document(page_content='state. In practice, this does not yield good results, as was demonstrated by César Laurent et al. in \\na 2015  paper : the authors found that BN was slightly beneficial only when it was applied to the \\ninputs, not to the hidden states. In other words, it was slightly better than nothing when applied \\nbetween recurrent layers (i.e., vertically in  Figure  7), but not within recurrent layers (i.e., \\nhorizontally). In Keras this can be done simply by adding a  BatchNormalization  layer before \\neach recurrent layer, but don’t expect too much from it.  \\nAnother  form of normalization often works better with RNNs:  Layer  Normalization . This idea \\nwas introduced by Jimmy Lei Ba et al. in a  2016  paper : it is very similar to Batch Normalization,', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 13}),\n",
       " Document(page_content='but instead of normalizing across the batch dimension, it normalizes across the features \\ndimension. One advantage is that it can compute the required statistics on the fly, at each time \\nstep, independently for each i nstance. This also means that it behaves the same way during \\ntraining and testing (as opposed to BN), and it does not need to use exponential  moving  averages \\nto estimate the feature statistics across all instances in the training set. Like BN, Layer \\nNormal ization learns a scale and an offset parameter for each input. In an RNN, it is typically \\nused right after the linear combination of the inputs and the hidden states.', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 13}),\n",
       " Document(page_content='Let’s use tf.keras to implement Layer Normalization within a simple memory cell. For this , we \\nneed to define a custom memory cell. It is just like a regular layer, except its  call()  method \\ntakes two arguments: the  inputs  at the current time step and the hidden  states  from the previous \\ntime step. Note that the  states  argument is a list containi ng one or more tensors. In the case of a \\nsimple RNN cell it contains a single tensor equal to the outputs of the previous time step, but \\nother cells may have multiple state tensors (e.g., an  LSTMCell  has a long -term state and a short -\\nterm state, as we will  see shortly). A cell must also have a  state_size  attribute and \\nan output_size  attribute. In a simple RNN, both are simply equal to the number of units. The', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 14}),\n",
       " Document(page_content='following code implements a custom memory cell which will behave like a  SimpleRNNCell , \\nexcept it wi ll also apply Layer Normalization at each time step:  \\nclass LNSimpleRNNCell (keras.layers.Layer): \\n    def __init__ (self, units, activation =\"tanh\", **kwargs): \\n        super().__init__ (**kwargs) \\n        self.state_size  = units \\n        self.output_size  = units \\n        self.simple_rnn_cell  = keras.layers.SimpleRNNCell (units, \\n                                                          activation =None) \\n        self.layer_norm  = keras.layers.LayerNormalization () \\n        self.activation  = keras.activations .get(activati on) \\n    def call(self, inputs, states): \\n        outputs, new_states  = self.simple_rnn_cell (inputs, states)', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 14}),\n",
       " Document(page_content='norm_outputs  = self.activation (self.layer_norm (outputs)) \\n        return norm_outputs , [norm_outputs ] \\nThe code is quite straightforward. 5 Our LNSimpleRNNCell  class inherits from \\nthe keras.layers.Layer  class, just like any custom layer. The constructor takes the numbe r of \\nunits and the desired activation function, and it sets the  state_size  and output_size  attributes, \\nthen creates a  SimpleRNNCell  with no activation function (because we want to perform Layer \\nNormalization after the linear operation but before the activa tion function). Then the constructor \\ncreates the  LayerNormalization  layer, and finally it fetches the desired activation function. \\nThe call()  method starts by applying the simple RNN cell, which computes a linear', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 14}),\n",
       " Document(page_content='combination of the current inputs and the p revious hidden states, and it returns the result twice \\n(indeed, in a  SimpleRNNCell , the outputs are just equal to the hidden states: in other \\nwords,  new_states[0]  is equal to  outputs , so we can safely ignore  new_states  in the rest of \\nthe call()  method). Ne xt, the  call()  method applies Layer Normalization, followed by the \\nactivation function. Finally, it returns the outputs twice (once as the outputs, and once as the new \\nhidden states). To use this custom cell, all we need to do is create a  keras.layers.RNN  layer, \\npassing it a cell instance:  \\nmodel = keras.models.Sequential ([ \\n    keras.layers.RNN(LNSimpleRNNCell (20), return_sequences =True, \\n                     input_shape =[None, 1]),', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 14}),\n",
       " Document(page_content='input_shape =[None, 1]), \\n    keras.layers.RNN(LNSimpleRNNCell (20), return_sequences =True), \\n    keras.layers.TimeDistributed (keras.layers.Dense(10)) \\n]) \\nSimilarly, you could create a custom cell to apply dropout between each time step. But there’s a \\nsimpler way: all recurrent layers (except for  keras.layers.RNN ) and all cells provided by Keras', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 14}),\n",
       " Document(page_content='have a  dropout  hyperparameter and a  recurrent_dropout  hyperparameter: the former defines the \\ndropout rate to apply to the inputs (at each time step), and the latter defines the dropout rate for \\nthe hidden states (also at each time step). No need to create a custom cell to apply dropout at \\neach time step in an RNN.  \\nWith these techniques, you can alleviate the unstable gradients problem and train an RNN much \\nmore efficiently. Now let’s look at how to deal with the short -term memory problem.  \\nTackling the Short -Term Memory P roblem  \\nDue to the transformations that the data goes through when traversing an RNN, some \\ninformation is lost at each time step. After a while, the RNN’s state contains virtually no trace of', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 15}),\n",
       " Document(page_content='the first inputs. This can be a showstopper. Imagine Dory the fis h trying to translate a long \\nsentence; by the time she’s finished reading it, she has no clue how it started. To tackle this  \\nproblem, various types of cells with long -term memory have been introduced. They have proven \\nso successful that the basic cells are not used much anymore. Let’s first look at the most popular \\nof these long -term memory cells: the LSTM cell.  \\nLSTM cells  \\nThe Long  Short -Term  Memory  (LSTM) cell  was proposed  in 1997  by Sepp Hochreiter and \\nJürgen S chmidhuber and gradually improved over the years by several researchers, such as  Alex  \\nGraves , Haşim  Sak, and Wojciech  Zaremba . If you consider the LSTM cell as a black box, it can', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 15}),\n",
       " Document(page_content='be used very much like a basic cell, except it will perform much better; training will converge \\nfaster, and it will detect l ong-term dependencies in the data. In Keras, you can simply use \\nthe LSTM layer instead of the  SimpleRNN  layer:  \\nmodel = keras.models.Sequential ([ \\n    keras.layers.LSTM(20, return_sequences =True, input_shape =[None, 1]), \\n    keras.layers.LSTM(20, return_seque nces=True), \\n    keras.layers.TimeDistributed (keras.layers.Dense(10)) \\n]) \\nAlternatively, you could use the general -purpose  keras.layers.RNN  layer, giving it \\nan LSTMCell  as an argument:  \\nmodel = keras.models.Sequential ([ \\n    keras.layers.RNN(keras.layers.LSTMCell(20), return_sequences =True, \\n                     input_shape =[None, 1]),', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 15}),\n",
       " Document(page_content='input_shape =[None, 1]), \\n    keras.layers.RNN(keras.layers.LSTMCell (20), return_sequences =True), \\n    keras.layers.TimeDistributed (keras.layers.Dense(10)) \\n]) \\nHowever, the  LSTM layer uses an optimized imple menta tion when running on a GPU  (we will see \\nlater), so in general it is preferable to use it (the  RNN layer is mostly useful w hen you define \\ncustom cells, as we did earlier).', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 15}),\n",
       " Document(page_content='So how does an LSTM cell work? Its architecture is shown in  Figure  9. \\nIf you don’t look at what’s inside the box, the LSTM cell looks exactly like a regular cell, except \\nthat its state is split into two vectors:  h(t) and c(t) (“c” stands for “cell”). You can think of  h(t) as \\nthe short -term state and  c(t) as the long -term state.  \\n \\nFigure  9. LSTM cell  \\nNow let’s open the box! The key idea is that the network can learn what to store in the long -term \\nstate, what to throw away, and what to read from it. As the long -term state  c(t–1) traverses the \\nnetwork from left to right, you can  see that it first goes through  a forget  gate, dropping some \\nmemories, and then it adds some new memories via the addition operation (which adds the', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 16}),\n",
       " Document(page_content='memories that were selected by an  input  gate). The result  c(t) is sent straight out, without any \\nfurther tr ansformation. So, at each time step, some memories are dropped and some memories \\nare added. Moreover, after the addition operation, the long -term state is copied and passed \\nthrough the tanh function, and then the result is filtered by  the output  gate. This  produces the \\nshort -term state  h(t) (which is equal to the cell’s output for this time step,  y(t)). Now let’s look at \\nwhere new memories come from and how the gates work.  \\nFirst, the current input vector  x(t) and the previous short -term state  h(t–1) are fed  to four different \\nfully connected layers. They all serve a different purpose:', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 16}),\n",
       " Document(page_content='\\uf0b7 The main layer is the one that outputs  g(t). It has the usual role of analyzing the current \\ninputs  x(t) and the previous (short -term) state  h(t–1). In a basic cell, there is not hing other \\nthan this layer, and its output goes straight out to  y(t) and h(t). In contrast, in an LSTM cell \\nthis layer’s output does not go straight out, but instead its most important parts are stored \\nin the long -term state (and the rest is  dropped ).', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 16}),\n",
       " Document(page_content='\\uf0b7 The three other layers  are gate controllers . Since they use the logistic activation function, \\ntheir outputs range from 0 to 1. As you can see, their outputs are fed to element -wise \\nmultiplication operations, so if they output 0s they close the gate, and if the y output 1s \\nthey open it. Specifically:  \\n\\uf0b7 The forget  gate (controlled by  f(t)) controls which parts of the long -term state \\nshould be erased.  \\n\\uf0b7 The input  gate (controlled by  i(t)) controls which parts of  g(t) should be added to \\nthe long -term state.  \\n\\uf0b7 Finally, the  output  gate (controlled by  o(t)) controls which parts of the long -term \\nstate should be read and output at this time step, both to  h(t) and to  y(t). \\nIn short, an LSTM cell can learn to recognize an important input (that’s the role of the input', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 17}),\n",
       " Document(page_content='gate), store  it in the long -term state, preserve it for as long as it is needed (that’s the role of the \\nforget gate), and extract it whenever it is needed. This explains why these cells have been \\namazingly successful at capturing long -term patterns in time series, lon g texts, audio recordings, \\nand more.  \\nEquation  3 summarizes how to compute the cell’s long -term state, its short -term state, an d its \\noutput at each time step for a single instance (the equations for a whole mini -batch are very \\nsimilar).  \\nEquation  3. LSTM computations  \\n \\nIn this equation:  \\n\\uf0b7 Wxi, Wxf, Wxo, Wxg are the weight matrices of each of the four layers for their \\nconnection to the input vector  x(t). \\n\\uf0b7 Whi, Whf, Who, and  Whg are the weight matrices of each of t he four layers for their', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 17}),\n",
       " Document(page_content='connection to the previous short -term state  h(t–1). \\n\\uf0b7 bi, bf, bo, and  bg are the bias terms for each of the four layers. Note that TensorFlow \\ninitializes  bf to a vector full of 1s instead of 0s. This prevents forgetting everything at  the \\nbeginning of training.  \\nPeephole connections  \\nIn a regular LSTM cell, the gate controllers can look only at the input  x(t) and the previous short -\\nterm state  h(t–1). It may be a good idea to give them a bit more context by letting them peek at the \\nlong-term state as well. This idea was  proposed  by Felix  Gers  and Jürgen  Schmidhuber  in', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 17}),\n",
       " Document(page_content='2000 . They proposed an LSTM variant with  extra connections called  peephole  connections : the \\nprevious long -term state  c(t–1) is added as an input to the controllers of the forget gate and the \\ninput gate, and the current long -term state  c(t) is added as input to  the controller of the output \\ngate. This often improves performance, but not always, and there is no clear pattern for which \\ntasks are better off with or without them: you will have to try it on your task and see if it helps.  \\nIn Keras, the  LSTM layer is ba sed on the  keras.layers.LSTMCell  cell, which does not support \\npeepholes. The experimental  tf.keras.experimental.PeepholeLSTMCell  does, however, so you \\ncan create a  keras.layers.RNN  layer and pass a  PeepholeLSTMCell  to its constructor.', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 18}),\n",
       " Document(page_content='There are many other variants of the LSTM cell. One particularly popular variant is the GRU \\ncell, which we will look at now.  \\nGRU cells  \\nThe Gated  Recurrent  Unit (GRU) cell  (see Figure  10) was proposed by Kyunghyun Cho et al. in \\na 2014  paper  that also introduced the Encoder –Decoder network we discussed earlier.  \\n \\nFigure  10. GRU cell  \\nThe GRU cell is a simplified version of the LSTM cell, and it seems to perform just as \\nwell (which explains its growing popularity). These are the main simplifications:  \\n\\uf0b7 Both state vectors are merged into a single vector  h(t). \\n\\uf0b7 A single gate controller  z(t) controls both the  forget gate and the input gate. If the gate \\ncontroller outputs a 1, the forget gate is open (=  1) and the input gate is closed (1  – 1 = 0).', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 18}),\n",
       " Document(page_content='If it outputs a 0, the opposite happens. In other words, whenever a memory must be \\nstored, the location where it wi ll be stored is erased first. This is actually a frequent \\nvariant to the LSTM cell in and of itself.  \\n\\uf0b7 There is no output gate; the full state vector is output at every time step. However, there \\nis a new gate controller  r(t) that controls which part of the p revious state will be shown to \\nthe main layer ( g(t)).', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 18}),\n",
       " Document(page_content='Equation  4 summarizes how to compute the cell’s state at each time step f or a single instance.  \\nEquation  4. GRU computations  \\n \\nKeras provides a  keras.layers.GRU  layer (based on the  keras.layers.GRUCell  memory cell); \\nusing it is just a matter of replacing  SimpleRNN  or LSTM with GRU. \\nLSTM and GRU cells are one of the main reasons behind the success of RNNs. Yet while they \\ncan tackle much longer sequences than simple RNNs, they still have a fairly limited short -term \\nmemory, and they have a hard time learning  long-term patterns in sequences of 100 time steps or \\nmore, such as audio samples, long time series, or long sentences. One way to solve this is to \\nshorten the input sequences, for example using 1D convolutional layers.', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 19}),\n",
       " Document(page_content='Using 1D convolutional layers to pro cess sequences  \\nBefore  we saw that a 2D convolutional layer works by sliding several fairly small kernels (or \\nfilters) acros s an image, producing multiple 2D feature maps (one per kernel). Similarly, a 1D \\nconvolutional layer slides several kernels across a sequence, producing a 1D feature map per \\nkernel. Each kernel will learn to detect a single very short sequential pattern (n o longer than the \\nkernel size). If you use 10 kernels, then the layer’s output will be composed of 10 1 -dimensional \\nsequences (all of the same length), or equivalently you can view this output as a single 10 -\\ndimensional sequence. This means that you can bu ild a neural network composed of a mix of', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 19}),\n",
       " Document(page_content='recurrent layers and 1D convolutional layers (or even 1D pooling layers). If you use a 1D \\nconvolutional layer with a stride of 1 and  \"same\"  padding, then the output sequence will have the \\nsame length as the input s equence. But if you use  \"valid\"  padding or a stride greater than 1, then \\nthe output sequence will be shorter than the input sequence, so make sure you adjust the targets \\naccordingly. For example, the following model is the same as earlier, except it starts  with a 1D \\nconvolutional layer that downsamples the input sequence by a factor of 2, using a stride of 2. The \\nkernel size is larger than the stride, so all inputs will be used to compute the layer’s output, and \\ntherefore the model can learn to preserve the  useful information, dropping only the unimportant', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 19}),\n",
       " Document(page_content='details. By shortening the sequences, the convolutional layer may help the  GRU layers detect \\nlonger patterns. Note that we must also crop off the first three time steps in the targets (since the \\nkernel’s s ize is 4, the first output of the convolutional layer will be based on the input time steps \\n0 to 3), and downsample the targets by a factor of 2:  \\nmodel = keras.models.Sequential ([ \\n    keras.layers.Conv1D(filters=20, kernel_size =4, strides=2, padding=\"valid\", \\n                        input_shape =[None, 1]), \\n    keras.layers.GRU(20, return_sequences =True), \\n    keras.layers.GRU(20, return_sequences =True), \\n    keras.layers.TimeDistributed (keras.layers.Dense(10))', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 19}),\n",
       " Document(page_content=']) \\n \\nmodel.compile(loss=\"mse\", optimizer =\"adam\", metrics=[last_time_step_mse ]) \\nhistory = model.fit(X_train, Y_train[:, 3::2], epochs=20, \\n                    validation_data =(X_valid, Y_valid[:, 3::2])) \\nIf you train and evaluate this model, you will find that it is the best model so far. The \\nconvolutional l ayer really helps. In fact, it is actually possible to use only 1D convolutional \\nlayers and drop the recurrent layers entirely!  \\nWaveNet  \\nIn a 2016  paper  Aaron van den Oord and other DeepMind researchers introduced an  architecture \\ncalled  WaveNet . They stacked 1D convolutional layers, doubling the dilation rate (how spread \\napart each neuron’s inputs are) at every layer: the first convolutional layer gets a glimpse of just', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 20}),\n",
       " Document(page_content='two time steps at a time, while the next one see s four time steps (its receptive field is four time \\nsteps long), the next one sees eight time steps, and so on (see  Figure  11). This way, the lower \\nlayers learn short -term patterns, while the higher layers learn long -term patterns. Thanks to the \\ndoubling dilation rate, the network can process extremely large sequences very efficiently.  \\n \\nFigure  11. WaveNet architecture  \\nIn the W aveNet paper, the authors actually stacked 10 convolutional layers with dilation rates of \\n1, 2, 4, 8, …, 256, 512, then they stacked another group of 10 identical layers (also with dilation \\nrates 1, 2, 4, 8, …, 256, 512), then again another identical group  of 10 layers. They justified this', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 20}),\n",
       " Document(page_content='architecture by pointing out that a single stack of 10 convolutional layers with these dilation rates \\nwill act like a super -efficient convolutional layer with a kernel of size 1,024 (except way faster, \\nmore powerful, and using significantly fewer parameters), which is why they stacked 3 such \\nblocks. They also left -padded the input sequences with a number of zeros equal to the dilation \\nrate before every layer, to preserve the same sequence length throughout the network. Her e is \\nhow to implement a simplified WaveNet to tackle the same sequences as  earlier : \\nmodel = keras.models.Sequential () \\nmodel.add(keras.layers.InputLayer (input_shape =[None, 1])) \\nfor rate in (1, 2, 4, 8) * 2:', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 20}),\n",
       " Document(page_content='model.add(keras.layers.Conv1D(filters=20, kernel_size =2, padding=\"causal\" , \\n                                  activation =\"relu\", dilation_rate =rate)) \\nmodel.add(keras.layers.Conv1D(filters=10, kernel_size =1)) \\nmodel.compile(loss=\"mse\", optimizer =\"adam\", metrics=[last_time_step_mse ]) \\nhistory = model.fit(X_train, Y_train, epochs=20, \\n                    validation_data =(X_valid, Y_valid)) \\nThis Sequential  model starts with an explicit input layer (this is simpler than trying to \\nset input_shape  only on the first layer), then continues with a 1D convolution al layer \\nusing  \"causal\"  padding: this ensures that the convolutional layer does not peek into the future \\nwhen making predictions (it is equivalent to padding the inputs with the right amount of zeros on', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 21}),\n",
       " Document(page_content='the left and using  \"valid\"  padding). We then add  similar pairs of layers using growing dilation \\nrates: 1, 2, 4, 8, and again 1, 2, 4, 8. Finally, we add the output layer: a convolutional layer with \\n10 filters of size 1 and without any activation function. Thanks to the padding layers, every \\nconvolutional lay er outputs a sequence of the same length as the input sequences, so the targets \\nwe use during training can be the full sequences: no need to crop them or downsample them.  \\nThe last two models offer the best performance so far in forecasting our time series!  In the \\nWaveNet paper, the authors achieved state -of-the-art performance on various audio tasks (hence \\nthe name of the architecture), including text -to-speech tasks, producing incredibly realistic', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 21}),\n",
       " Document(page_content='voices across several languages. They also used the model t o generate music, one audio sample \\nat a time. This feat is all the more impressive when you realize that a single second of audio can \\ncontain tens of thousands of time steps —even LSTMs and GRUs cannot handle such long \\nsequences.  \\nIn next section  we will con tinue to explore RNNs, and we will see how they can tackle various \\nNLP tasks.', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 21})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = chunk_data(docs=docs)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x000001D457CB4AD0>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x000001D457C13550>, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key='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', openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, http_client=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = embeddings.embed_query(\"How are you?\")\n",
    "len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.init(\n",
    "    api_key = \"7e6142eb-a390-4f10-9a37-7840a50a87e4\",\n",
    "    environment = \"gcp-starter\"\n",
    ")\n",
    "index_name = \"langchainvector\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = Pinecone.from_documents(documents,embeddings,index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_query(query,k=2):\n",
    "    matching_results = index.similarity_search(query,k=k)\n",
    "    return matching_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.5,model_name=\"\")\n",
    "chain = load_qa_chain(llm,chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_answer(query):\n",
    "    doc_search = retrieve_query(query)\n",
    "    print(doc_search)\n",
    "    response = chain.run(input_documents=doc_search,question=query)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='voices across several languages. They also used the model t o generate music, one audio sample \\nat a time. This feat is all the more impressive when you realize that a single second of audio can \\ncontain tens of thousands of time steps —even LSTMs and GRUs cannot handle such long \\nsequences.  \\nIn next section  we will con tinue to explore RNNs, and we will see how they can tackle various \\nNLP tasks.', metadata={'page': 21.0, 'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf'}), Document(page_content='voices across several languages. They also used the model t o generate music, one audio sample \\nat a time. This feat is all the more impressive when you realize that a single second of audio can \\ncontain tens of thousands of time steps —even LSTMs and GRUs cannot handle such long \\nsequences.  \\nIn next section  we will con tinue to explore RNNs, and we will see how they can tackle various \\nNLP tasks.', metadata={'page': 21.0, 'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf'})]\n",
      " LSTMs and GRUs are used to handle long sequences that contain tens of thousands of time steps. They are used to tackle various NLP tasks.\n"
     ]
    }
   ],
   "source": [
    "our_query = \"Why are we using LSTM?\"\n",
    "result = retrieve_answer(our_query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Deep Computer Vision Using  Convolutional  \\nNeural  Networks  \\nAlthough  IBM’s Deep Blue supercomputer beat the chess world champion Garry Kasparov back \\nin 1996, it wasn’t until fairly recently that computers were able to reliably perform seemingly \\ntrivial tasks such as detecting a puppy in a picture or recognizing spoken words. Why are these \\ntasks so effortless to us humans? The answer lies in the fact that perception largely takes place \\noutside the realm of our consciousness, within specialized visual, auditory,  and other sensory \\nmodules in our brains. By the time sensory information reaches our consciousness, it is already \\nadorned with high -level features; for example, when you look at a picture of a cute puppy, you \\ncannot choose  not to see the puppy,  not to not ice its cuteness. Nor can you explain  how you \\nrecognize a cute puppy; it’s just obvious to you. Thus, we cannot trust our subjective experience: \\nperception is not trivial at all, and to understand it we must look at how the sensory modules \\nwork.  \\nConvolutio nal neural networks (CNNs) emerged from the study of the brain’s visual cortex, and \\nthey have been used in image recognition since the 1980s. In the last few years, thanks to the \\nincrease in computational power, the amount of available training da ta, and t he tricks presented \\nbefore  for training deep nets, CNNs have managed to achieve superhuman performance on some \\ncomplex visual tasks. They power image search services, self -driving cars, automatic video \\nclassification systems, and more. Moreover, CNNs  are n ot restricted to visual perception: they \\nare also successful at many other tasks, such as voice recognition and natural language \\nprocessing. However, we will focus on visual applications for now.  \\nIn this section  we will explore where CNNs came from, what t heir building blocks look like, and \\nhow to implement them using TensorFlow and Keras. Then we will discuss some of the best \\nCNN architectures, as well as other visual tasks, including object detection (classifying multiple \\nobjects in an image and placing b ounding boxes around them) and semantic segmentation \\n(classifying each pixel according to the class of the object it belongs to).  \\nThe Architecture of the Visual Cortex  \\nDavid H. Hubel  and Torsten Wiesel performed a series of experiments on cats in  1958  and 1959  \\n(and a  few years  later on monkeys ), giving crucial insights into the structure of the visual cortex \\n(the authors received the Nobel Prize in  Physiology or Medicine in 1981 for their work). In \\nparticular, they showed that many neurons in the visual cortex have a small  local  receptive  field, \\nmeaning they react only to visual stimuli located in a limited region of the visual field \\n(see Figure  1, in which the local receptive fields of five neurons are represented by dashed \\ncircles). The receptive fields of diffe rent neurons may overlap, and together they tile the whole \\nvisual field.  \\nMoreover, the authors showed that some neurons react only to images of horizontal lines, while \\nothers react only to lines with different orientations (two neurons may have the same re ceptive ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 0}), Document(page_content='field but react to different line orientations). They also noticed that some neurons have larger \\nreceptive fields, and they react to more complex patterns that are combinations of the lower -level \\npatterns. These observations led to the idea that th e higher -level neurons are based on the outputs \\nof neighboring lower -level neurons (in  Figure  1, notice that each neu ron is connected only to a \\nfew neurons from the previous layer). This powerful architecture is able to detect all sorts of \\ncomplex patterns in any area of the visual field.  \\n \\nFigure  1. Biological neurons in the visual cortex respond to specific patterns in  small regions of the visual field called receptive fields; as the visual \\nsignal makes its way through consecutive brain modules, neurons respond to more complex patterns in larger receptive fields.  \\nThese studies of the visual cortex inspired the  neocognitron , introduced in 1980, which gradually \\nevolved into what we now call  convolutional  neural  networks . An important milestone was \\na 1998  paper  by Yann LeCun et al. that introduced the  famous  LeNet -5 architecture, widely used \\nby banks to recognize handwritten check numbers. This architecture has some building blocks \\nthat you already know, such as fully connected layers and sigmoid activation functions, but it \\nalso introduces two new bui lding blocks:  convolutional  layers  and pooling  layers . Let’s look at \\nthem now.  \\nNOTE  \\nWhy not simply use a deep neural network with fully connected layers for image recognition tasks? \\nUnfortunately, although this works fine for small images (e.g., MNIST), it  breaks down for larger images \\nbecause of the huge number of parameters it requires. For example, a 100 × 100 –pixel image has 10,000 \\npixels, and if the first layer has just 1,000 neurons (which already severely restricts the amount of \\ninformation transmitt ed to the next layer), this means a total of 10 million connections. And that’s just the \\nfirst layer. CNNs solve this problem using partially connected layers and weight sharing.  \\nConvolutional Layers  \\nThe most important building block of a CNN is the  convol utional  layer : neurons in the first \\nconvolutional layer are not connected to every single pixel in the input image (like they were in \\nthe layers discussed in previous section s), but only to pixels in their receptive fields \\n(see Figure  2). In turn, each neuron in the second convolutional layer is connected only to \\nneurons located within a small rectangle in the first layer. This  architecture allows the network to \\nconcentrate on small low -level features in the first hidden layer, then assemble them into larger \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 1}), Document(page_content='higher -level features in the next hidden layer, and so on. This hierarchical structure is common in \\nreal-world images, whi ch is one of the reasons why CNNs work so well for image recognition.  \\n \\nFigure  2. CNN layers with rectangular local receptive fields  \\nNOTE  \\nAll the multilayer neural networks we’ve looked at so far had layers composed of a long line of neurons, \\nand we had to  flatten input images to 1D before feeding them to the neural network. In a CNN each layer \\nis represented in 2D, which makes it easier to match neurons with their corresponding inputs.  \\nA neuron located in row  i, column  j of a given layer is connected to th e outputs of the neurons in \\nthe previous layer located in rows  i to i + fh – 1, columns  j to j + fw – 1, where  fh and fw are the \\nheight and width of the receptive field (see  Figure  3). In order for a layer to have the same height \\nand width as the previous layer, it is common to add zeros around the inputs, as shown in the \\ndiagram. This  is called  zero padding . \\n \\nFigure  3. Connections between layers and zero padding  \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 2}), Document(page_content='It is also possible to connect a large input layer to a much smaller layer by spacing out the \\nreceptive fields, as shown in  Figure  4. This dramatically reduces the model’s computational \\ncomplexity. The shift from one receptive field to  the next is called the  stride . In the diagram, a 5 \\n× 7 input layer (plus zero padding) is connected to a 3 × 4 layer, using 3 × 3 receptive fields and \\na stride of 2 (in this example the stride is the same in both directions, but it does not have to be \\nso). A neuron located in row  i, column  j in the upper layer is connected to the outputs of the \\nneurons in t he previous layer located in rows  i × sh to i × sh + fh – 1, \\ncolumns  j × sw to j × sw + fw – 1, where  sh and sw are the vertical and horizontal strides.  \\n \\nFigure  4. Reducing dimensionality using a stride of 2  \\nFilters  \\nA neuron’s weights can be represented a s a small image the size of the receptive field. For \\nexample,  Figure  5 shows two possible sets of weights, called  filters  (or convolution  kernels ). The \\nfirst one is represented as a black square with a vertical white line in the middle (it is a 7 × 7 \\nmatrix full of 0s except for the central column, which is full of 1s); neurons using these weights \\nwill ignore everything in their r eceptive field except for the central vertical line (since all inputs \\nwill get multiplied by 0, except for the ones located in the central vertical line). The second filter \\nis a black square with a horizontal white line in the middle. Once again, neurons u sing these \\nweights will ignore everything in their receptive field except for the central horizontal line.  \\nNow if all neurons in a layer use the same vertical line filter (and the same bias term), and you \\nfeed the network the input image shown in  Figure  5 (the bottom image), the layer will output the \\ntop-left image. Notice that the vertical white lines get enhanced while the rest gets blurred. \\nSimilarly, the upper -right image is what you get if all neurons use the same horizontal line filter; \\nnotice that the horizontal white lines get enhanced while the rest is blurred out. Thus, a layer  full \\nof neurons using the same filter output s a feature  map, which highlights the areas in an image \\nthat activate the filter the most. Of course, you do not have to define the filters manually: instead, \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 3}), Document(page_content='during training the convolutional layer will automatically learn the most useful filters for its task, \\nand the layers above will learn to combine them into more complex  patterns . \\n \\nFigure  5. Applying two different filters to get two feature maps  \\nStacking Multiple Feature Maps  \\nUp to now, for simplicity, I have represented the output of each convolution al layer as a 2D \\nlayer, but in reality a convolutional layer has multiple filters (you decide how many) and outputs \\none feature map per filter, so it is more accurately represented in 3D (see  Figure  6). It has one \\nneuron per pixel in each feature map, and all neurons within a given feature map share the same \\nparameters (i.e., the same weights and bias term). Neurons in d ifferent feature maps use different \\nparameters. A neuron’s receptive field is the same as described earlier, but it extends across all \\nthe previous layers’ feature maps. In short, a convolutional layer simultaneously applies multiple \\ntrainable filters to i ts inputs, making it capable of detecting multiple features anywhere in its \\ninputs.  \\nNOTE  \\nThe fact that all neurons in a feature map share the same parameters dramatically reduces the number of \\nparameters in the model. Once the CNN has learned to recognize a pattern in one location, it can \\nrecognize it in any other location. In contrast, once a regular DNN has learned to recognize a pattern in \\none location, it can recognize it only in that particular location.  \\nInput images are also composed of multiple subla yers: one  per color  channel . There are typically \\nthree: red, green, and blue (RGB). Grayscale images have just one  channel , but some images \\nmay have much more —for example, satellite images that capture extra light frequencies (such as \\ninfrared).  \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 4}), Document(page_content=' \\nFigure  6. Convolutional layers with multiple feature maps, and images with three color channels  \\nSpecifically, a neuron located in row  i, column  j of the feature map  k in a given convolutional \\nlayer  l is connected to the outputs of the neurons in the previous layer  l – 1, located in \\nrows  i × sh to i × sh + fh – 1 and columns  j × sw to j × sw + fw – 1, across all feature maps (in \\nlayer  l – 1). Note that all neurons located in the same row  i and column  j but in different feature \\nmaps are connected to the outputs of th e exact same neurons in the previous layer.  \\nEquation  1 summarizes the preceding explanations in one big mathematic al equation: it shows \\nhow to compute the output of a given neuron in a convolutional layer. It is a bit ugly due to all \\nthe different indices, but all it does is calculate the weighted sum of all the inputs, plus the bias \\nterm.  \\nEquation  1. Computing the ou tput of a neuron in a convolutional layer  \\n \\nIn this equation:  \\n\\uf0b7 zi, j, k is the output of the neuron located in row  i, column  j in feature map  k of the \\nconvolutional layer (layer  l). \\n\\uf0b7 As explained earlier,  sh and sw are the v ertical and horizontal strides,  fh and fw are the \\nheight and width of the receptive field, and  fn′ is the number of feature maps in the \\nprevious layer (layer  l – 1). \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 5}), Document(page_content='\\uf0b7 xi′, j′, k′ is the output of the neuron located in layer  l – 1, row  i′, column  j′, feature  map k′ (or \\nchannel  k′ if the previous layer is the input layer).  \\n\\uf0b7 bk is the bias term for feature map  k (in layer  l). You can think of it as a knob that tweaks \\nthe overall brightness of the feature map  k. \\n\\uf0b7 wu, v, k′ ,k is the connection weight between any n euron in feature map  k of the layer  l and \\nits input located at row  u, column  v (relative to the neuron’s receptive field), and feature \\nmap k′. \\nTensorFlow Implementation  \\nIn TensorFlow, each input image is typically represented as a 3D tensor of shape [ heigh t, width,  \\nchannels ]. A mini -batch is represented as a 4D tensor of shape [ mini-batch  size, height,  width,  \\nchannels ]. The weights of a convolutional layer are represented as a 4D tensor of shape \\n[fh, fw, fn′, fn]. The bias terms of a convolutional layer are  simply represented as a 1D tensor of \\nshape [ fn]. \\nLet’s look at a simple example. The following code loads two sample images, using Scikit -\\nLearn’s  load_sample_image()  (which loads two color images, one of a Chinese temple, and \\nthe other of a flower), then it creates two filters and applies them to both images, and finally it \\ndisplays one of the resulting feature maps. Note that you must pip install the  Pillow  package to \\nuse load_sample_image() . \\nfrom sklearn.datasets  import load_sample_image  \\n \\n# Load sample images \\nchina = load_sample_image (\"china.jpg\" ) / 255 \\nflower = load_sample_image (\"flower.jpg\" ) / 255 \\nimages = np.array([china, flower]) \\nbatch_size , height, width, channels = images.shape \\n \\n# Create 2 filters \\nfilters = np.zeros(shape=(7, 7, channels , 2), dtype=np.float32) \\nfilters[:, 3, :, 0] = 1  # vertical  line \\nfilters[3, :, :, 1] = 1  # horizontal  line \\n \\noutputs = tf.nn.conv2d(images, filters, strides=1, padding=\"SAME\") \\n \\nplt.imshow(outputs[0, :, :, 1], cmap=\"gray\") # plot 1st image\\'s 2nd feature map \\nplt.show() \\nLet’s go through this code:  \\n\\uf0b7 The pixel intensity for each color channel is represented as a byte from 0 to 255, so we \\nscale these features simply by dividing by 255, to get floats ranging from 0 to 1.  \\n\\uf0b7 Then we create two 7 × 7 filters (one with a vertical wh ite line in the middle, and the \\nother with a horizontal white line in the middle).  ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 6}), Document(page_content='\\uf0b7 We apply them to both images using the  tf.nn.conv2d()  function, which is part of \\nTensorFlow’s low -level Deep Learning API. In this example, we use zero padding \\n(padding=\"SAM E\") and a stride of 1.  \\n\\uf0b7 Finally, we plot one of the resulting feature maps (similar to the top -right image \\nin Figure  5). \\nThe tf.nn.conv2d()  line deserves a bit more explanation:  \\n\\uf0b7 images  is the input mini -batch (a 4D tensor, as explained earlier).  \\n\\uf0b7 filters  is the set of filters to apply (also a 4D tensor, as explained earlier).  \\n\\uf0b7 strides  is equal to  1, but it could also be a 1D array  with four elements, where the two \\ncentral elements are the vertical and horizontal strides ( sh and sw). The first and last \\nelements must currently be equal to  1. They may one day be used to specify a batch stride \\n(to skip some instances) and a channel str ide (to skip some of the previous layer’s feature \\nmaps or channels).  \\n\\uf0b7 padding  must be either  \"SAME\"  or \"VALID\" : \\n\\uf0b7 If set to  \"SAME\" , the convolutional layer uses zero padding if necessary. The \\noutput size is set to the number of input neurons divided by the st ride, rounded up. \\nFor example, if the input size is 13 and the stri de is 5 (see  Figure  7), then the \\noutput size is 3  (i.e., 13 / 5 = 2.6, rounded up to 3). Then zeros are added as \\nevenly as possible around the inputs, as needed. When  strides=1 , the layer’s \\noutputs will have the same spatial dimensions (width and height) as its inputs, \\nhence the name  same . \\n\\uf0b7 If set to  \"VALID\", the convolutional layer does  not use zero padding and may \\nignore some rows and columns at the bottom and right of the input image, \\ndepending on the stride, as shown in  Figure  7 (for simplicity , only the horizontal \\ndimension is shown here, but of course the same logic applies to the vertical \\ndimension). This means that every neuron’s receptive field lies strictly wi thin \\nvalid positions inside the input (it does not go out of bounds), hence the \\nname  valid . \\n \\nFigure  7. “SAME” or “VALID” padding (with input width 13, filter width 6, stride 5)  \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 7}), Document(page_content='In this example we manually defined the filters, but in a real CNN you woul d normally define \\nfilters as trainable variables so the neural net can learn which filters work best, as explained \\nearlier. Instead of manually creating the variables, use the  keras.layers.Conv2D  layer:  \\nconv = keras.layers.Conv2D(filters=32, kernel_size =3, strides=1, \\n                           padding=\"same\", activation =\"relu\") \\nThis code creates a  Conv2D  layer with 32 filters, each 3 × 3, using a stride of 1 (both \\nhorizontally and vertically) and  \"same\"  padding, and applying the ReLU activation function to \\nits outputs. As you can see, convolutional layers have quite a few hyperparameters: you must \\nchoose the number of filters, their height and width, the strides, and the padding type. As always, \\nyou can use cross -validation to find the right hyperparameter v alues, but this is very time -\\nconsuming. We will discuss common CNN architectures later, to give you some idea of which \\nhyperparameter values work best in practice.  \\nMemory Requirements  \\nAnother  problem with CNNs is that the convolutional layers require a hug e amount of RAM. \\nThis is especially true during training, because the reverse pass of backpropagation requires all \\nthe intermediate values computed during the forward pass.  \\nFor example, consider a convolutional layer with 5 × 5 filters, outputting 200 feat ure maps of \\nsize 150 × 100, with stride 1 and  \"same\"  padding. If the input is a 150 × 100 RGB image (three \\nchannels), then the number of parameters is (5 × 5 × 3 + 1) × 200 = 15,200 (the + 1 corresponds \\nto the bias terms), which is fairly small compared to  a fully connected layer. 7 However, each of \\nthe 200 feature maps contains 150 × 100 neurons, and each of these neurons needs to compute a \\nweighted sum of its 5 × 5 × 3 = 75 inputs: that’s a total of 225 million float multiplications. Not \\nas bad as a fully connected layer, but still quite computationally intensive. Moreover, if the \\nfeature maps are represented using 32 -bit floats , then the convolutional layer’s output will \\noccupy 200 × 150 × 100 × 32 = 96 million bits (12 MB) of RAM. 8 And that’s just f or one \\ninstance —if a training batch contains 100 instances, then this layer will use up 1.2 GB of RAM!  \\nDuring inference (i.e., when making a prediction for a new instance) the RAM occupied by one \\nlayer can be released as soon as the next layer has been com puted, so you only need as much \\nRAM as required by two consecutive layers. But during training everything computed during the \\nforward pass needs to be preserved for the reverse pass, so the amount of RAM needed is (at \\nleast) the total amount of RAM require d by all layers.  \\nTIP  \\nIf training crashes because of an out -of-memory error, you can try reducing the mini -batch size. \\nAlternatively, you can try reducing dimensionality using a stride, or removing a few layers. Or you can \\ntry using 16 -bit floats instead of  32-bit floats. Or you could distribute the CNN across multiple devices.  \\nNow let’s look at the second common building block of CNNs: the  pooling  layer . ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 8}), Document(page_content='Pooling Layers  \\nOnce  you understand how convolutional layers work, the pooling layers are quite easy to g rasp. \\nTheir  goal is to  subsample  (i.e., shrink) the input image in order to reduce the computational \\nload, the memory usage, and the number of parameters  (thereby limiting the risk of overfitting).  \\nJust like in convolutional layers, each neuron in a poolin g layer is connected to the outputs of a \\nlimited number of neurons in the previous layer, located within a small rectangular receptive \\nfield. You must define its size, the stride, and the padding type, just like before. However, a \\npooling neuron has no weights; all it does is aggregate the inputs using an aggregation function \\nsuch as the max or mean.  Figure  8 shows  a max pooling  layer , which is the most common type \\nof pooling layer. In this example, we use a 2 × 2  pooling  kernel , with a stride o f 2 and no \\npadding. Only the max input value in each receptive field makes it to the next layer, while the \\nother inputs are dropped. For example, in the lower -left receptive field in  Figure  8, the input \\nvalues are 1, 5, 3, 2, so only the max value, 5, is propagated to the next layer. Because of the \\nstride of 2, the output image has half the height and half the width of the input image (rounded \\ndown since we use no padding).  \\n \\nFigure  8. Max pooling layer (2 × 2 pooling kernel, stride 2, no padding)  \\nNOTE  \\nA pooling layer typically works on every input channel independently, so the output depth is the same as \\nthe input depth.  \\nOther  than reducing computations, memory usage, and the number of parameters, a max pooling \\nlayer also introduces some level of  invariance  to small translations, as shown in  Figure  9. Here \\nwe assume that the bright pixels have a lower value than dark pixels, and we consider three \\nimages (A, B, C) going through a max pooling layer with a 2 × 2 kernel and stride 2. Im ages B \\nand C are the same as image A, but shifted by one and two pixels to the right. As you can see, \\nthe outputs of the max pooling layer for images A and B are identical. This is what translation \\ninvariance means. For image C, the output is different: it  is shifted one pixel to the right (but \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 9}), Document(page_content='there is still 50% invariance). By inserting a max pooling layer every few layers in a CNN, it is \\npossible to get some level of translation invariance at a larger scale. Moreover, max pooling \\noffers a small amount of  rotational invariance and a slight scale invariance. Such invariance \\n(even if it is limited) can be useful in cases where the prediction should not depend on these \\ndetails, such as in classification tasks.  \\n \\nFigure  9. Invariance to small translations  \\nHowever, max pooling has some downsides too. Firstly, it is obviously very destructive: even \\nwith a tiny 2 × 2 kernel and a stride of 2, the output will be two times smaller in both directions \\n(so its area will be four times smaller), simply dropping 75% of  the input values. And in some \\napplications, invariance is not desirable. Take  semantic segmentation (the task of classifying \\neach pixel in an image according to the object that pixel belongs to, which we’ll explore later in \\nthis section ): obviously, if th e input image is translated by one pixel to the right, the output \\nshould also be translated by one pixel to the right. The goal in this case is  equivariance , not \\ninvariance: a small change to the inputs should lead to a corresponding small change in the \\noutput. \\nTensorFlow Implementation  \\nImplementing  a max pooling layer in TensorFlow is quite easy. The following code creates a \\nmax pooling layer using a 2 × 2 kernel. The strides default to the kernel size, so this layer will \\nuse a stride of 2 (both horizontal ly and vertically). By default, it uses  \"valid\"  padding (i.e., no \\npadding at all):  \\nmax_pool = keras.layers.MaxPool2D (pool_size =2) \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 10}), Document(page_content='To create an  average  pooling  layer , just use  AvgPool2D  instead of  MaxPool2D . As you might \\nexpect, it works exactly like a max pooling layer, except it computes the mean rather than the \\nmax. Average pooling layers used to be very popular, but people mostly use max pooling layers \\nnow, as they generally perform better. This may seem surprising, since computing the mean \\ngenerally los es less information than computing the max. But on the other hand, max pooling \\npreserves only the strongest features, getting rid of all the meaningless ones, so the next layers \\nget a cleaner signal to work with. Moreover, max pooling offers stronger trans lation invariance \\nthan average pooling, and it requires slightly less compute.  \\nNote that max pooling and average pooling can be performed along the depth dimension rather \\nthan the spatial dimensions, although this is not as common. This can allow the CNN t o learn to \\nbe invariant to various features. For example, it could learn multiple filters, each detecting a \\ndifferent rotation of the same pattern (such as hand -written digits; see  Figure  10), and the \\ndepthwise max pooling layer would ensure that the output is the same regardless of the rotation. \\nThe CNN could similarly learn to be invariant to anything else: thickne ss, brightness, skew, \\ncolor, and so on.  \\n \\nFigure  10. Depthwise max pooling can help the CNN learn any invariance  \\nKeras does not include a depthwise max pooling layer, but TensorFlow’s low -level Deep \\nLearning API does: just use the  tf.nn.max_pool()  funct ion, and specify the kernel size and \\nstrides as 4 -tuples (i.e., tuples of size 4). The first three values of each should be 1: this indicates \\nthat the kernel size and stride along the batch, height, and width dimensions should be 1. The last \\nvalue should b e whatever kernel size and stride you want along the depth dimension —for \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 11}), Document(page_content='example, 3 (this must be a divisor of the input depth; it will not work if the previous layer \\noutputs 20 feature maps, since 20 is not a multiple of 3):  \\noutput = tf.nn.max_pool (images, \\n                        ksize=(1, 1, 1, 3), \\n                        strides=(1, 1, 1, 3), \\n                        padding=\"VALID\") \\nIf you want to include this as a layer in your Keras models, wrap it in a  Lambda  layer (or create a \\ncustom Keras layer):  \\ndepth_pool = keras.layers.Lambda( \\n    lambda X: tf.nn.max_pool (X, ksize=(1, 1, 1, 3), strides=(1, 1, 1, 3), \\n                             padding=\"VALID\")) \\nOne last type of pooling layer that you will often see in modern architectures is the  global  \\naverage  pooling  layer . It works very differently: all it does is compute the mean of each entire \\nfeature map (it’s like an average pooling layer using a pooling kernel with the same spatial \\ndimensions as the inputs). This means that it just outputs a single number p er feature map and \\nper instance. Although this is of course extremely destructive (most of the information in the \\nfeature map is lost), it can be useful as the output layer, as we will see later in this section . To \\ncreate such a layer, simply use the  keras.layers.GlobalAvgPool2D  class:  \\nglobal_avg_pool  = keras.layers.GlobalAvgPool2D () \\nIt’s equivalent to this simple  Lambda  layer, which computes the mean over the spatial \\ndimensions (height and width):  \\nglobal_avg_pool  = keras.layers.Lambda(lambda X: tf.reduce_m ean(X, axis=[1, 2])) \\nNow you know all the building blocks to create convolutional neural networks. Let’s see how to \\nassemble them.  \\nCNN Architectures  \\nTypical CNN  architectures stack a few convolutional layers (each one generally followed by a \\nReLU layer), t hen a pooling layer, then another few convolutional layers (+ReLU), then another \\npooling layer, and so on. The image gets smaller and smaller as it progresses through the \\nnetwork, but it also typically gets deeper and deeper (i.e., with more feature maps),  thanks to the \\nconvolutional layers ( see Figure  11). At the top of the stack, a regular feedforward neural \\nnetwork is added, composed of a few fully connected layers (+ReLUs), and the final layer \\noutputs the prediction (e.g., a softmax layer that outputs estimated class probabilities).  ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 12}), Document(page_content=' \\nFigure  11. Typical CNN architecture  \\nTIP  \\nA common mistake is to use convolution kernels that are too large. For example, instead of using a \\nconvolutional layer with a 5 × 5 kernel, stack two layers with 3 × 3 kernels: it will use fewer parameters \\nand require fewer computations, and it will usually perform better. One exception is for the first \\nconvolutional layer: it can typically have a large kernel (e.g., 5 × 5), usually with a stride of 2 or more: \\nthis will reduce the spatial dimension of the image without losing too much information, and since the \\ninput image only has three channel s in general, it will not be too costly.  \\nHere is how you can implement a simple CNN to tackle the Fashion MNIST dataset : \\nmodel = keras.models.Sequential ([ \\n    keras.layers.Conv2D(64, 7, activation =\"relu\", padding=\"same\", \\n                        input_shape =[28, 28, 1]), \\n    keras.layers.MaxPooling2D (2), \\n    keras.layers.Conv2D(128, 3, activation =\"relu\", padding=\"same\"), \\n    keras.layers.Conv2D(128, 3, activation =\"relu\", padding=\"same\"), \\n    keras.layers.MaxPooling2D (2), \\n    keras.layers.Conv2D(256, 3, activation =\"relu\", padding=\"same\"), \\n    keras.layers.Conv2D(256, 3, activation =\"relu\", padding=\"same\"), \\n    keras.layers.MaxPooling2D (2), \\n    keras.layers.Flatten(), \\n    keras.layers.Dense(128, activation =\"relu\"), \\n    keras.layers.Dropout(0.5), \\n    keras.layers.Dense(64, activation =\"relu\"), \\n    keras.layers.Dropout(0.5), \\n    keras.layers.Dense(10, activation =\"softmax\") \\n]) \\nLet’s go through this model:  \\n\\uf0b7 The first layer uses 64 fairly large filters (7 × 7) but only stride 1 because the input \\nimages are not very large. It also sets  input_shape=[28,  28, 1], because the images are \\n28 × 28 pixels, with a single color chan nel (i.e., grayscale).  \\n\\uf0b7 Next we have a max pooling layer which uses a pool size of 2, so it divides each spatial \\ndimension by a factor of 2.  \\n\\uf0b7 Then we repeat the same structure twice: two convolutional layers followed by a max \\npooling layer. For larger images , we could repeat this structure several more times (the \\nnumber of repetitions is a hyperparameter you can tune).  \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 13}), Document(page_content='\\uf0b7 Note that the number of filters grows as we climb up the CNN toward the output layer (it \\nis initially 64, then 128, then 256): it makes sense for it to grow, since the number of low -\\nlevel features is often fairly low (e.g., small circles, horizontal lines), but there are many \\ndifferent ways to combine them into higher -level features. It is a common practice to \\ndouble the number of filters after each pooling layer: since a pooling layer divides each \\nspatial dimension by a factor of 2, we can afford to double the number of feature maps in \\nthe next layer without fear of exploding the number of parameters, memory usage, or \\ncomputational load.  \\n\\uf0b7 Next is  the fully connected network, composed of two hidden dense layers and a dense \\noutput layer. Note that we must flatten its inputs, since a dense network expects a 1D \\narray of features for each instance. We also add two dropout layers, with a dropout rate of  \\n50% each, to reduce overfitting.  \\nThis CNN reaches over 92% accuracy on the test set. It’s not state of the art, but it is pretty good, \\nand clearly much better than what we achieved with dense networks in previous section . \\nOver the years, variants of this fundamental architecture have been developed, leading to \\namazing advances in the field. A good measure of this progress is the error rate  in competitions \\nsuch as the ILSVRC  ImageNet  challenge . In this competition the top -five error rate for image \\nclassification fell from over 26% to less than 2.3% in just six years. The top -five error rate is the \\nnumber  of test images for which the system’s top five predictions did not include the correct \\nanswer. The images are large (256 pixels high) and there are 1,000 classes, some of which are \\nreally subtle (try distinguishing 120 dog breeds). Looking at the evolutio n of the winning entries \\nis a good way to understand how CNNs work.  \\nWe will first look at the classical LeNet -5 architecture (1998), then three of the winners of the \\nILSVRC challenge: AlexNet (2012), GoogLeNet (2014), and ResNet (2015).  \\nLeNet -5 \\nThe LeNet -5 architecture  is perhaps the most widely known CNN architecture. As mentioned \\nearlier, it was  created by Yann LeCun in 1998 and has been widely used for handwritten digit \\nrecognition (MNIST). It is composed of the layers shown in  Table  1. \\nLayer  Type  Maps  Size Kernel size  Stride  Activation  \\nOut Fully connected  – 10 – – RBF  \\nF6 Fully connected  – 84 – – tanh \\nC5 Convolution  120 1 × 1  5 × 5  1 tanh ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 14}), Document(page_content='Layer  Type  Maps  Size Kernel size  Stride  Activation  \\nS4 Avg pooling  16 5 × 5  2 × 2  2 tanh \\nC3 Convolution  16 10 × 10  5 × 5  1 tanh \\nS2 Avg pooling  6 14 × 14  2 × 2  2 tanh \\nC1 Convolution  6 28 × 28  5 × 5  1 tanh \\nIn Input  1 32 × 32  – – – \\nTable  1. LeNet -5 architecture  \\nThere are a few extra details to be noted:  \\n\\uf0b7 MNIST images are 28 × 28 pixels, but they are zero -padded to 32 × 3 2 pixels and \\nnormalized before being fed to the network. The rest of the network does not use any \\npadding, which is why the size keeps shrinking as the image progresses through the \\nnetwork.  \\n\\uf0b7 The average pooling layers are slightly more complex than usual: e ach neuron computes \\nthe mean of its inputs, then multiplies the result by a learnable coefficient (one per map) \\nand adds a learnable bias term (again, one per map), then finally applies the activation \\nfunction.  \\n\\uf0b7 Most neurons in C3 maps are connected to neur ons in only three or four S2 maps (instead \\nof all six S2 maps). See table  1 (page 8) in the original paper  for details.  \\n\\uf0b7 The output lay er is a bit special: instead of computing the matrix multiplication of the \\ninputs and the weight vector, each neuron outputs the square of the Euclidian distance \\nbetween its input vector and its weight vector. Each output measures how much the \\nimage belong s to a particular digit class. The cross -entropy cost function is now \\npreferred, as it penalizes bad predictions much more, producing larger gradients and \\nconverging faster.  \\nYann LeCun’s  website  features great demos of LeNet -5 classifying digits.  \\nAlexNet  \\nThe AlexNet  CNN  architecture  won the 2012 ImageNet ILSVRC challenge by a large margin: it \\nachieved a top -five error rate of 17%, while the second best achieved only 26%! It was \\ndeveloped by Alex Krizhevsky (hence the name), Ilya Sutskever, and Geoffrey Hinton. It is \\nsimilar to LeNet -5, only much larger and deeper, and it was the first to stack convolutional layers ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 15}), Document(page_content='directly on top of one another, instead of stacking a pooling layer on top of each convolutional \\nlayer.  Table  2 presents this architecture.  \\nLaye\\nr Type  Maps  Size Kerne\\nl size  Strid\\ne Paddin\\ng Activatio\\nn \\nOut Fully \\nconnected  – 1,00\\n0 – – – Softmax  \\nF10 Fully \\nconnected  – 4,09\\n6 – – – ReLU  \\nF9 Fully  \\nconnected  – 4,09\\n6 – – – ReLU  \\nS8 Max \\npooling  256 6 × 6  3 × 3  2 valid  – \\nC7 Convolutio\\nn 256 13 × \\n13 3 × 3  1 same  ReLU  \\nC6 Convolutio\\nn 384 13 × \\n13 3 × 3  1 same  ReLU  \\nC5 Convolutio\\nn 384 13 × \\n13 3 × 3  1 same  ReLU  \\nS4 Max \\npooling  256 13 × \\n13 3 × 3  2 valid  – \\nC3 Convolutio\\nn 256 27 × \\n27 5 × 5  1 same  ReLU  \\nS2 Max \\npooling  96 27 × \\n27 3 × 3  2 valid  – \\nC1 Convolutio\\nn 96 55 × \\n55 11 × \\n11 4 valid  ReLU  ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 16}), Document(page_content='Laye\\nr Type  Maps  Size Kerne\\nl size  Strid\\ne Paddin\\ng Activatio\\nn \\nIn Input  3 \\n(RGB\\n) 227 \\n× \\n227 – – – – \\nTable  2. AlexNet architecture  \\nTo reduce overfitting, the authors used two regula rization techniques . First, they applied dropout  \\nwith a 50% dropout rate during training to the outputs  of layers F9 and F10. Second, they \\nperformed  data augmentation  by randomly shifting the training images by various offsets, \\nflipping them horizontally, and changing the lighting conditions.  \\nDATA AUGMENTATION  \\nData augmentation artificially increases the si ze of the training set by generating many realistic \\nvariants of each training instance. This reduces overfitting, making this a regularization \\ntechnique. The generated instances should be as realistic as possible: ideally, given an image \\nfrom the augmented  training set, a human should not be able to tell whether it was augmented or \\nnot. Simply adding white noise will not help; the modifications should be learnable (white noise \\nis not).  \\nFor example, you can slightly shift, rotate, and resize every picture in  the training set by various \\namounts and add the resulting pictures to the training set (see  Figure  12). This forc es the model \\nto be more tolerant to variations in the position, orientation, and size of the objects in the \\npictures. For a model that’s more tolerant of different lighting conditions, you can similarly \\ngenerate many images with various contrasts. In gener al, you can also flip the pictures \\nhorizontally (except for text, and other asymmetrical objects). By combining these \\ntransformations, you can greatly increase the size of your training set.  \\n \\nFIGURE  12. GENERATING NEW TRAIN ING INSTANCES FROM E XISTING O NES  \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 17}), Document(page_content='AlexNet  also uses a competitive normalization step immediately after the ReLU step of layers \\nC1 and C3, called  local  response  normalization  (LRN): the most strongly activated neurons \\ninhibit other neurons located at the same position in neighboring fea ture maps (such competitive \\nactivation has been observed in biological neurons). This encourages different feature maps to \\nspecialize, pushing them apart and forcing them to explore a wider range of features, ultimately \\nimproving generalization.  Equation  2 shows  how to apply LRN.  \\nEquation  2. Local response normalization (LRN)  \\n \\nIn this equation:  \\n\\uf0b7 bi is the normalized output of the neuron located in feature map  i, at some row  u and \\ncolumn  v (note that in this equation we consider only neurons located at this row and \\ncolumn, so  u and v are not shown).  \\n\\uf0b7 ai is the activation of that neuron after the ReLU step, but before normalization.  \\n\\uf0b7 k, α, β, and  r are hyperparameters.  k is called the  bias, and  r is called the  depth  radius . \\n\\uf0b7 fn is the number of feature maps.  \\nFor example, if  r = 2 and a neuron has a strong activation, it will inhibit the activation of the \\nneurons located in the feature maps immediately above and below its own.  \\nIn AlexNet, the hyperparameters are set as follows:  r = 5, α = 0.0001,  β = 0.75, and  k = 2. This \\nstep can be implemented using the  tf.nn.local_response_normalization()  function \\n(which you can wrap in a  Lambda  layer if you want to use it in a Keras model).  \\nA variant of AlexNet called  ZF Net was developed by Matthew Zeiler and Rob Fergus and won \\nthe 2013 ILSVRC challenge. It is essentially AlexNet with a few tweaked hyperparameters \\n(numbe r of feature maps, kernel size, stride, etc.).  \\nGoogLeNet  \\nThe GoogLeNet  architecture  was developed by Christian Szegedy et al. from Google \\nResearch,  and it won the ILSVRC 2014 challenge by pushing the top -five error rate below 7%. \\nThis great performance came in large part from the fact that the network was much deeper than \\nprevious CNNs (as you’ll see in  Figure  14). This was made possible by subnetworks \\ncalled  inception  modules , which allow GoogLeNet to use parameters much more efficiently than \\nprevious architectures: GoogLeNet actually has 10 times fewer parameters than AlexNet \\n(roughly 6 million instead of 60 million).  \\nFigure  13 shows the architecture of an inception module. The notation “3 ×  3 + 1(S)” means that \\nthe layer uses a 3 × 3 kernel, stride 1, and  \"same\"  padding. The input signal is first copied and \\nfed to four different layers. All convolutional layers use the ReLU activation function. Note that \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 18}), Document(page_content='the second set of convolutional layer s uses different kernel sizes (1 × 1, 3 × 3, and 5 × 5), \\nallowing them to capture patterns at different scales. Also note that every single layer uses a \\nstride of 1 and  \"same\"  padding (even the max pooling layer), so their outputs all have the same \\nheight and width as their inputs. This makes it possible to concatenate all the outputs along the \\ndepth dimension  in the final  depth  concatenation  layer  (i.e., stack the feature maps from all four \\ntop convolutional layers). This concatenation layer can be impleme nted in TensorFlow using \\nthe tf.concat()  operation, with  axis=3  (the axis is the depth).  \\n \\nFigure  13. Inception module  \\nYou may wonder why inception modules have convolutional layers with 1 × 1 kernels. Surely \\nthese layers cannot capture any features bec ause they look at only one pixel at a time? In fact, the \\nlayers serve three purposes:  \\n\\uf0b7 Although they cannot capture spatial patterns, they can capture patterns along the depth \\ndimension.  \\n\\uf0b7 They  are configured to output fewer feature maps than their inputs, so  they serve \\nas bottleneck  layers , meaning they reduce dimensionality. This cuts the computational \\ncost and the number of parameters, speeding up training and improving generalization.  \\n\\uf0b7 Each pair of convolutional layers ([1 × 1, 3 × 3] and [1 × 1, 5 × 5]) ac ts like a single \\npowerful convolutional layer, capable of capturing more complex patterns. Indeed, \\ninstead of sweeping a simple linear classifier across the image (as a single convolutional \\nlayer does), this pair of convolutional layers sweeps a two -layer neural network across \\nthe image.  \\nIn short, you can think of the whole inception module as a convolutional layer on steroids, able \\nto output feature maps that capture complex patterns at various scales.  \\nWARNING  \\nThe number of convolutional kernels for each c onvolutional layer is a hyperparameter. Unfortunately, this \\nmeans that you have six more hyperparameters to tweak for every inception layer you add.  \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 19}), Document(page_content='Now let’s look at the architecture of the GoogLeNet CNN ( see Figure  14). The number of \\nfeature maps output by each convolutional layer and each pooling layer is shown before the \\nkernel size. The architecture is so deep that it ha s to be represented in three columns, but \\nGoogLeNet is actually one tall stack, including nine inception modules (the boxes with the \\nspinning tops). The six numbers in the inception modules represent the number of feature maps \\noutput by each convolutional layer in the module (in the same order as in  Figure  13). Note that \\nall the convolutional layers use the ReLU activa tion function.  \\n \\nFigure  14. GoogLeNet architecture  \\nLet’s go through this network:  \\n\\uf0b7 The first two layers divide the image’s height and width by 4 (so its area is divided by \\n16), to reduce the computational load. The first layer uses a large kernel size so  that much \\nof the information is preserved.  \\n\\uf0b7 Then the local response normalization layer ensures that the previous layers learn a wide \\nvariety of features (as discussed earlier).  \\n\\uf0b7 Two convolutional layers follow, where the first acts like a bottleneck layer.  As \\nexplained earlier, you can think of this pair as a single smarter convolutional layer.  \\n\\uf0b7 Again, a local response normalization layer ensures that the previous layers capture a \\nwide variety of patterns.  \\n\\uf0b7 Next, a max pooling layer reduces the image height a nd width by 2, again to speed up \\ncomputations.  \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 20}), Document(page_content='\\uf0b7 Then comes the tall stack of nine inception modules, interleaved with a couple max \\npooling layers to reduce dimensionality and speed up the net.  \\n\\uf0b7 Next, the global average pooling layer outputs the mean of each feature map: this drops \\nany remaining spatial information, which is fine because there was not much spatial \\ninformation left at that point. Indeed, GoogLeNet input images are typically expected to \\nbe 224 × 224 pixels, so after 5 max pooling layers, each di viding the height and width by \\n2, the feature maps are down to 7 × 7. Moreover, it is a classification task, not \\nlocalization, so it does not matter where the object is. Thanks to the dimensionality \\nreduction brought by this layer, there is no need to have  several fully connected layers at \\nthe top of the CNN (like in AlexNet), and this considerably reduces the number of \\nparameters in the network and limits the risk of overfitting.  \\n\\uf0b7 The last layers are self -explanatory: dropout for regularization, then a full y connected \\nlayer with 1,000 units (since there are 1,000 classes) and a softmax activation function  to \\noutput estimated class probabilities.  \\nThis diagram is slightly simplified: the original GoogLeNet architecture also included two \\nauxiliary classifiers p lugged on top of the third and sixth inception modules. They were both \\ncomposed of one average pooling layer, one convolutional layer, two fully connected layers, and \\na softmax activation layer. During training, their loss (scaled down by 70%) was added to  the \\noverall loss. The goal was to fight the vanishing gradients problem and regularize the network. \\nHowever, it was later shown that their effect was relatively minor.  \\nSeveral variants of the GoogLeNet architecture were later proposed by Google researcher s, \\nincluding Inception -v3 and Inception -v4, using slightly different inception modules and reaching \\neven better performance.  \\nVGGNet  \\nThe runner -up in the ILSVRC 2014 challenge was  VGGNet , developed by Karen Simonyan and \\nAndrew Zisserman from the Visual Geometry Group (VGG) research lab at Oxford University. \\nIt had a very simple and classical  architecture, with 2 or 3 convolutional layers and a pooling \\nlayer, then again 2 or 3 convolutional layers and a pooling layer, and so on (reaching a total of \\njust 16 or 19 convolutional layers, depending on the VGG variant), plus a final dense network \\nwith 2 hidden layers and the output layer. It used only 3 × 3 filters, but many filters.  \\nResNet  \\nKaiming He et al. won  the ILSVRC 2015 challenge using a  Residual  Network  (or ResNet ), that \\ndelivered an astounding top -five error rate under 3.6%. The winning variant used an extremely \\ndeep CNN composed of 152 layers (other variants had 34, 50, and  101 layers). It confirmed the \\ngeneral trend: models are getting deeper and deeper, with fewer and fewer parameters. The key \\nto being able to train such a deep network is to  use skip connections  (also called  shortcut  \\nconnections ): the signal feeding into a  layer is also added to the output of a layer located a bit \\nhigher up the stack. Let’s see why this is useful.  ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 21}), Document(page_content='When training a neural network, the goal is to make it model a target function  h(x). If you add \\nthe input  x to the output of the network (i.e., y ou add a skip connection), then the network will be \\nforced to model  f(x) = h(x) – x rather than  h(x). This  is called  residual  learning  (see Figure  15). \\n \\nFigure  15. Residual learning  \\nWhen you initialize a regular neural network, its weights are close to zero, so the network just \\noutputs values close to zero. If you add a skip connection, the resulting network just outputs a \\ncopy of its inputs; in other words, it initially models the identity function. If the target function is \\nfairly close to the identity function (which is often the case), this will speed up training \\nconsiderably.  \\nMoreover, if you add many skip con nections, the network can start making progress even if \\nseveral layers have not started learning yet (see  Figure  16). Thanks to skip connections, the \\nsignal can easily make its way across the whole network. The deep residual network can be seen \\nas a stack of  residual  units  (RUs), where each residual unit is a small neural network with a skip \\nconnection.  \\n \\nFigure  16. Regular deep neural network (left) and deep residual network (right)  \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 22}), Document(page_content='Now let’s look at ResNet’s architecture ( see Figure  17). It is surprisingly simple. It starts and \\nends exactly like GoogLeNet (except without a dropout layer), and in between is just a very deep \\nstack of simple residual units. Each residual unit is composed of two convolutional layers (and \\nno pooling laye r!), with Batch Normalization (BN) and ReLU activation, using 3 × 3 kernels and \\npreserving spatial dimensions (stride 1,  \"same\"  padding).  \\n \\nFigure  17. ResNet architecture  \\nNote that the number of feature maps is doubled every few residual units, at the s ame time as \\ntheir height and width are halved (using a convolutional layer with stride 2). When this happens, \\nthe inputs cannot be added directly to the outputs of the residual unit because they don’t have the \\nsame shape (for example, this problem affects the skip connection represented by the dashed \\narrow in  Figure  17). To solve this problem, the inputs are passed through a 1 ×  1 convolutional \\nlayer with stride 2 and the right number of output feature maps (see  Figure  18). \\n \\nFigure  18. Skip connection when changing feature map size and depth  \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 23}), Document(page_content='ResNet -34 is the ResNet with 34 layers (only counting the convolutional layers and the fully \\nconnected layer)  containing 3 residual units that output 64 feature maps, 4 RUs with 128 maps, 6 \\nRUs with 256 maps, and 3 RUs with 512 maps. We will implement this architecture later in this \\nsection . \\nResNets deeper than that , such as ResNet -152, use slightly different residual units. Instead of \\ntwo 3 × 3 convolutional layers with, say, 256 feature maps, they use three convolutional layers: \\nfirst a 1 × 1 convolutional layer with just 64 feature maps (4 times less), which acts as a \\nbottleneck layer (as discussed already), then a 3 × 3 layer with 64 feature maps, and finally \\nanother 1 × 1 convolutional layer with 256 feature maps (4 times 64) that restores the original \\ndepth. ResNet -152 contains 3 such RUs that output 256 maps, t hen 8 RUs with 512 maps, a \\nwhopping 36 RUs with 1,024 maps, and finally 3 RUs with 2,048 maps.  \\nNOTE  \\nGoogle’s  Inception -v4 architecture merged the ideas of GoogLeNet and ResNet and achieved a top -five \\nerror rate of close to 3% on ImageNet classification.  \\nXception  \\nAnother  variant of the GoogLeNet architecture is worth noting:  Xception  (which stands \\nfor Extreme  Inception ) was proposed in 2016 by François Chollet (the auth or of Keras), and it \\nsignificantly outperformed Inception -v3 on a huge vision task (350 million images and 17,000 \\nclasses). Just like Inception -v4, it merges the ideas of GoogLeNet and ResNet, but it replaces \\nthe inception modules with a special type of la yer called a  depthwise  separable  convolution  \\nlayer  (or separable  convolution  layer  for short ). These layers had been used b efore in some CNN \\narchitectures, but they were not as central as in the Xception architecture. While a regular \\nconvolutional layer uses filters that try to simultaneously capture spatial patterns (e.g., an oval) \\nand cross -channel patterns (e.g., mouth + no se + eyes = face), a separable convolutional layer \\nmakes the strong assumption that spatial patterns and cross -channel patterns can be modeled \\nseparately (see  Figure  19). Thus, it is composed of two parts: the first part applies a single spatial \\nfilter for each input feature map, then the second part looks exclusively for cross -channel \\npatterns —it is just a regula r convolutional layer with 1 × 1 filters.  \\n \\nFigure  19. Depthwise separable convolutional layer  \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 24}), Document(page_content='Since separable convolutional layers only have one spatial filter per input channel, you should \\navoid using them after layers that have too few channels, such  as the input layer (granted, that’s \\nwhat  Figure  19 represents, but it is just for illustration purposes). For  this reason, the Xception \\narchitecture starts with 2 regular convolutional layers, but then the rest of the architecture uses \\nonly separable convolutions (34 in all), plus a few max pooling layers and the usual final layers \\n(a global average pooling layer  and a dense output layer).  \\nYou might wonder why Xception is considered a variant of GoogLeNet, since it contains no \\ninception module at all. Well, as we discussed earlier, an inception module contains \\nconvolutional layers with 1 × 1 filters: these look ex clusively for cross -channel patterns. \\nHowever, the convolutional layers that sit on top of them are regular convolutional layers that \\nlook both for spatial and cross -channel patterns. So you can think of an inception module as an \\nintermediate between a reg ular convolutional layer (which considers spatial patterns and cross -\\nchannel patterns jointly) and a separable convolutional layer (which considers them separately). \\nIn practice, it seems that separable convolutional layers generally perform better.  \\nTIP  \\nSeparable convolutional layers use fewer parameters, less memory, and fewer computations than regular \\nconvolutional layers, and in general they even perform better, so you should consider using them by \\ndefault (except after layers with few channels).  \\nThe ILS VRC 2016 challenge was won by the CUImage team from the Chinese University of \\nHong Kong. They used an ensemble of many different techniques, including a sophisticated \\nobject -detection system called  GBD -Net, to achieve a top -five error rate below 3%. Although \\nthis result is unquestionably impressive, the complexity of the solution contra sted with the \\nsimplicity of ResNets. Moreover, one year later another fairly simple architecture performed \\neven better, as we will see now.  \\nSENet  \\nThe winning architecture in the ILSVRC 2017 challenge was the  Squeeze -and-Excitation  \\nNetwork  (SENet) . This architecture extends existing architectures such as inception networks \\nand ResNets, an d boosts their performance. This allowed SENet to win the competition with an \\nastonishing 2.25% top -five error rate! The extended versions of inception networks  and ResNets \\nare called  SE-Inception  and SE-ResNet , respectively. The boost comes from the fact that a SENet \\nadds a small neural network, called an  SE block , to every unit in the original architecture (i.e., \\nevery inception module or every residual unit), as shown in  Figure  20. ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 25}), Document(page_content=' \\nFigure  20. SE-Inception module (left) and SE -ResNet unit (right)  \\nAn SE block analyzes the output of the unit it is attached to, focusing exclusively on the depth \\ndimension (it does not look for any spatial pattern), and it learns which features are usually most \\nactive together. It then uses this information to recalibrate the feature maps, as shown \\nin Figure  21. For example, an SE block may learn that mouths, noses, and eyes usually appear \\ntogether in pictures: if you see a mouth and a nose, you should expect to see eyes as well. So if \\nthe block sees a strong ac tivation in the mouth and nose feature maps, but only mild activation in \\nthe eye feature map, it will boost the eye feature map (more accurately, it will reduce irrelevant \\nfeature maps). If the eyes were somewhat confused with something else, this feature map \\nrecalibration will help resolve the ambiguity.  \\n \\nFigure  21. An SE block performs feature map recalibration  \\nAn SE block is composed of just three layers: a global average pooling layer, a hidden dense \\nlayer using the ReLU activation function, and a d ense output layer using the sigmoid activation \\nfunction (see  Figure  22). \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 26}), Document(page_content=' \\nFigure  22. SE block architecture  \\nAs earlier, t he global average pooling layer computes the mean activation for each feature map: \\nfor example, if its input contains 256 feature maps, it will output 256  numbers  representing the \\noverall level of response for each filter. The next layer is where the “sque eze” happens: this layer \\nhas significantly fewer than 256 neurons —typically 16 times fewer than the number of feature \\nmaps (e.g., 16 neurons) —so the 256 numbers get compressed into a small vector (e.g., 16 \\ndimensions). This is a low -dimensional vector repr esentation (i.e., an embedding) of the \\ndistribution of feature responses. This bottleneck step forces the SE block to learn a general \\nrepresentation of the feature combinations (we will see this principle in action again when we \\ndiscuss autoencoders in later  section ). Finally, the output layer takes the embedding and outputs \\na recalibration vector containing one number per featu re map (e.g., 256), each between 0 and 1. \\nThe feature maps are then multiplied by this recalibration vector, so irrelevant features (with a \\nlow recalibration score) get scaled down while relevant features (with a recalibration score close \\nto 1) are left al one. \\nImplementing a ResNet -34 CNN Using Keras  \\nMost  CNN architectures described so far are fairly straightforward to implement (although \\ngenerally you would load a pretrained network instead, as we will see). To illustrate the process, \\nlet’s implement a Res Net-34 from scratch using Keras. First, let’s create \\na ResidualUnit  layer:  \\nclass ResidualUnit (keras.layers.Layer): \\n    def __init__ (self, filters, strides=1, activation =\"relu\", **kwargs): \\n        super().__init__ (**kwargs) \\n        self.activation  = keras.activations .get(activation ) \\n        self.main_layers  = [ \\n            keras.layers.Conv2D(filters, 3, strides=strides, \\n                                padding=\"same\", use_bias =False), \\n            keras.layers.BatchNormalization (), \\n            self.activation , \\n            keras.layers.Conv2D(filters, 3, strides=1, \\n                                padding=\"same\", use_bias =False), \\n            keras.layers.BatchNormalization ()] \\n        self.skip_layers  = [] \\n        if strides > 1: \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 27}), Document(page_content='            self.skip_layers  = [ \\n                keras.layers.Conv2D(filters, 1, strides=strides, \\n                                    padding=\"same\", use_bias =False), \\n                keras.layers.BatchNormalization ()] \\n \\n    def call(self, inputs): \\n        Z = inputs \\n        for layer in self.main_layers : \\n            Z = layer(Z) \\n        skip_Z = inputs \\n        for layer in self.skip_layers : \\n            skip_Z = layer(skip_Z) \\n        return self.activation (Z + skip_Z) \\nAs you can see, this code matches  Figure  18 pretty  closely. In the constructor, we create all the \\nlayers we will need: the main layers are the ones on the right side of the diagram, a nd the skip \\nlayers are the ones on the left (only needed if the stride is greater than 1). Then in \\nthe call()  method, we make the inputs go through the main layers and the skip layers (if any), \\nthen we add both outputs and apply the activation function.  \\nNext, we can build the ResNet -34 using a  Sequential  model, since it’s really just a long \\nsequence of layers (we can treat each residual unit as a single layer now that we have \\nthe ResidualUnit  class):  \\nmodel = keras.models.Sequential () \\nmodel.add(keras.layers.Conv2D(64, 7, strides=2, input_shape =[224, 224, 3], \\n                              padding=\"same\", use_bias =False)) \\nmodel.add(keras.layers.BatchNormalization ()) \\nmodel.add(keras.layers.Activation (\"relu\")) \\nmodel.add(keras.layers.MaxPool2D (pool_size =3, strides=2, padding=\"same\")) \\nprev_filters  = 64 \\nfor filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3: \\n    strides = 1 if filters == prev_filters  else 2 \\n    model.add(ResidualUnit (filters, strides=strides)) \\n    prev_filters  = filters \\nmodel.add(keras.layers.GlobalAvgPool2D ()) \\nmodel.add(keras.layers.Flatten()) \\nmodel.add(keras.layers.Dense(10, activation =\"softmax\" )) \\nThe only slightly tricky part in this code is the loop that adds the  ResidualUnit  layers to the \\nmodel: as explained earlier, the first 3 RUs have 64 filters, then the next 4 RUs have 128 filters, \\nand so on. We then set the stride to 1 when the number of filters is the same as in the previous \\nRU, or else we set it to 2. Then we add the  ResidualUnit , and finally we \\nupdate  prev_filters . \\nIt is amazing that  in fewer than 40 lines of code, we can build the model that won the ILSVRC \\n2015 challenge! This demonstrates both the elegance of the ResNet model and the \\nexpressiveness of the Keras API. Implementing the other CNN architectures is not much harder. \\nHoweve r, Keras comes with several of these architectures built in, so why not use them instead?  ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 28}), Document(page_content='Using Pretrained Models from Keras  \\nIn general, you won’t have to implement standard models like GoogLeNet or ResNet manually, \\nsince pretrained networks are readily av ailable with a single line of code in \\nthe keras.applications  package. For example, you can load the ResNet -50 model, \\npretrained on ImageNet, with the following line of code:  \\nmodel = keras.applications .resnet50 .ResNet50 (weights=\"imagenet\" ) \\nThat’s all! This will create a ResNet -50 model and download weights pretrained on the \\nImageNet dataset. To use it, you first need to ensure that the images have the right size. A \\nResNet -50 model expects 224 × 224 -pixel images (other models may expect other sizes, such as \\n299 × 299), so let’s use TensorFlow’s  tf.image.resize()  function to resize the images we \\nloaded earlier:  \\nimages_resized  = tf.image.resize(images, [224, 224]) \\nTIP  \\nThe tf.image.resize()  will not preserve the aspect ratio. If this is a problem, try cropping th e images \\nto the appropriate aspect ratio before resizing. Both operations can be done in one shot \\nwith tf.image.crop_and_resize() . \\nThe pretrained models assume that the images are preprocessed in a specific way. In some cases \\nthey may expect the inputs to be scaled from 0 to 1, or –1 to 1, and so on. Each model provides \\na preprocess_input()  function that you can use to preprocess your images. These functions \\nassume that the pixel values range from 0 to 255, so we must multiply them by 255 (since earlier \\nwe scaled them to the 0 –1 range):  \\ninputs = keras.applications .resnet50 .preprocess_input (images_resized  * 255) \\nNow we can use the pretrained model to make predictions:  \\nY_proba = model.predict(inputs) \\nAs usual, the output  Y_proba  is a matrix with one row per im age and one column per class (in \\nthis case, there are 1,000 classes). If you want to display the top  K predictions, including the \\nclass name and the estimated probability of each predicted class, use \\nthe decode_predictions()  function. For each image, it re turns an array containing the \\ntop K predictions, where each prediction is represented as an array containing the class \\nidentifier,  its name, and the corresponding confidence score:  \\ntop_K = keras.applications .resnet50 .decode_predictions (Y_proba, top=3) \\nfor image_index  in range(len(images)): \\n    print(\"Image #{}\".format(image_index )) \\n    for class_id , name, y_proba in top_K[image_index]: \\n        print(\"  {} - {:12s} {:.2f}%\" .format(class_id , name, y_proba * 100)) ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 29}), Document(page_content='    print() \\nThe output looks like this:  \\nImage #0  \\n  n03877845 - palace       42.87%  \\n  n02825657 - bell_cote    40.57%  \\n  n03781244 - monastery    14.56%  \\n \\nImage #1  \\n  n04522168 - vase         46.83%  \\n  n07930864 - cup          7.78%  \\n  n11939491 - daisy        4.87%  \\nThe correct classes (monastery and daisy) appear in the top three results for both images. That’s \\npretty good, considering that the model had to choose from among 1 ,000 classes.  \\nAs you can see, it is very easy to create a pretty good image classifier using a pretrained model. \\nOther vision models are available in  keras.applications , including several ResNet variants, \\nGoogLeNet variants like Inception -v3 and Xception, VGGNet variants, and MobileNet and \\nMobileNetV2 (lightweight models for use in mobile applications).  \\nBut what if you want to use an image classifier for classes of images that are not part of \\nImageNet? In that case, you may still benefit from the pretrained  models to perform transfer \\nlearning.  \\nPretrained Models for Transfer Learning  \\nIf you want to build an image classifier but you do not have enough training data, then it is often \\na good idea to reuse the low er layers of a pretrained model . For example, let’s train a model to \\nclassify pictures of flowers, reusing a pretrained Xception model. First, let’s load the dat aset \\nusing TensorFlow Datasets : \\nimport tensorflow_datasets  as tfds \\n \\ndataset, info = tfds.load(\"tf_flowers\", as_supervised =True, with_info =True) \\ndataset_size  = info.splits[\"train\"].num_examples  # 3670 \\nclass_names  = info.features [\"label\"].names # [\"dandelion\",  \"daisy\",  ...] \\nn_classes  = info.features [\"label\"].num_classes  # 5 \\nNote that you can get information  about the dataset by setting  with_info=True . Here, we get \\nthe dataset size and the names of the classes. Unfortunately, there is only a  \"train\"  dataset, no \\ntest set or validation set, so we need to split the training set. The TF Datasets project provides an \\nAPI for this. For example, let’s take the first 10% of the dataset for testing, the next 15% for \\nvalidation, and the remaining 75% for  training : \\ntest_set_raw , valid_set_raw , train_set_raw  = tfds.load( \\n    \"tf_flowers\" , ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 30}), Document(page_content='    split=[\"train[:10%]\" , \"train[1 0%:25%]\" , \"train[25%:]\" ], \\n    as_supervised =True) \\nNext we must preprocess the images. The CNN expects 224 × 224 images, so we need to resize \\nthem. We also need to run the images through Xception’s  preprocess_input()  function:  \\ndef preprocess (image, label): \\n    resized_image  = tf.image.resize(image, [224, 224]) \\n    final_image  = keras.applications .xception .preprocess_input (resized_image ) \\n    return final_image , label \\nLet’s apply this preprocessing function to all three datasets, shuffle the training set, and add \\nbatching and prefetching to all the datasets:  \\nbatch_size  = 32 \\ntrain_set  = train_set .shuffle(1000) \\ntrain_set  = train_set .map(preprocess ).batch(batch_size ).prefetch (1) \\nvalid_set  = valid_set .map(preprocess ).batch(batch_size ).prefetch (1) \\ntest_set = test_set.map(preprocess ).batch(batch_size ).prefetch (1) \\nIf you want to perform some data augmentation, change the preprocessing function for the \\ntraining set, adding some random transformations to the training images. For example, \\nuse tf.image.random_crop()  to ran domly crop the images, \\nuse tf.image.random_flip_left_right()  to randomly flip the images horizontally, and \\nso on (see the “Pretrained Models for Transfer Learning” section of the notebook for an \\nexample).  \\nTIP  \\nThe keras.preprocessing.image.ImageDataGenerato r class makes it easy to load images from \\ndisk and augment them in various ways: you can shift each image, rotate it, rescale it, flip it horizontally \\nor vertically, shear it, or apply any transformation function you want to it. This is very convenient for  \\nsimple projects. However, building a tf.data pipeline has many advantages: it can read the images \\nefficiently (e.g., in parallel) from any source, not just the local disk; you can manipulate the  Dataset  as \\nyou wish; and if you write a preprocessing functi on based on  tf.image  operations, this function can be \\nused both in the tf.data pipeline and in the model you will deplo y to production . \\nNext let’s load an Xception model, pretrained on ImageNet. We exclude the top of the network \\nby setting  include_top=False : this excludes the global average pooling layer and the dense \\noutput layer. We then add our own global av erage pooling layer, based on the output of the base \\nmodel, followed by a dense output layer with one unit per class, using the  softmax activation \\nfunction. Finally, we create the Keras  Model : \\nbase_model  = keras.applications .xception .Xception (weights=\"imagenet\", \\n                                                  include_top =False) \\navg = keras.layers.GlobalAveragePooling2D ()(base_model .output) \\noutput = keras.layers.Dense(n_classes , activation =\"softmax\" )(avg) \\nmodel = keras.Model(inputs=base_model .input, outputs=output) ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 31}), Document(page_content='As explained before , it’s usually a good idea to freeze the weights of the pretrained layers, at \\nleast at the beg inning of training:  \\nfor layer in base_model .layers: \\n    layer.trainable  = False \\nNOTE  \\nSince our model uses the base model’s layers directly, rather than the  base_model  object itself, \\nsetting  base_model.trainable=False  would have no effect.  \\nFinally, we can c ompile the model and start training:  \\noptimizer  = keras.optimizers .SGD(lr=0.2, momentum =0.9, decay=0.01) \\nmodel.compile(loss=\"sparse_categorical_crossentropy\" , optimizer =optimizer , \\n              metrics=[\"accuracy\" ]) \\nhistory = model.fit(train_set , epochs=5, validation_data =valid_set ) \\nWARNING  \\nThis will be very slow, unless you have a GPU. If you do not, then you should run this code in Colab, \\nusing a GPU runtime (it’s free!).  \\nAfter training the model for a few epochs, its validation accuracy should reach about 75 –80% \\nand stop making much progress. This means that the top layers are now pretty well trained, so \\nwe are ready to unfreez e all the layers (or you could try unfreezing just the top ones) and \\ncontinue training (don’t forget to compile the model when you freeze or unfreeze layers). This \\ntime we use a much lower learning rate to avoid damaging the pretrained weights:  \\nfor layer in base_model .layers: \\n    layer.trainable  = True \\n \\noptimizer  = keras.optimizers .SGD(lr=0.01, momentum =0.9, decay=0.001) \\nmodel.compile(...) \\nhistory = model.fit(...) \\nIt will take a while, but this model should reach around 95% accuracy on the test set. With th at, \\nyou can start training amazing image classifiers! But there’s more to computer vision than just \\nclassification. For example, what if you also want to know  where  the flower is in the picture? \\nLet’s look at this now.  \\nClassification and Localization  \\nLocal izing  an object in a picture can be expressed as a regression task, as discussed  before : to \\npredict a bounding box around th e object, a common approach is to predict the horizontal and \\nvertical coordinates of the object’s center, as well as its height and width. This means we have \\nfour numbers to predict. It does not require much change to the model; we just need to add a ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 32}), Document(page_content='secon d dense output layer with four units (typically on top of the global average pooling layer), \\nand it can be trained using the MSE loss:  \\nbase_model  = keras.applications .xception .Xception (weights=\"imagenet\" , \\n                                                  include_top =False) \\navg = keras.layers.GlobalAveragePooling2D ()(base_model .output) \\nclass_output  = keras.layers.Dense(n_classes , activation =\"softmax\" )(avg) \\nloc_output  = keras.layers.Dense(4)(avg) \\nmodel = keras.Model(inputs=base_model .input, \\n                    outputs=[class_output , loc_output ]) \\nmodel.compile(loss=[\"sparse_categorical_crossentropy\" , \"mse\"], \\n              loss_weights =[0.8, 0.2], # depends on what you care most about \\n              optimizer =optimizer , metrics=[\"accuracy\" ]) \\nBut now we have a pro blem: the flowers dataset does not have bounding boxes around the \\nflowers. So, we need to add them ourselves. This is often one of the hardest and most costly parts \\nof a Machine Learning project: getting the labels. It’s a good idea to spend time looking f or the \\nright tools. To annotate images with bounding boxes, you may want to use an open source image \\nlabeling tool like VGG Image Annotator, LabelImg, OpenLabeler, or ImgLab, or perhaps a \\ncommercial tool like LabelBox or Supervisely. You may also want to c onsider crowdsourcing \\nplatforms such as Amazon Mechanical Turk if you have a very large number of images to \\nannotate. However, it is quite a lot of work to set up a crowdsourcing platform, prepare the form \\nto be sent to the workers, supervise them, and ens ure that the quality of the bounding boxes they \\nproduce is good, so make sure it is worth the effort. If there are just a few thousand images to \\nlabel, and you don’t plan to do this frequently, it may be preferable to do it yourself. Adriana \\nKovashka et al . wrote a very practical  paper  about crowdsourcing in computer vision. I \\nrecommend you check it out, even if you do not plan to use crowdsourcing.  \\nLet’s suppose you’ve obtained the bounding boxes for every image in the flowers dataset (for \\nnow we will assume there is a single bounding box per image). You then need to create a dataset \\nwhose items will be batches of preprocessed images along with their class labels and their \\nbounding boxes. Each item should be a tuple of the form  (images,  (class_labels,  \\nbounding_boxes)) . Then you are ready to train your model!  \\nTIP  \\nThe bounding boxes should be normalized so that the horizontal and vertical coordinates, as well as the \\nheight and width, all range from 0 to 1. Also, it is common to predict the square root of the height and \\nwidth rather than the height and width directly: this way, a 10 -pixel err or for a large bounding box will \\nnot be penalized as much as a 10 -pixel error for a small bounding box.  \\nThe MSE often works fairly well as a cost function to train the model, but it is not a great metric \\nto evaluate how well the model can predict bounding boxes. The most common metric for this is \\nthe Intersection  over Union  (IoU): the area of overlap between the predicted bounding box and \\nthe target bounding box, divided by the area of their union (see  Figure  23). In tf.keras, it is \\nimplemented by the  tf.keras.metrics.MeanIoU  class.  ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 33}), Document(page_content=' \\nFigure  23. Intersection over Union (IoU) metric for bounding boxes  \\nClassifying and localizing a single object is nice, but what if the images contain multiple objects \\n(as is often the case in the flowers dataset)?  \\nObject Detection  \\nThe task of classifying and localizing multiple objects in an image is called  object  detection . \\nUntil a few years ago, a common approach was to take a CNN that was trained to classify and \\nlocate a single object, then slide it across the image, as shown in  Figure  24. In this example, the \\nimage was chopped into a 6 × 8 grid, and we show a CNN (the thick black rectangle) sliding \\nacross all 3 × 3 regions. When the CNN was looking at the top left of the image, it detected part \\nof the leftmost rose , and then it detected that same rose again when it was first shifted one step to \\nthe right. At the next step, it started detecting part of the topmost rose, and then it detected it \\nagain once it was shifted one more step to the right. You would then conti nue to slide the CNN \\nthrough the whole image, looking at all 3 × 3 regions. Moreover, since objects can have varying \\nsizes, you would also slide the CNN across regions of different sizes. For example, once you are \\ndone with the 3 × 3 regions, you might wan t to slide the CNN across all 4 × 4 regions as well.  \\n \\nFigure  24. Detecting multiple objects by sliding a CNN across the image  \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 34}), Document(page_content='This technique is fairly straightforward, but as you can see it will detect the same object multiple \\ntimes, at slightly differ ent positions. Some post -processing will then be needed to get rid of all \\nthe unnecessary bounding boxes. A  common approach for this is called  non-max suppression . \\nHere’s how you do it:  \\n1. First, you  need to add an extra  objectness  output to your CNN, to esti mate the probability \\nthat a flower is indeed present in the image (alternatively, you could add a “no -flower” \\nclass, but this usually does not work as well). It must use the sigmoid activation function, \\nand you can train it using binary cross -entropy loss.  Then get rid of all the bounding \\nboxes for which the objectness score is below some threshold: this will drop all the \\nbounding boxes that don’t actually contain a flower.  \\n2. Find the bounding box with the highest objectness score, and get rid of all the othe r \\nbounding boxes that overlap a lot with it (e.g., with an IoU greater than 60%). For \\nexamp le, in  Figure  24, the boundin g box with the max objectness score is the thick \\nbounding box over the topmost rose (the objectness score is represented by the thickness \\nof the bounding boxes). The other bounding box over that same rose overlaps a lot with \\nthe max bounding box, so we wil l get rid of it.  \\n3. Repeat step two until there are no more bounding boxes to get rid of.  \\nThis simple approach to object detection works pretty well, but it requires running the CNN \\nmany times, so it is quite slow. Fortunately, there is a much faster way to s lide a CNN across an \\nimage: using a  fully convolutional  network  (FCN).  \\nFully Convolutional Networks  \\nThe idea  of FCNs was first introduced in a  2015  paper  by Jonathan Long et al., for semantic \\nsegmentation (the task of classifying every pixel in an image according to the class of the object \\nit belongs to). The authors pointed out that you c ould replace the dense layers at the top of a \\nCNN by convolutional layers. To understand this, let’s look at an example: suppose a dense layer \\nwith 200 neurons sits on top of a convolutional layer that outputs 100 feature maps, each of size \\n7 × 7 (this is the feature map size, not the kernel size). Each neuron will compute a weighted sum \\nof all 100 × 7 × 7 activations from the convolutional layer (plus a bias term). Now let’s see what \\nhappens if we replace the dense layer with a convolutional layer using 20 0 filters, each of size 7 \\n× 7, and with  \"valid\"  padding. This layer will output 200 feature maps, each 1 × 1 (since the \\nkernel is exactly the size of the input feature maps and we are using  \"valid\"  padding). In other \\nwords, it will output 200 numbers, just  like the dense layer did; and if you look closely at the \\ncomputations performed by a convolutional layer, you will notice that these numbers will be \\nprecisely the same as those the dense layer produced. The only difference is that the dense \\nlayer’s output  was a tensor of shape [ batch  size, 200], while the convolutional layer will output a \\ntensor of shape [ batch  size, 1, 1, 200].  \\nTIP  \\nTo convert a dense layer to a convolutional layer, the number of filters in the convolutional layer must be \\nequal to the numb er of units in the dense layer, the filter size must be equal to the size of the input feature \\nmaps, and you must use  \"valid\"  padding. The stride may be set to 1 or more, as we will see shortly.  ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 35}), Document(page_content='Why is this important? Well, while a dense layer expects a sp ecific input size (since it has one \\nweight per input feature), a convolutional layer will happily process images of any \\nsize (however, it does expect its inputs to have a specific number of channels, since each kernel \\ncontains a different set of weights for each input channel). Since an FCN contains only \\nconvolutional layers (and pooling layers, which have the same property), it can be trained and \\nexecuted on images of any size!  \\nFor example, suppose we’d already trained a CNN for flower classification and localization. It \\nwas trained on 224 × 224 images, and it outputs 10 numbers: outputs 0 to 4 are sent through \\nthe softmax act ivation function, and this gives the class probabilities (one per class); output 5 is \\nsent through the logistic activation function, and this gives the objectness score; outputs 6 to 9 do \\nnot use any activation function, and they represent the bounding box ’s center coordinates, as well \\nas its height and width. We can now convert its dense layers to convolutional layers. In fact, we \\ndon’t even need to retrain it; we can just copy the weights from the dense layers to the \\nconvolutional layers! Alternatively, w e could have converted the CNN into an FCN before \\ntraining.  \\nNow suppose the last convolutional layer before the output layer (also called the bottleneck \\nlayer) outputs 7 × 7 feature maps when the network is fed a 224 × 224 image (see the left side \\nof Figure  25). If we feed the FCN a 448 × 448 image (see the right side of  Figure  25), the \\nbottleneck layer will now output 14 × 14 feature maps.  Since the dense output layer was replaced \\nby a convolutional layer using 10 filters of size 7 × 7, with  \"valid\"  padding and stride 1, the \\noutput will be composed of 10 features maps, each of size 8 × 8 (since 14 – 7 + 1 = 8). In other \\nwords,  the FCN will process the whole image only once, and it will output an 8 × 8 grid where \\neach cell contains 10 numbers (5 class probabilities, 1 objectness score, and 4 bounding box \\ncoordinates). It’s exactly like taking the original CNN and sliding it acro ss the image using 8 \\nsteps per row and 8 steps per column. To visualize this, imagine chopping the original image into \\na 14 × 14 grid, then sliding a 7 × 7 window across this grid; there will be 8 × 8 = 64 possible \\nlocations for the window, hence 8 × 8 pre dictions. However, the FCN approach is  much  more \\nefficient, since the network only looks at the image once. In fact,  You Only  Look  Once  (YOLO) \\nis the name of a very popular object detection architecture, which we’ll look at next.  \\n \\nFigure  14-25. The same f ully convolutional network processing a small image (left) and a large one (right)  \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 36}), Document(page_content='You Only Look Once (YOLO)  \\nYOLO  is an extremely fast and accurate object detection architecture proposed by Joseph \\nRedmon et al. in a  2015 paper  and subsequently improved  in 2016  (YOLOv2) and  in \\n2018  (YOLOv3). It is so fast that it can run in real time on a video, as seen in Redmon’s  demo . \\nYOLOv3’s architecture is quite similar to the one we just discussed, but with a few impor tant \\ndifferences:  \\n\\uf0b7 It outputs five bounding boxes for each grid cell (instead of just one), and each bounding \\nbox comes with an objectness score. It also outputs 20 class probabilities per grid cell, as \\nit was trained on the PASCAL VOC dataset, which contai ns 20 classes. That’s a total of \\n45 numbers per grid cell: 5 bounding boxes, each with 4 coordinates, plus 5 objectness \\nscores, plus 20 class probabilities.  \\n\\uf0b7 Instead of predicting the absolute coordinates of the bounding box centers, YOLOv3 \\npredicts an offs et relative to the coordinates of the grid cell, where (0, 0) means the top \\nleft of that cell and (1, 1) means the bottom right. For each grid cell, YOLOv3 is trained \\nto predict only bounding boxes whose center lies in that cell (but the bounding box itsel f \\ngenerally extends well beyond the grid cell). YOLOv3 applies the logistic activation \\nfunction to the bounding box coordinates to ensure they remain in the 0 to 1 range.  \\n\\uf0b7 Before training the neural net, YOLOv3 finds five representative bounding \\nbox dimensi ons, called  anchor  boxes  (or bounding  box priors ). It does this by ap plying \\nthe K -Means algorithm  to the height and width of the training set bounding boxes. For \\nexample, if the training images contain many pedestrians, then one of the anchor boxes \\nwill likely have the dimensions of a typical pedestrian. Then when the neural net predicts \\nfive bounding  boxes per grid cell, it actually predicts how much to rescale each of the \\nanchor boxes. For example, suppose one anchor box is 100 pixels tall and 50 pixels wide, \\nand the network predicts, say, a vertical rescaling factor of 1.5 and a horizontal rescaling  \\nof 0.9 (for one of the grid cells). This will result in a predicted bounding box of size 150 \\n× 45 pixels. To be more precise, for each grid cell and each anchor box, the network \\npredicts the log of the vertical and horizontal rescaling factors. Having the se priors makes \\nthe network more likely to predict bounding boxes of the appropriate dimensions, and it \\nalso speeds up training because it will more quickly learn what reasonable bounding \\nboxes look like.  \\n\\uf0b7 The network is trained using images of different sc ales: every few batches during \\ntraining, the network randomly chooses a new image dimension (from 330 × 330 to 608 \\n× 608 pixels). This allows the network to learn to detect objects at different scales. \\nMoreover, it makes it possible to use YOLOv3 at differ ent scales: the smaller scale will \\nbe less accurate but faster than the larger scale, so you can choose the right trade -off for \\nyour use case.  \\nThere are a few more innovations you might be interested in, such as the use of skip connections \\nto recover some of the spatial resolution that is lost in the CNN (we will discuss this shortly, \\nwhen we look at semantic segmentation). In the 2016 paper, the authors introduce the \\nYOLO9000 model that uses hierarchical classification: the  model predicts a probability for  each \\nnode in a visual hierarchy called  WordTree . This makes it possible for the network to predict ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 37}), Document(page_content='with high confidence that an image represents, say, a dog, even though it is unsure what specific \\ntype of dog. I encourage you to go ahead and read all thre e papers: they are quite pleasant to \\nread, and they provide excellent examples of how Deep Learning systems can be incrementally \\nimproved.  \\nMEAN AVERAGE PRECISI ON (MAP)  \\nA very common metric used in object detection tasks is the  mean  Average  Precision  (mAP).  \\n“Mean Average” sounds a bit redundant, doesn’t it? To understand this metric, let’s go back to \\ntwo classification metrics : precision and recall. Remember the trade -off: the higher the recall, the \\nlower the precision. You can visualize th is in a precision/recall curve . To summarize this curve \\ninto a single number, we could compute its area under the curve (AUC). But note that the \\nprecision/recall curve may contain a few sections where precision ac tually goes up when recall \\nincreases, e specially at low recall values . This is one of the motivations for the mAP metric.  \\nSuppose the classifier has 90% precision at 10% recall, but 96% precision at 20% recall. There’s \\nreally no trade -off here: it simply makes more sense to use the classifier at 20% recall rather than \\nat 10 % recall, as you will get both higher recall and higher precision. So instead of looking at the \\nprecision  at 10% recall, we should really be looking at the  maximum  precision that the classifier \\ncan offer with  at least  10% recall. It would be 96%, not 90%. Therefore, one way to get a fair \\nidea of the model’s performance is to compute the maximum precision you can get with at least \\n0% recall, then 10% recall, 20%, and so on up to 100%, and then calculate the mean of these \\nmaximum precisions. This  is called th e Average  Precision  (AP) metric. Now when there are \\nmore than two classes, we can compute the AP for each class, and then compute the mean AP \\n(mAP). That’s it!  \\nIn an object detection system, there is an additional level of complexity: what if the system \\ndetected the correct class, but at the wrong location (i.e., the bounding box is completely off)? \\nSurely we should not count this as a positive prediction. One approach is to define an IOU \\nthreshold: for example, we may consider that a prediction is correct only if the IOU is greater \\nthan, say, 0.5, and the predicted class is correct. The corresponding mAP is generally noted \\nmAP@0.5 (or mAP@50%, or sometimes just AP 50). In some competitions (such as the PASCAL \\nVOC challenge), this is what is done. In others ( such as the COCO competition), the mAP is \\ncomputed for different IOU thresholds (0.50, 0.55, 0.60, …, 0.95), and the final metric is the \\nmean of all these mAPs (noted mAP@[.50:.95] or mAP@[.50:0.05:.95]). Yes, that’s a mean \\nmean average.  \\nSeveral YOLO imple mentations built using TensorFlow are available on GitHub. In particular, \\ncheck out  Zihao  Zang’s  TensorFlow  2 implementation . Other object detection models are \\navailable in the TensorFlow Models project, many with pretrained weights; and some have even \\nbeen ported to TF Hub, such as  SSD and Faster -RCNN , which are both quite popular. SSD is \\nalso a “single shot” detection mode l, similar to YOLO. Faster R -CNN is more complex: the \\nimage first goes through a CNN, then the output  is passed to a  Region  Proposal  Network  (RPN) \\nthat proposes bounding boxes that are most likely to contain an object, and a classifier is run for \\neach boun ding box, based on the cropped output of the CNN.  ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 38}), Document(page_content='The choice of detection system depends on many factors: speed, accuracy, available pretrained \\nmodels, training time, complexity, etc. The papers contain tables of metrics, but there is quite a \\nlot of variab ility in the testing environments, and the technologies evolve so fast that it is \\ndifficult to make a fair comparison that will be useful for most people and remain valid for more \\nthan a few months.  \\nSo, we can locate objects by drawing bounding boxes aroun d them. Great! But perhaps you want \\nto be a bit more precise. Let’s see how to go down to the pixel level.  \\nSemantic Segmentation  \\nIn semantic  segmentation , each  pixel is classified according to the class of the object it belongs \\nto (e.g., road, car, pedestr ian, building, etc.), as shown in  Figure  26. Note that different objects of \\nthe same class are  not distinguish ed. For example, all the bicycles on the right side of the \\nsegmented image end up as one big lump of pixels. The main difficulty in this task is that when \\nimages go through a regular CNN, they gradually lose their spatial resolution (due to the layers \\nwith strides greater than 1); so, a regular CNN may end up knowing that there’s a person \\nsomewhere in the bottom left of the image, but it will not be much more precise than that.  \\nJust like for object detection, there are many different approaches to tackle th is problem, some \\nquite complex. However, a fairly simple solution was proposed in the 2015 paper by Jonathan \\nLong et al. we discussed earlier. The authors start by taking a pretrained CNN and turning it into \\nan FCN. The CNN applies an overall stride of 32 to the input image (i.e., if you add up all the \\nstrides greater than 1), meaning the last layer outputs feature maps that are 32 times smaller than \\nthe input image. This is clearly too coarse, so they add a single  upsampling  layer  that multiplies \\nthe resol ution by 32.  \\n \\nFigure  26. Semantic segmentation  \\nThere are  several solutions available for upsampling (increasing the size of an image), such as \\nbilinear interpolation, but that only works reasonably well up to ×4 or ×8. Instead, they  use \\na transposed  convolutional  layer : it is equivalent to first stretching the image by inserting empty \\nrows and columns (full of zeros), then  performing a regular convolution ( see Figure  27). \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 39}), Document(page_content='Alternatively, some people prefer to think of it as a regular co nvolutional layer that uses \\nfractional strides (e.g., 1/2 in  Figure  27). The transposed convolutional layer can be initialized to \\nperform something close to linear interpolation, but since it is a trainable layer, it will learn to do \\nbetter during training. In tf.keras, you can use the  Conv2DTranspose  layer.  \\n \\nFigure  27. Upsampling using a transposed convolutional l ayer \\nNOTE  \\nIn a transposed convolutional layer, the stride defines how much the input will be stretched, not the size \\nof the filter steps, so the larger the stride, the larger the output (unlike for convolutional layers or pooling \\nlayers).  \\nTENSORFLOW CONVOL UTION OPERATIONS  \\nTensorFlow  also offers a few other kinds of convolutional layers:  \\nkeras.layers.Conv1D  \\nCreates a convolutional layer for 1D inputs, such as time series or text (sequences of \\nletters or words), as we will see in next section . \\nkeras.layers.Conv3D  \\nCreates a convolutional layer for 3D inputs, such as 3D PET scans.  \\ndilation_rate  \\nSetting the  dilation_rate  hyperparameter of any  convolutional layer to a value of 2 \\nor more creates an  à-trous  convolutional  layer  (“à trous” is French for “with holes”). This \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 40}), Document(page_content='is equivalent to using a regular convolutional layer with a filter dilated by inserting rows \\nand columns of zeros (i.e., holes) . For example, a 1 × 3 filter equal to  [[1,2,3]]  may \\nbe dilated with a  dilation  rate of 4, resulting in a  dilated  filter  of [[1, 0, 0, 0, 2, \\n0, 0, 0, 3]]. This lets the convolutional layer have a larger receptive field at no \\ncomputational price and using n o extra parameters.  \\ntf.nn.depthwise_conv2d()  \\nCan be used to create a  depthwise  convolutional  layer  (but you need to create the \\nvariables yourself). It applies every filter to every individual input channel independently. \\nThus, if there are  fn filters and  fn′ input channels, then this will output  fn × fn′ feature \\nmaps.  \\nThis solution is OK, but still too imprecise. To do better, the authors added skip connections \\nfrom lower layers: for example, they upsampled the output image by a factor of 2 (instead of 32),  \\nand they added the output of a lower layer that had this double resolution. Then they upsampled \\nthe result by a factor of 16, leading to a total upsampling factor of 3 2 (see  Figure  28). This \\nrecovered some of the spatial resolution that was lost in earlier pooling layers. In their best \\narchitecture, they used a second similar skip connection to recover even finer de tails from an \\neven lower layer. In short, the output of the original CNN goes through the following extra steps: \\nupscale ×2, add the output of a lower layer (of the appropriate scale), upscale ×2, add the output \\nof an even lower layer, and finally upscale ×8. It is even possible to scale up beyond the size of \\nthe original image: this can be used to increase the resolution of an image, which is a technique \\ncalled  super -resolution . \\n \\nFigure  28. Skip layers recover some spatial resolution from lower layers  \\nOnce again, many GitHub repositories provide TensorFlow implementations of semantic \\nsegmentation  (TensorFlow  1 for now), and you will even find pretrained  instance  \\nsegmentation  models in the TensorFlow Models project. Instance segmentation is similar to \\nsemantic segmentation, but instead of merging all objects of the same class into one big lump, \\neach object is distinguished from the others (e.g., it identifies each individual bicycle). At \\npresent,  the instance segmentation models available in the TensorFlo w Models project are based \\non the  Mask  R-CNN  architecture, which was proposed in a  2017  paper : it extends the Faster R -\\nCNN model by additionally producing a pixel mask for each bounding box. So not only do you \\n', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 41}), Document(page_content='get a bounding box around each object, with a set of estimated class probabilities, but you also \\nget a pixel mask that locates  pixels in the bounding box that belong to the object.  \\nAs you can see, the field of Deep Computer Vision is vast and moving fast, with all sorts of \\narchitectures popping out every year, all based on convolutional neural networks. The  progress \\nmade in just a few years has been astounding, and researchers are now focusing on harder and \\nharder problems, such as  adversarial  learning  (which attempts to make the network more \\nresistant to images designed to fool it), explainability (understanding why the network m akes a \\nspecific classification), realistic  image  generation  (which we will come back in later section ), \\nand single -shot learning  (a system that can recognize an object after it has seen it just once). \\nSome even explore completely novel architectures, such as Geoffrey Hinton’s  capsule  \\nnetworks  (I presented them in a couple of  videos , with the corresponding code in a notebook). \\nNow on to the next section , wher e we will look at how to process sequential data such as time \\nseries using recurrent neural networks and convolutional neural networks.  \\n ', metadata={'source': 'documents\\\\Deep Computer Vision Using CNN.pdf', 'page': 42}), Document(page_content='Processing Sequences Using  RNNs  and CNNs  \\nThe batter hits the ball. The outfielder immediately starts running, anticipating the ball’s \\ntrajectory. He tracks it, adapts his movements, and finally catches it (under a thunder of \\napplause). Predicting the futur e is something you do all the time, whether you are finishing a \\nfriend’s sentence or anticipating the smell of coffee at breakfast. In this section  we will discuss \\nrecurrent neural networks (RNNs), a class of nets that can predict the future (well, up to a  point, \\nof course). They  can analyze time series data such as stock prices, and tell you when to buy or \\nsell. In  autonomous driving systems, they can anticipate car trajectories and help avoid accidents. \\nMore generally, they  can work on sequences of arbitr ary lengths, rather than on fixed -sized \\ninputs like all the nets we have considered so far. For example, they can take sentences, \\ndocuments, or audio samples as input, making  them extremely useful for natural language \\nprocessing applications such as automa tic translation or speech -to-text. \\nIn this section  we will first look at the fundamental concepts underlying RNNs and how to train \\nthem using backpropagation through time, then we will use them to forecast a time series. After \\nthat we’ll explore the two ma in difficulties that RNNs face:  \\n\\uf0b7 Unstable gradients (di scussed before ), which can be alleviated using various techniques, \\nincluding recurrent dropout and recurrent layer normalization  \\n\\uf0b7 A (very) limited short -term memory, which can be extended using LSTM and GRU cells  \\nRNNs are not the only types of neural networks capable of handling sequential data: for small \\nsequences, a regular dense network can do the trick; and for very long sequences, such as audio \\nsamples or text, convolutional neural networks can actua lly work quite well too. We will discuss \\nboth of these possibilities, and we will finish this section  by implementing a  WaveNet : this is a \\nCNN architecture capable of handling sequences of tens  of thousands of time steps. In next \\nsection , we will continue to explore RNNs and see how to use them for natural language \\nprocessing, along with more recent architectures based on attention mechanisms. Let’s get \\nstarted!  \\nRecurrent Neurons and Layers  \\nUp to now we have focused on feedforward neural networks, where the  activations flow only in \\none direction, from the i nput layer to the output layer . A recurrent neural network looks very \\nmuch like a feedforward neural network, except it also has connections pointing backward. Let’s \\nlook at the simplest possible RNN, comp osed of one neuron receiving inputs, producing an \\noutput, and sending that output back to itself, as shown in  Figure  1 (left). At each time \\nstep t (also called a  frame ), this  recurrent  neuron  receives the inputs  x(t) as well as its own output \\nfrom the previous time step,  y(t–1). Since there is no previous output at the first time step, it is \\ngenerally set to 0. We can represent  this tiny network against the time axis, as shown \\nin Figure  1 (right). This  is called  unrolling  the network  through  time (it’s the same recurrent \\nneuron represented once per time step).  ', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 0}), Document(page_content=' \\nFigure  1. A recurrent neuron (left) unrolled through time (right)  \\nYou can easily create a layer of recurrent neurons. At each time step  t, every neuron receives \\nboth the input vector  x(t) and the output vector from the previous time step  y(t–1), as shown \\nin Figure  2. Note that both the inputs and outputs are vector s now (when there was just a single \\nneuron, the output was a scalar).  \\n \\nFigu re 2. A layer of recurrent neurons (left) unrolled through time (right)  \\nEach recurrent neuron has two sets of weights: one for the inputs  x(t) and the other for the outputs \\nof the previous time step,  y(t–1). Let’s call these weight vectors  wx and wy. If we consider the \\nwhole recurrent layer instead of just one recurrent neuron, we can place all the weight vectors in \\ntwo weight matrices,  Wx and Wy. The output vector of the whole recu rrent layer can then be \\ncomputed pretty much as you might expect, as shown in  Equation  1 (b is the bias vector and  ϕ(·) \\nis the activation function (e.g., ReLU).  \\n \\n', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 1}), Document(page_content='Equation  1. Output of a recurrent layer for a single instance  \\n \\nJust as with feedforward neural networks, we can compute a recurrent layer’s output in one shot \\nfor a whole mini -batch by placing all the inputs at time step  t in an input \\nmatrix  X(t) (see Equation  2). \\nEquation  2. Outputs of a la yer of recurrent neurons for all instances in a mini -batch  \\n \\nIn this equation:  \\n\\uf0b7 Y(t) is an m × nneurons matrix containing the layer’s outputs at time step  t for each instance \\nin the mini -batch ( m is the number of instances in the mini -batch and  nneurons is the number \\nof neurons).  \\n\\uf0b7 X(t) is an m × ninputs matrix containing the inputs for all instances ( ninputs is the number of \\ninput features).  \\n\\uf0b7 Wx is an ninputs × nneurons matrix containing the connection weights for the inputs of the \\ncurrent time step.  \\n\\uf0b7 Wy is an nneurons × nneurons matrix containing the connection weights for the outputs of the \\nprevious time step.  \\n\\uf0b7 b is a vector of size  nneurons containing each neuron’s bias term.  \\n\\uf0b7 The weight matrices  Wx and Wy are often concatenated vertically into a single weight \\nmatrix  W of shape ( ninputs + nneurons) × nneurons (see the second line of  Equation  2). \\n\\uf0b7 The notation [ X(t) Y(t–1)] represents  the horizontal concatenation of the \\nmatrices  X(t) and Y(t–1). \\nNotice that  Y(t) is a function of  X(t) and Y(t–1), which is a function of  X(t–1) and Y(t–2), which is a \\nfunction of  X(t–2) and Y(t–3), and so on. This makes  Y(t) a function of all the inputs si nce time  t = 0 \\n(that is,  X(0), X(1), …, X(t)). At the first time step,  t = 0, there are no previous outputs, so they are \\ntypically assumed to be all zeros.  \\nMemory Cells  \\nSince  the output of a recurrent neuron at time step  t is a function of all the inputs f rom previous \\ntime steps, you could say it has a form of  memory . A part of a neural network that preserves \\nsome state across time steps is called a  memory  cell (or simply a  cell). A single recurrent neuron, \\nor a layer of recurrent neurons, is a very basic c ell, capable of learning only short patterns \\n(typically about 10 steps long, but this varies depending on the task). Later in this section , we \\nwill look at some more complex and powerful types of cells capable of learning longer patterns \\n(roughly 10 times longer, but again, this depends on the task).  \\n', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 2}), Document(page_content='In general a cell’s state at time step  t, denoted  h(t) (the “h” stands for “hidden”), is a function of \\nsome inputs at that time step and its state at the previous time step:  h(t) = f(h(t–1), x(t)). Its output a t \\ntime step  t, denoted  y(t), is also a function of the previous state and the current inputs. In the case \\nof the basic cells we have discussed so far, the output is simply equal to the state, but in more \\ncomplex cells this is not always the case, as shown in Figure  3. \\n \\nFigure  3. A cell’s hidden state and its output may be different  \\nInput and Output Sequences  \\nAn RNN can simul taneously take a sequence of inputs and produce a sequence of outputs (see \\nthe top -left network in  Figure  4). This type of  sequence -to-sequence  network  is useful for \\npredicting time series such as stock prices: you feed it the prices over the last  N days, and it must \\noutput the prices shifted by one day into the future (i.e., from  N – 1 days ago to tomorrow).  \\nAlternatively, you  could feed the network a sequence of inputs and ignore all outputs except for \\nthe last one (see the top -right network in  Figure 4). In other words, this  is a sequence -to-vector  \\nnetwork . For example, you could feed the network a sequence of words corresponding to a \\nmovie review, and the network would output a sentiment score (e.g., from –1 [hate] to +1 \\n[love]).  \\nConversely, you could feed the network the same input vector over and over again at each time \\nstep and let it output a sequence (see the bottom -left network of  Figure  4). This  is a vector -to-\\nsequence  network . For example, the input could be an image (or the output of a CNN), and the \\noutput could be a caption for that image.  \\nLastly, you could have a sequence -to-vector network, called  an encoder , followed by a vector -to-\\nsequence network, called a  decoder  (see the bottom -right network of  Figure  4). For example, this \\ncould be used for translating a sentence from one language to another. You would feed the \\nnetwork a sentence in one language, the encoder would convert this sentence into a single vector \\nrepresentation, and then the decoder would decode this vector into a s entence in another \\n', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 3}), Document(page_content='language. This two -step model, called  an Encoder–Decoder , works much better than trying to \\ntranslate on the fly with a single sequence -to-sequence RNN (like the one represented at the top \\nleft): the last words of a sentence can affect th e first words of the translation, so you need to wait \\nuntil you have seen the whole sentence before translating it. We will see how to implement an \\nEncoder –Decoder in next section  (as we will see, it is a bit more complex than \\nin Figure  4 suggests).  \\n \\nFigure  4. Seq-to-seq (top left), seq -to-vector (top right), vector -to-seq (bottom left), and Encoder –Decoder (bottom right) n etworks  \\nSounds promising, but how do you train a recurrent neural network?  \\nTraining RNNs  \\nTo train an RNN, the trick is to unroll it through time (like we just did) and then simply use \\nregular backpropagation ( see Figure  5). This  strategy is called  backpropagation  through  \\ntime (BPTT).  \\nJust like in regular backpropagation, there is a first forward pass through the unrolled network \\n(represented by the dashed arrows). Then the output sequence is evaluated using a cost \\nfunction  C(Y(0), Y(1), …Y(T)) (where  T is the max time step). Note that this cost function may \\nignore some outputs, as shown in  Figure  5 (for example, in a sequence -to-vector RNN, all \\noutputs are ignored except for the very last one). The gradients of that cost function are then \\npropagated backwa rd through the unrolled network (represented by the solid arrows). Finally the \\nmodel parameters are updated using the gradients computed during BPTT. Note that the \\ngradients flow backward through all the outputs used by the cost function, not just through the \\n', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 4}), Document(page_content='final output (for example, in  Figure  5 the cost function is computed using the last three outputs of \\nthe network,  Y(2), Y(3), and  Y(4), so gradients flow through these three outputs, but not \\nthrough  Y(0) and Y(1)). Moreover, since the same parameters  W and b are used at each time step, \\nbackpropagation will do the right thing and sum over all time steps.  \\n \\nFigure  5. Backprop agation through time  \\nFortunately, tf.keras takes care of all of this complexity for you —so let’s start coding!  \\nForecasting a Time Series  \\nSuppose  you are studying the number of active users per hour on your website, or the daily \\ntemperature in your city, or  your company’s financial health, measured quarterly using multiple \\nmetrics. In all these cases, the data will be a sequence of one or more values per time step. This \\nis called a  time series . In the first two examples there is a single value per time step,  so these \\nare univariate  time series , while in the financial example there are multiple values per time step \\n(e.g., the company’s revenue, debt, and so on), so  it is a  multivariate  time series . A typical task is \\nto predict future values, which  is called  forecasting . Another common task is to fill in the blanks: \\nto predict (or rather “postdict”) missing values from the past. This  is called  imputation . For \\nexam ple, Figure  6 shows 3 univariate time series, each of them 50 time steps long, and the goal \\nhere is to forecast the value at the next time step (represented by the X) for each of them.  \\n', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 5}), Document(page_content=' \\nFigure  6. Time series forecasting  \\nFor simplicity, we are using a time series generated by the  generate_time_series()  function, \\nshown here:  \\ndef generate_time_series (batch_size , n_steps): \\n    freq1, freq2, offsets1 , offsets2 = np.random.rand(4, batch_size , 1) \\n    time = np.linspace (0, 1, n_steps) \\n    series = 0.5 * np.sin((time - offsets1 ) * (freq1 * 10 + 10))  #   wave 1 \\n    series += 0.2 * np.sin((time - offsets2 ) * (freq2 * 20 + 20)) # + wave 2 \\n    series += 0.1 * (np.random.rand(batch_size , n_steps) - 0.5)   # + noise \\n    return series[..., np.newaxis].astype(np.float32) \\nThis function creates as many time series as requested (via the  batch_size  argument), each of \\nlength  n_steps , and there is just one value per time step in each series (i.e., all series are \\nunivariate). The function return s a NumPy array of shape [ batch  size, time steps , 1], where each \\nseries is the sum of two sine waves of fixed amplitudes but random frequencies and phases, plus \\na bit of noise.  \\nNOTE  \\nWhen dealing with time series (and other types of sequences such as senten ces), the input features are \\ngenerally represented as 3D arrays of shape [ batch  size, time steps , dimensionality ], \\nwhere  dimensionality  is 1 for univariate time series and more for multivariate time series.  \\nNow let’s create a training set, a validation set , and a test set using this function:  \\nn_steps = 50 \\nseries = generate_time_series (10000, n_steps + 1) \\nX_train, y_train = series[:7000, :n_steps], series[:7000, -1] \\nX_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1] \\nX_test, y_test = series[9000:, :n_steps], series[9000:, -1] \\nX_train  contains 7,000 time series (i.e., its shape is [7000, 50, 1]), while  X_valid  contains 2,000 \\n(from the 7,000th time series to the 8,999th) and  X_test  contains 1,000 (from the 9,000th to the \\n9,999th). Since we wan t to forecast a single value for each series, the targets are column vectors \\n(e.g.,  y_train  has a shape of [7000, 1]).  \\n', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 6}), Document(page_content='Baseline Metrics  \\nBefore  we start using RNNs, it is often a good idea to have a few baseline metrics, or else we \\nmay end up thinking our m odel works great when in fact it is doing worse than basic models. For \\nexample, the simplest approach is to predict the last value in each series. This is called  naive  \\nforecasting , and it is sometimes surprisingly difficult to outperform. In this case, it  gives us a \\nmean squared error of about 0.020:  \\n>>> y_pred = X_valid[:, -1] \\n>>> np.mean(keras.losses.mean_squared_error (y_valid, y_pred)) \\n0.020211367  \\nAnother simple approach is to use a fully connected network. Since it expects a flat list of \\nfeatures for ea ch input, we need to add a  Flatten  layer. Let’s just use a simple Linear Regression \\nmodel so that each prediction will be a linear combination of the values in the time series:  \\nmodel = keras.models.Sequential ([ \\n    keras.layers.Flatten(input_shape =[50, 1]), \\n    keras.layers.Dense(1) \\n]) \\nIf we compile this model using the MSE loss and the default Adam optimizer, then fit it on the \\ntraining set for 20 epochs and evaluate it on the validation set, we get an MSE of about 0.004. \\nThat’s much better than the naive approach!  \\nImplementing a Simple RNN  \\nLet’s  see if we can beat that with a simple RNN:  \\nmodel = keras.models.Sequential ([ \\n  keras.layers.SimpleRNN (1, input_shape =[None, 1]) \\n]) \\nThat’s really the simplest RNN you can build. It just contains a single layer, with  a single \\nneuron, as we saw in  Figure  1. We do not need to specify the length of the input sequences \\n(unlike in the previ ous model), since a recurrent neural network can process any number of time \\nsteps (this is why we set the first input dimension to  None). By default, the  SimpleRNN  layer uses \\nthe hyperbolic tangent activation function. It works exactly as we saw earlier: t he initial \\nstate h(init) is set to 0, and it is passed to a single recurrent neuron, along with the value of the first \\ntime step,  x(0). The neuron computes a weighted sum of these values and applies the hyperbolic \\ntangent activation function to the result,  and this gives the first output,  y0. In a simple RNN, this \\noutput is also the new state  h0. This new state is passed to the same recurrent neuron along with \\nthe next input value,  x(1), and the process is repeated until the last time step. Then the layer j ust \\noutputs the last value,  y49. All of this is performed simultaneously for every time series.  \\nNOTE  ', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 7}), Document(page_content='By default, recurrent layers in Keras only return the final output. To make them return one output per time \\nstep, you must set  return_sequences=True , as we  will see.  \\nIf you compile, fit, and evaluate this model (just like earlier, we train for 20 epochs using Adam), \\nyou will find that its MSE reaches only 0.014, so it is better than the naive approach but it does \\nnot beat a simple linear model. Note that for  each neuron, a linear model has one parameter per \\ninput and per time step, plus a bias term (in the simple linear model we used, that’s a total of 51 \\nparameters). In contrast, for each recurrent neuron in a simple RNN, there is just one parameter \\nper inpu t and per hidden state dimension (in a simple RNN, that’s just the number of recurrent \\nneurons in the layer), plus a bias term. In this simple RNN, that’s a total of just three parameters.  \\nTREND AND SEASONALIT Y \\nThere  are many other models to forecast time series, such as  weighted  moving  \\naverage  models  or autoregressive  integrated  moving  average  (ARIMA) models. Some of them \\nrequire you to first remove the trend and seasonality. For example, if you are studying the \\nnumber of active users on your website, and it is growing by 10% every month, you would have \\nto remove this trend from the time series. Once the model is trained and starts making \\npredictions, you would have to add the trend back to get the final predictions. Similarly, if you \\nare trying to predict the amount of sunscreen lotion sold every month, you will probably observe \\nstrong seasonality: since it sells well every summer, a similar pattern will be repeated every year. \\nYou would have to remove this seasonality from the time series, for example by c omputing the \\ndifference between the value at each time step and the value one year earlier (this technique  is \\ncalled  differencing ). Again, after the model is trained and makes predictions, you would have to \\nadd the seasonal pattern back to get the final pr edictions.  \\nWhen using RNNs, it is generally not necessary to do all this, but it may improve performance in \\nsome cases, since the model will not have to learn the trend or the  seasonality . \\nApparently our simple RNN was too simple to get good performance. S o let’s try to add more \\nrecurrent layers!  \\nDeep RNNs  \\nIt is quite common to stack multiple layers of cells, as shown in Figure  7. This  gives you a  deep  \\nRNN . \\n \\nFigure  7. Deep RNN (left) unrolled through time (right)  \\n', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 8}), Document(page_content='Implementing a deep RNN with tf.keras is quite simple: just stack recurrent layers. In this \\nexample, we use three  SimpleRNN  layers (but we could add any other type  of recurrent layer, such \\nas an  LSTM layer or a  GRU layer, which we will discuss shortly):  \\nmodel = keras.models.Sequential ([ \\n    keras.layers.SimpleRNN (20, return_sequences =True, input_shape =[None, 1]), \\n    keras.layers.SimpleRNN (20, return_sequences =True), \\n    keras.layers.SimpleRNN (1) \\n]) \\nWARNING  \\nMake sure to set  return_sequences=True  for all recurrent layers (except the last one, if you only care \\nabout the last output). If you don’t, they will output a 2D array (containing only the output of the last time  \\nstep) instead of a 3D array (containing outputs for all time steps), and the next recurrent layer will \\ncomplain that you are not feeding it sequences in the expected 3D format.  \\nIf you compile, fit, and evaluate this model, you will find that it reaches an  MSE of 0.003. We \\nfinally managed to beat the linear model!  \\nNote that the last layer is not ideal: it must have a single unit because we want to forecast a \\nunivariate time series, and this means we must have a single output value per time step. \\nHowever, ha ving a single unit means that the hidden state is just a single number. That’s really \\nnot much, and it’s probably not that useful; presumably, the RNN will mostly use the hidden \\nstates of the other recurrent layers to carry over all the information it need s from time step to \\ntime step, and it will not use the final layer’s hidden state very much. Moreover, since \\na SimpleRNN  layer uses the tanh activation function by default, the predicted values must lie \\nwithin the range –1 to 1. But what if you want to use  another activation function? For both these \\nreasons, it might be preferable to replace the output layer with a  Dense  layer: it would run \\nslightly faster, the accuracy would be roughly the same, and it would allow us to choose any \\noutput activation functio n we want. If you make this change, also make sure to \\nremove  return_sequences=True  from the second (now last) recurrent layer:  \\nmodel = keras.models.Sequential ([ \\n    keras.layers.SimpleRNN (20, return_sequences =True, input_shape =[None, 1]), \\n    keras.layers.SimpleRNN (20), \\n    keras.layers.Dense(1) \\n]) \\nIf you train this model, you will see that it converges faster and performs just as well. Plus, you \\ncould change the output activation function if you wanted.  \\nForecasting Several Time Steps Ahead  \\nSo far we have o nly predicted the value at the next time step, but we could just as easily have \\npredicted the value several steps ahead by changing the targets appropriately (e.g., to predict 10 \\nsteps ahead, just change the targets to be the value 10 steps ahead instead o f 1 step ahead). But \\nwhat if we want to predict the next 10 values?  ', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 9}), Document(page_content='The first option is to use the model we already trained, make it predict the next value, then add \\nthat value to the inputs (acting as if this predicted value had actually occurred), and us e the \\nmodel again to predict the following value, and so on, as in the following code:  \\nseries = generate_time_series (1, n_steps + 10) \\nX_new, Y_new = series[:, :n_steps], series[:, n_steps:] \\nX = X_new \\nfor step_ahead  in range(10): \\n    y_pred_one  = model.predict(X[:, step_ahead :])[:, np.newaxis, :] \\n    X = np.concatenate ([X, y_pred_one ], axis=1) \\n \\nY_pred = X[:, n_steps:] \\nAs you might expect, the prediction for the next step will usually be more accurate than the \\npredictions for later time steps, since the error s might accumulate (as you can see in  Figure  8). If \\nyou evaluate this approach on the validation set, you will find an M SE of about 0.029. This is \\nmuch higher than the previous models, but it’s also a much harder task, so the comparison \\ndoesn’t mean much. It’s much more meaningful to compare this performance with naive \\npredictions (just forecasting that the time series will  remain constant for 10 time steps) or with a \\nsimple linear model. The naive approach is terrible (it gives an MSE of about 0.223), but the \\nlinear model gives an MSE of about 0.0188: it’s much better than using our RNN to forecast the \\nfuture one step at a time, and also much faster to train and run. Still, if you only want to forecast \\na few time steps ahead, on more complex tasks, this approach may work well.  \\n \\nFigure  8. Forecasting 10 steps ahead, 1 step at a time  \\nThe second option is to train an RNN to  predict all 10 next values at once. We can still use a \\nsequence -to-vector model, but it will output 10 values instead of 1. However, we first need to \\nchange the targets to be vectors containing the next 10 values:  \\nseries = generate_time_series (10000, n_steps + 10) \\nX_train, Y_train = series[:7000, :n_steps], series[:7000, -10:, 0] \\n', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 10}), Document(page_content='X_valid, Y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0] \\nX_test, Y_test = series[9000:, :n_steps], series[9000:, -10:, 0] \\nNow we just need the output layer to ha ve 10 units instead of 1:  \\nmodel = keras.models.Sequential ([ \\n    keras.layers.SimpleRNN (20, return_sequences =True, input_shape =[None, 1]), \\n    keras.layers.SimpleRNN (20), \\n    keras.layers.Dense(10) \\n]) \\nAfter training this model, you can predict the next 10 v alues at once very easily:  \\nY_pred = model.predict(X_new) \\nThis model works nicely: the MSE for the next 10 time steps is about 0.008. That’s much better \\nthan the linear model. But we can still do better: indeed, instead of training the model to forecast \\nthe next 10 values only at the very last time step, we can train it to forecast the next 10 values at \\neach and every time step. In other words, we can turn this sequence -to-vector RNN into a \\nsequence -to-sequence RNN. The advantage of this technique is that th e loss will contain a term \\nfor the output of the RNN at each and every time step, not just the output at the last time step. \\nThis means there will be many more error gradients flowing through the model, and they won’t \\nhave to flow only through time; they w ill also flow from the output of each time step. This will \\nboth stabilize and speed up training.  \\nTo be clear, at time step 0 the model will output a vector containing the forecasts for time steps 1 \\nto 10, then at time step 1 the model will forecast time st eps 2 to 11, and so on. So each target \\nmust be a sequence of the same length as the input sequence, containing a 10 -dimensional vector \\nat each step. Let’s prepare these target sequences:  \\nY = np.empty((10000, n_steps, 10)) # each target is a sequence  of 10D vectors \\nfor step_ahead  in range(1, 10 + 1): \\n    Y[:, :, step_ahead  - 1] = series[:, step_ahead :step_ahead  + n_steps, 0] \\nY_train = Y[:7000] \\nY_valid = Y[7000:9000] \\nY_test = Y[9000:] \\nNOTE  \\nIt may be surprising that the targets will contain values that appear in the inputs (there is a lot of overlap \\nbetween  X_train  and Y_train ). Isn’t that cheating? Fortunately, not at all: at each time step, the model \\nonly knows about past time steps, so it cannot look ahead. It  is said to be a  causal  model.  \\nTo turn the model into a sequence -to-sequence model, we must set  return_sequences=True  in all \\nrecurrent layers (even the last one), and we must apply the output  Dense  layer at every time step. \\nKeras offers a  TimeDistributed  layer for this very purpose: it wraps any layer (e .g., \\na Dense  layer) and applies it at every time step of its input sequence. It does this efficiently, by \\nreshaping the inputs so that each time step is treated as a separate instance (i.e., it reshapes the \\ninputs from [ batch  size, time steps , input  dimens ions] to [ batch  size × time steps , input  ', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 11}), Document(page_content='dimensions ]; in this example, the number of input dimensions is 20 because the \\nprevious  SimpleRNN  layer has 20 units), then it runs the  Dense  layer, and finally it reshapes the \\noutputs back to sequences (i.e., it re shapes the outputs from [ batch  size × time steps , output  \\ndimensions ] to [ batch  size, time steps , output  dimensions ]; in this example the number of output \\ndimensions is 10, since the  Dense  layer has 10 units).  Here is the updated model:  \\nmodel = keras.models.Sequential ([ \\n    keras.layers.SimpleRNN (20, return_sequences =True, input_shape =[None, 1]), \\n    keras.layers.SimpleRNN (20, return_sequences =True), \\n    keras.layers.TimeDistributed (keras.layers.Dense(10)) \\n]) \\nThe Dense  layer actually supports sequences as inputs (and even higher -dimensional inputs): it \\nhandles them just like  TimeDistributed(Dense(…)) , meaning it is applied to the  last input \\ndimension only (independently across all time steps). Thus, we could replace the last layer with \\njust Dense(10) . For the sake of clarity, however, we will keep \\nusing  TimeDistributed(Dense(10))  because it makes it clear that the  Dense  layer is a pplied \\nindependently at each time step and that the model will output a sequence, not just a single \\nvector.  \\nAll outputs are needed during training, but only the output at the last time step is useful for \\npredictions and for evaluation. So although we will rely on the MSE over all the outputs for \\ntraining, we will use a custom metric for evaluation, to only compute the MSE over the output at \\nthe last time step:  \\ndef last_time_step_mse (Y_true, Y_pred): \\n    return keras.metrics.mean_squared_error (Y_true[:, -1], Y_pred[:, -1]) \\n \\noptimizer  = keras.optimizers .Adam(lr=0.01) \\nmodel.compile(loss=\"mse\", optimizer =optimizer , metrics=[last_time_step_mse ]) \\nWe get a validation MSE of about 0.006, which is 25% better than the previous model. You can \\ncombine this approach with  the first one: just predict the next 10 values using this RNN, then \\nconcatenate these values to the input time series and use the model again to predict the next 10 \\nvalues, and repeat the process as many times as needed. With this approach, you can genera te \\narbitrarily long sequences. It may not be very accurate for long -term predictions, but it may be \\njust fine if your goal is to generate original m usic or text, as we will see in next section.  \\nTIP  \\nWhen forecasting time series, it is often useful to have some error bars along with your predictions. For \\nthis, an efficient techniq ue is MC Dropout, introduced before : add an MC Dropout layer within each \\nmemory cell, dropping part of the inputs and hidden states. After training, to forecast a new time series, \\nuse the model many times and compute the mean and standard deviation of the predictions at each time \\nstep. \\nSimple RNNs can be quite good at forecasting time series or handling other kinds of sequences, \\nbut they do not perform as well on long time series or s equences. Let’s discuss why and see what \\nwe can do about it.  ', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 12}), Document(page_content='Handling Long Sequences  \\nTo train an RNN on long sequences, we must run it over many time steps, making the unrolled \\nRNN a very deep network. Just like any deep neural network it may suffer from t he unstabl e \\ngradients problem, discussed before : it may take forever to train, or training may be unstable. \\nMoreover, when an RNN processes a long sequence, it will gradually forget the first inputs in the \\nsequence. Let’s look at both these problems, starting with the unstable gradients problem.  \\nFighting the Unstable Gradients Problem  \\nMany  of the tricks we used in deep nets to  alleviate the unstable gradients problem can also be \\nused for RNNs: good parameter initialization, faster optimizers, dropout, and so on. However, \\nnonsaturating activation functions (e.g., ReLU) may not help as much here; in fact, they may \\nactually lead t he RNN to be even more unstable during training. Why? Well, suppose Gradient \\nDescent updates the weights in a way that increases the outputs slightly at the first time step. \\nBecause the same weights are used at every time step, the outputs at the second ti me step may \\nalso be slightly increased, and those at the third, and so on until the outputs explode —and a \\nnonsaturating activation function does not prevent that. You can reduce this risk by using a \\nsmaller learning rate, but you can also simply use a satu rating activation function like the \\nhyperbolic tangent (this explains why it is the default). In much the same way, the gradients \\nthemselves can explode. If you notice that training is unstable, you may want to monitor the size \\nof the gradients (e.g., usin g TensorBoard) and perhaps use Gradient Clipping.  \\nMoreover, Batch Normalization cannot be used as efficiently with RNNs as with deep \\nfeedforward nets. In fact, you cannot use it between time steps, only between recurrent layers. To \\nbe more precise, it is t echnically possible to add a BN layer to a memory cell (as we will see \\nshortly) so that it will be applied at each time step (both on the inputs for that time step and on \\nthe hidden state from the previous step). However, the same BN layer will be used at each time \\nstep, with the same parameters, regardless of the actual scale and offset of the inputs and hidden \\nstate. In practice, this does not yield good results, as was demonstrated by César Laurent et al. in \\na 2015  paper : the authors found that BN was slightly beneficial only when it was applied to the \\ninputs, not to the hidden states. In other words, it was slightly better than nothing when applied \\nbetween recurrent layers (i.e., vertically in  Figure  7), but not within recurrent layers (i.e., \\nhorizontally). In Keras this can be done simply by adding a  BatchNormalization  layer before \\neach recurrent layer, but don’t expect too much from it.  \\nAnother  form of normalization often works better with RNNs:  Layer  Normalization . This idea \\nwas introduced by Jimmy Lei Ba et al. in a  2016  paper : it is very similar to Batch Normalization, \\nbut instead of normalizing across the batch dimension, it normalizes across the features \\ndimension. One advantage is that it can compute the required statistics on the fly, at each time \\nstep, independently for each i nstance. This also means that it behaves the same way during \\ntraining and testing (as opposed to BN), and it does not need to use exponential  moving  averages \\nto estimate the feature statistics across all instances in the training set. Like BN, Layer \\nNormal ization learns a scale and an offset parameter for each input. In an RNN, it is typically \\nused right after the linear combination of the inputs and the hidden states.  ', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 13}), Document(page_content='Let’s use tf.keras to implement Layer Normalization within a simple memory cell. For this , we \\nneed to define a custom memory cell. It is just like a regular layer, except its  call()  method \\ntakes two arguments: the  inputs  at the current time step and the hidden  states  from the previous \\ntime step. Note that the  states  argument is a list containi ng one or more tensors. In the case of a \\nsimple RNN cell it contains a single tensor equal to the outputs of the previous time step, but \\nother cells may have multiple state tensors (e.g., an  LSTMCell  has a long -term state and a short -\\nterm state, as we will  see shortly). A cell must also have a  state_size  attribute and \\nan output_size  attribute. In a simple RNN, both are simply equal to the number of units. The \\nfollowing code implements a custom memory cell which will behave like a  SimpleRNNCell , \\nexcept it wi ll also apply Layer Normalization at each time step:  \\nclass LNSimpleRNNCell (keras.layers.Layer): \\n    def __init__ (self, units, activation =\"tanh\", **kwargs): \\n        super().__init__ (**kwargs) \\n        self.state_size  = units \\n        self.output_size  = units \\n        self.simple_rnn_cell  = keras.layers.SimpleRNNCell (units, \\n                                                          activation =None) \\n        self.layer_norm  = keras.layers.LayerNormalization () \\n        self.activation  = keras.activations .get(activati on) \\n    def call(self, inputs, states): \\n        outputs, new_states  = self.simple_rnn_cell (inputs, states) \\n        norm_outputs  = self.activation (self.layer_norm (outputs)) \\n        return norm_outputs , [norm_outputs ] \\nThe code is quite straightforward. 5 Our LNSimpleRNNCell  class inherits from \\nthe keras.layers.Layer  class, just like any custom layer. The constructor takes the numbe r of \\nunits and the desired activation function, and it sets the  state_size  and output_size  attributes, \\nthen creates a  SimpleRNNCell  with no activation function (because we want to perform Layer \\nNormalization after the linear operation but before the activa tion function). Then the constructor \\ncreates the  LayerNormalization  layer, and finally it fetches the desired activation function. \\nThe call()  method starts by applying the simple RNN cell, which computes a linear \\ncombination of the current inputs and the p revious hidden states, and it returns the result twice \\n(indeed, in a  SimpleRNNCell , the outputs are just equal to the hidden states: in other \\nwords,  new_states[0]  is equal to  outputs , so we can safely ignore  new_states  in the rest of \\nthe call()  method). Ne xt, the  call()  method applies Layer Normalization, followed by the \\nactivation function. Finally, it returns the outputs twice (once as the outputs, and once as the new \\nhidden states). To use this custom cell, all we need to do is create a  keras.layers.RNN  layer, \\npassing it a cell instance:  \\nmodel = keras.models.Sequential ([ \\n    keras.layers.RNN(LNSimpleRNNCell (20), return_sequences =True, \\n                     input_shape =[None, 1]), \\n    keras.layers.RNN(LNSimpleRNNCell (20), return_sequences =True), \\n    keras.layers.TimeDistributed (keras.layers.Dense(10)) \\n]) \\nSimilarly, you could create a custom cell to apply dropout between each time step. But there’s a \\nsimpler way: all recurrent layers (except for  keras.layers.RNN ) and all cells provided by Keras ', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 14}), Document(page_content='have a  dropout  hyperparameter and a  recurrent_dropout  hyperparameter: the former defines the \\ndropout rate to apply to the inputs (at each time step), and the latter defines the dropout rate for \\nthe hidden states (also at each time step). No need to create a custom cell to apply dropout at \\neach time step in an RNN.  \\nWith these techniques, you can alleviate the unstable gradients problem and train an RNN much \\nmore efficiently. Now let’s look at how to deal with the short -term memory problem.  \\nTackling the Short -Term Memory P roblem  \\nDue to the transformations that the data goes through when traversing an RNN, some \\ninformation is lost at each time step. After a while, the RNN’s state contains virtually no trace of \\nthe first inputs. This can be a showstopper. Imagine Dory the fis h trying to translate a long \\nsentence; by the time she’s finished reading it, she has no clue how it started. To tackle this  \\nproblem, various types of cells with long -term memory have been introduced. They have proven \\nso successful that the basic cells are not used much anymore. Let’s first look at the most popular \\nof these long -term memory cells: the LSTM cell.  \\nLSTM cells  \\nThe Long  Short -Term  Memory  (LSTM) cell  was proposed  in 1997  by Sepp Hochreiter and \\nJürgen S chmidhuber and gradually improved over the years by several researchers, such as  Alex  \\nGraves , Haşim  Sak, and Wojciech  Zaremba . If you consider the LSTM cell as a black box, it can \\nbe used very much like a basic cell, except it will perform much better; training will converge \\nfaster, and it will detect l ong-term dependencies in the data. In Keras, you can simply use \\nthe LSTM layer instead of the  SimpleRNN  layer:  \\nmodel = keras.models.Sequential ([ \\n    keras.layers.LSTM(20, return_sequences =True, input_shape =[None, 1]), \\n    keras.layers.LSTM(20, return_seque nces=True), \\n    keras.layers.TimeDistributed (keras.layers.Dense(10)) \\n]) \\nAlternatively, you could use the general -purpose  keras.layers.RNN  layer, giving it \\nan LSTMCell  as an argument:  \\nmodel = keras.models.Sequential ([ \\n    keras.layers.RNN(keras.layers.LSTMCell(20), return_sequences =True, \\n                     input_shape =[None, 1]), \\n    keras.layers.RNN(keras.layers.LSTMCell (20), return_sequences =True), \\n    keras.layers.TimeDistributed (keras.layers.Dense(10)) \\n]) \\nHowever, the  LSTM layer uses an optimized imple menta tion when running on a GPU  (we will see \\nlater), so in general it is preferable to use it (the  RNN layer is mostly useful w hen you define \\ncustom cells, as we did earlier).  ', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 15}), Document(page_content='So how does an LSTM cell work? Its architecture is shown in  Figure  9. \\nIf you don’t look at what’s inside the box, the LSTM cell looks exactly like a regular cell, except \\nthat its state is split into two vectors:  h(t) and c(t) (“c” stands for “cell”). You can think of  h(t) as \\nthe short -term state and  c(t) as the long -term state.  \\n \\nFigure  9. LSTM cell  \\nNow let’s open the box! The key idea is that the network can learn what to store in the long -term \\nstate, what to throw away, and what to read from it. As the long -term state  c(t–1) traverses the \\nnetwork from left to right, you can  see that it first goes through  a forget  gate, dropping some \\nmemories, and then it adds some new memories via the addition operation (which adds the \\nmemories that were selected by an  input  gate). The result  c(t) is sent straight out, without any \\nfurther tr ansformation. So, at each time step, some memories are dropped and some memories \\nare added. Moreover, after the addition operation, the long -term state is copied and passed \\nthrough the tanh function, and then the result is filtered by  the output  gate. This  produces the \\nshort -term state  h(t) (which is equal to the cell’s output for this time step,  y(t)). Now let’s look at \\nwhere new memories come from and how the gates work.  \\nFirst, the current input vector  x(t) and the previous short -term state  h(t–1) are fed  to four different \\nfully connected layers. They all serve a different purpose:  \\n\\uf0b7 The main layer is the one that outputs  g(t). It has the usual role of analyzing the current \\ninputs  x(t) and the previous (short -term) state  h(t–1). In a basic cell, there is not hing other \\nthan this layer, and its output goes straight out to  y(t) and h(t). In contrast, in an LSTM cell \\nthis layer’s output does not go straight out, but instead its most important parts are stored \\nin the long -term state (and the rest is  dropped ). \\n', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 16}), Document(page_content='\\uf0b7 The three other layers  are gate controllers . Since they use the logistic activation function, \\ntheir outputs range from 0 to 1. As you can see, their outputs are fed to element -wise \\nmultiplication operations, so if they output 0s they close the gate, and if the y output 1s \\nthey open it. Specifically:  \\n\\uf0b7 The forget  gate (controlled by  f(t)) controls which parts of the long -term state \\nshould be erased.  \\n\\uf0b7 The input  gate (controlled by  i(t)) controls which parts of  g(t) should be added to \\nthe long -term state.  \\n\\uf0b7 Finally, the  output  gate (controlled by  o(t)) controls which parts of the long -term \\nstate should be read and output at this time step, both to  h(t) and to  y(t). \\nIn short, an LSTM cell can learn to recognize an important input (that’s the role of the input \\ngate), store  it in the long -term state, preserve it for as long as it is needed (that’s the role of the \\nforget gate), and extract it whenever it is needed. This explains why these cells have been \\namazingly successful at capturing long -term patterns in time series, lon g texts, audio recordings, \\nand more.  \\nEquation  3 summarizes how to compute the cell’s long -term state, its short -term state, an d its \\noutput at each time step for a single instance (the equations for a whole mini -batch are very \\nsimilar).  \\nEquation  3. LSTM computations  \\n \\nIn this equation:  \\n\\uf0b7 Wxi, Wxf, Wxo, Wxg are the weight matrices of each of the four layers for their \\nconnection to the input vector  x(t). \\n\\uf0b7 Whi, Whf, Who, and  Whg are the weight matrices of each of t he four layers for their \\nconnection to the previous short -term state  h(t–1). \\n\\uf0b7 bi, bf, bo, and  bg are the bias terms for each of the four layers. Note that TensorFlow \\ninitializes  bf to a vector full of 1s instead of 0s. This prevents forgetting everything at  the \\nbeginning of training.  \\nPeephole connections  \\nIn a regular LSTM cell, the gate controllers can look only at the input  x(t) and the previous short -\\nterm state  h(t–1). It may be a good idea to give them a bit more context by letting them peek at the \\nlong-term state as well. This idea was  proposed  by Felix  Gers  and Jürgen  Schmidhuber  in \\n', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 17}), Document(page_content='2000 . They proposed an LSTM variant with  extra connections called  peephole  connections : the \\nprevious long -term state  c(t–1) is added as an input to the controllers of the forget gate and the \\ninput gate, and the current long -term state  c(t) is added as input to  the controller of the output \\ngate. This often improves performance, but not always, and there is no clear pattern for which \\ntasks are better off with or without them: you will have to try it on your task and see if it helps.  \\nIn Keras, the  LSTM layer is ba sed on the  keras.layers.LSTMCell  cell, which does not support \\npeepholes. The experimental  tf.keras.experimental.PeepholeLSTMCell  does, however, so you \\ncan create a  keras.layers.RNN  layer and pass a  PeepholeLSTMCell  to its constructor.  \\nThere are many other variants of the LSTM cell. One particularly popular variant is the GRU \\ncell, which we will look at now.  \\nGRU cells  \\nThe Gated  Recurrent  Unit (GRU) cell  (see Figure  10) was proposed by Kyunghyun Cho et al. in \\na 2014  paper  that also introduced the Encoder –Decoder network we discussed earlier.  \\n \\nFigure  10. GRU cell  \\nThe GRU cell is a simplified version of the LSTM cell, and it seems to perform just as \\nwell (which explains its growing popularity). These are the main simplifications:  \\n\\uf0b7 Both state vectors are merged into a single vector  h(t). \\n\\uf0b7 A single gate controller  z(t) controls both the  forget gate and the input gate. If the gate \\ncontroller outputs a 1, the forget gate is open (=  1) and the input gate is closed (1  – 1 = 0). \\nIf it outputs a 0, the opposite happens. In other words, whenever a memory must be \\nstored, the location where it wi ll be stored is erased first. This is actually a frequent \\nvariant to the LSTM cell in and of itself.  \\n\\uf0b7 There is no output gate; the full state vector is output at every time step. However, there \\nis a new gate controller  r(t) that controls which part of the p revious state will be shown to \\nthe main layer ( g(t)). \\n', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 18}), Document(page_content='Equation  4 summarizes how to compute the cell’s state at each time step f or a single instance.  \\nEquation  4. GRU computations  \\n \\nKeras provides a  keras.layers.GRU  layer (based on the  keras.layers.GRUCell  memory cell); \\nusing it is just a matter of replacing  SimpleRNN  or LSTM with GRU. \\nLSTM and GRU cells are one of the main reasons behind the success of RNNs. Yet while they \\ncan tackle much longer sequences than simple RNNs, they still have a fairly limited short -term \\nmemory, and they have a hard time learning  long-term patterns in sequences of 100 time steps or \\nmore, such as audio samples, long time series, or long sentences. One way to solve this is to \\nshorten the input sequences, for example using 1D convolutional layers.  \\nUsing 1D convolutional layers to pro cess sequences  \\nBefore  we saw that a 2D convolutional layer works by sliding several fairly small kernels (or \\nfilters) acros s an image, producing multiple 2D feature maps (one per kernel). Similarly, a 1D \\nconvolutional layer slides several kernels across a sequence, producing a 1D feature map per \\nkernel. Each kernel will learn to detect a single very short sequential pattern (n o longer than the \\nkernel size). If you use 10 kernels, then the layer’s output will be composed of 10 1 -dimensional \\nsequences (all of the same length), or equivalently you can view this output as a single 10 -\\ndimensional sequence. This means that you can bu ild a neural network composed of a mix of \\nrecurrent layers and 1D convolutional layers (or even 1D pooling layers). If you use a 1D \\nconvolutional layer with a stride of 1 and  \"same\"  padding, then the output sequence will have the \\nsame length as the input s equence. But if you use  \"valid\"  padding or a stride greater than 1, then \\nthe output sequence will be shorter than the input sequence, so make sure you adjust the targets \\naccordingly. For example, the following model is the same as earlier, except it starts  with a 1D \\nconvolutional layer that downsamples the input sequence by a factor of 2, using a stride of 2. The \\nkernel size is larger than the stride, so all inputs will be used to compute the layer’s output, and \\ntherefore the model can learn to preserve the  useful information, dropping only the unimportant \\ndetails. By shortening the sequences, the convolutional layer may help the  GRU layers detect \\nlonger patterns. Note that we must also crop off the first three time steps in the targets (since the \\nkernel’s s ize is 4, the first output of the convolutional layer will be based on the input time steps \\n0 to 3), and downsample the targets by a factor of 2:  \\nmodel = keras.models.Sequential ([ \\n    keras.layers.Conv1D(filters=20, kernel_size =4, strides=2, padding=\"valid\", \\n                        input_shape =[None, 1]), \\n    keras.layers.GRU(20, return_sequences =True), \\n    keras.layers.GRU(20, return_sequences =True), \\n    keras.layers.TimeDistributed (keras.layers.Dense(10)) \\n', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 19}), Document(page_content=']) \\n \\nmodel.compile(loss=\"mse\", optimizer =\"adam\", metrics=[last_time_step_mse ]) \\nhistory = model.fit(X_train, Y_train[:, 3::2], epochs=20, \\n                    validation_data =(X_valid, Y_valid[:, 3::2])) \\nIf you train and evaluate this model, you will find that it is the best model so far. The \\nconvolutional l ayer really helps. In fact, it is actually possible to use only 1D convolutional \\nlayers and drop the recurrent layers entirely!  \\nWaveNet  \\nIn a 2016  paper  Aaron van den Oord and other DeepMind researchers introduced an  architecture \\ncalled  WaveNet . They stacked 1D convolutional layers, doubling the dilation rate (how spread \\napart each neuron’s inputs are) at every layer: the first convolutional layer gets a glimpse of just \\ntwo time steps at a time, while the next one see s four time steps (its receptive field is four time \\nsteps long), the next one sees eight time steps, and so on (see  Figure  11). This way, the lower \\nlayers learn short -term patterns, while the higher layers learn long -term patterns. Thanks to the \\ndoubling dilation rate, the network can process extremely large sequences very efficiently.  \\n \\nFigure  11. WaveNet architecture  \\nIn the W aveNet paper, the authors actually stacked 10 convolutional layers with dilation rates of \\n1, 2, 4, 8, …, 256, 512, then they stacked another group of 10 identical layers (also with dilation \\nrates 1, 2, 4, 8, …, 256, 512), then again another identical group  of 10 layers. They justified this \\narchitecture by pointing out that a single stack of 10 convolutional layers with these dilation rates \\nwill act like a super -efficient convolutional layer with a kernel of size 1,024 (except way faster, \\nmore powerful, and using significantly fewer parameters), which is why they stacked 3 such \\nblocks. They also left -padded the input sequences with a number of zeros equal to the dilation \\nrate before every layer, to preserve the same sequence length throughout the network. Her e is \\nhow to implement a simplified WaveNet to tackle the same sequences as  earlier : \\nmodel = keras.models.Sequential () \\nmodel.add(keras.layers.InputLayer (input_shape =[None, 1])) \\nfor rate in (1, 2, 4, 8) * 2: \\n', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 20}), Document(page_content='    model.add(keras.layers.Conv1D(filters=20, kernel_size =2, padding=\"causal\" , \\n                                  activation =\"relu\", dilation_rate =rate)) \\nmodel.add(keras.layers.Conv1D(filters=10, kernel_size =1)) \\nmodel.compile(loss=\"mse\", optimizer =\"adam\", metrics=[last_time_step_mse ]) \\nhistory = model.fit(X_train, Y_train, epochs=20, \\n                    validation_data =(X_valid, Y_valid)) \\nThis Sequential  model starts with an explicit input layer (this is simpler than trying to \\nset input_shape  only on the first layer), then continues with a 1D convolution al layer \\nusing  \"causal\"  padding: this ensures that the convolutional layer does not peek into the future \\nwhen making predictions (it is equivalent to padding the inputs with the right amount of zeros on \\nthe left and using  \"valid\"  padding). We then add  similar pairs of layers using growing dilation \\nrates: 1, 2, 4, 8, and again 1, 2, 4, 8. Finally, we add the output layer: a convolutional layer with \\n10 filters of size 1 and without any activation function. Thanks to the padding layers, every \\nconvolutional lay er outputs a sequence of the same length as the input sequences, so the targets \\nwe use during training can be the full sequences: no need to crop them or downsample them.  \\nThe last two models offer the best performance so far in forecasting our time series!  In the \\nWaveNet paper, the authors achieved state -of-the-art performance on various audio tasks (hence \\nthe name of the architecture), including text -to-speech tasks, producing incredibly realistic \\nvoices across several languages. They also used the model t o generate music, one audio sample \\nat a time. This feat is all the more impressive when you realize that a single second of audio can \\ncontain tens of thousands of time steps —even LSTMs and GRUs cannot handle such long \\nsequences.  \\nIn next section  we will con tinue to explore RNNs, and we will see how they can tackle various \\nNLP tasks.  \\n ', metadata={'source': 'documents\\\\Processing Sequences Using RNNs and CNNs.pdf', 'page': 21})]\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
